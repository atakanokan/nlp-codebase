{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses bag-of-ngrams approach to sentiment classification using the IMDB review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# logger.info(f\"asdasdasdasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"data/imdb_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "    file_list = os.listdir(folder_path)\n",
    "    # for debugging, printing out the folder path and some files in it\n",
    "    print(folder_path)\n",
    "    print(file_list[:10])\n",
    "    \n",
    "    all_reviews = []\n",
    "    for file_path in file_list:\n",
    "        f = open(folder_path + file_path,\"r\")\n",
    "        all_reviews.append(f.readline())\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_reviews/train/pos/\n",
      "['4715_9.txt', '12390_8.txt', '8329_7.txt', '9063_8.txt', '3092_10.txt', '9865_8.txt', '6639_10.txt', '10460_10.txt', '10331_10.txt', '11606_10.txt']\n",
      "12500\n",
      "data/imdb_reviews/train/neg/\n",
      "['1821_4.txt', '10402_1.txt', '1062_4.txt', '9056_1.txt', '5392_3.txt', '2682_3.txt', '3351_4.txt', '399_2.txt', '10447_1.txt', '10096_1.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/pos/\n",
      "['4715_9.txt', '1930_9.txt', '3205_9.txt', '10186_10.txt', '147_10.txt', '7511_7.txt', '616_10.txt', '10460_10.txt', '3240_9.txt', '1975_9.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/neg/\n",
      "['1821_4.txt', '9487_1.txt', '4604_4.txt', '2828_2.txt', '10890_1.txt', '3351_4.txt', '8070_2.txt', '1027_4.txt', '8248_3.txt', '4290_4.txt']\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "print(len(train_pos))\n",
    "train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "print(len(train_neg))\n",
    "test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "print(len(test_pos))\n",
    "test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)\n",
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive examples = 12500\n",
      "Train Negative examples = 12500\n",
      "Test Positive examples = 12500\n",
      "Test Negative examples = 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "train_pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the labels of the test set for Test Error Measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "print(len(test_data_labels))\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "test_neg_clean = [cleanhtml(x) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing dots & question marks & paranthesis with space\n",
    "\n",
    "It seems that punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"asdasdasds.asdasda\".replace(\".\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_dqmp(review):\n",
    "    \n",
    "#     review = review.replace(\".\",\" \")\n",
    "#     review = review.replace(\"?\",\" \")\n",
    "#     review = review.replace(\")\",\" \")\n",
    "#     review = review.replace(\"(\",\" \")\n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove_dqmp(train_pos_clean[random_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_clean = [remove_dqmp(x) for x in train_pos_clean]\n",
    "# train_neg_clean = [remove_dqmp(x) for x in train_neg_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# This is word tokenizer\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# Modified for n-grams\n",
    "def tokenize(sent, n_gram = 0):\n",
    "    \n",
    "    tokens = tokenizer(sent)\n",
    "    \n",
    "    # unigrams\n",
    "    #unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    unigrams = [token.lemma_.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    output = []\n",
    "    output.extend(unigrams)\n",
    "    \n",
    "    n = 2\n",
    "    while n <= n_gram:\n",
    "        ngram_tokens = [\" \".join(unigrams[x:x+n]) \\\n",
    "                            for x in range(len(unigrams)-n+1)]\n",
    "        output.extend(ngram_tokens)\n",
    "        n = n + 1\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While it certainly wasn't the best movie I've ever seen, it was certainly worth the $8 (which can't be said for many movies these days.)This was a pleasant account of a true story, although many of the details of the real story were twisted for the movie, (ie, Billy Sunday's character was three or four people in the real story combined together.) Robert DeNiro was of course good, and Cuba Gooding, Jr., was also impressive.\""
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', '-pron-', 'certainly', 'be', 'not', 'the', 'good', 'movie', '-pron-', 'have', 'ever', 'see', '-pron-', 'be', 'certainly', 'worth', 'the', '8', 'which', 'can', 'not', 'be', 'say', 'for', 'many', 'movie', 'these', 'days.)this', 'be', 'a', 'pleasant', 'account', 'of', 'a', 'true', 'story', 'although', 'many', 'of', 'the', 'detail', 'of', 'the', 'real', 'story', 'be', 'twist', 'for', 'the', 'movie', 'ie', 'billy', 'sunday', \"'s\", 'character', 'be', 'three', 'or', 'four', 'people', 'in', 'the', 'real', 'story', 'combine', 'together', 'robert', 'deniro', 'be', 'of', 'course', 'good', 'and', 'cuba', 'gooding', 'jr.', 'be', 'also', 'impressive', 'while -pron-', '-pron- certainly', 'certainly be', 'be not', 'not the', 'the good', 'good movie', 'movie -pron-', '-pron- have', 'have ever', 'ever see', 'see -pron-', '-pron- be', 'be certainly', 'certainly worth', 'worth the', 'the 8', '8 which', 'which can', 'can not', 'not be', 'be say', 'say for', 'for many', 'many movie', 'movie these', 'these days.)this', 'days.)this be', 'be a', 'a pleasant', 'pleasant account', 'account of', 'of a', 'a true', 'true story', 'story although', 'although many', 'many of', 'of the', 'the detail', 'detail of', 'of the', 'the real', 'real story', 'story be', 'be twist', 'twist for', 'for the', 'the movie', 'movie ie', 'ie billy', 'billy sunday', \"sunday 's\", \"'s character\", 'character be', 'be three', 'three or', 'or four', 'four people', 'people in', 'in the', 'the real', 'real story', 'story combine', 'combine together', 'together robert', 'robert deniro', 'deniro be', 'be of', 'of course', 'course good', 'good and', 'and cuba', 'cuba gooding', 'gooding jr.', 'jr. be', 'be also', 'also impressive', 'while -pron- certainly', '-pron- certainly be', 'certainly be not', 'be not the', 'not the good', 'the good movie', 'good movie -pron-', 'movie -pron- have', '-pron- have ever', 'have ever see', 'ever see -pron-', 'see -pron- be', '-pron- be certainly', 'be certainly worth', 'certainly worth the', 'worth the 8', 'the 8 which', '8 which can', 'which can not', 'can not be', 'not be say', 'be say for', 'say for many', 'for many movie', 'many movie these', 'movie these days.)this', 'these days.)this be', 'days.)this be a', 'be a pleasant', 'a pleasant account', 'pleasant account of', 'account of a', 'of a true', 'a true story', 'true story although', 'story although many', 'although many of', 'many of the', 'of the detail', 'the detail of', 'detail of the', 'of the real', 'the real story', 'real story be', 'story be twist', 'be twist for', 'twist for the', 'for the movie', 'the movie ie', 'movie ie billy', 'ie billy sunday', \"billy sunday 's\", \"sunday 's character\", \"'s character be\", 'character be three', 'be three or', 'three or four', 'or four people', 'four people in', 'people in the', 'in the real', 'the real story', 'real story combine', 'story combine together', 'combine together robert', 'together robert deniro', 'robert deniro be', 'deniro be of', 'be of course', 'of course good', 'course good and', 'good and cuba', 'and cuba gooding', 'cuba gooding jr.', 'gooding jr. be', 'jr. be also', 'be also impressive', 'while -pron- certainly be', '-pron- certainly be not', 'certainly be not the', 'be not the good', 'not the good movie', 'the good movie -pron-', 'good movie -pron- have', 'movie -pron- have ever', '-pron- have ever see', 'have ever see -pron-', 'ever see -pron- be', 'see -pron- be certainly', '-pron- be certainly worth', 'be certainly worth the', 'certainly worth the 8', 'worth the 8 which', 'the 8 which can', '8 which can not', 'which can not be', 'can not be say', 'not be say for', 'be say for many', 'say for many movie', 'for many movie these', 'many movie these days.)this', 'movie these days.)this be', 'these days.)this be a', 'days.)this be a pleasant', 'be a pleasant account', 'a pleasant account of', 'pleasant account of a', 'account of a true', 'of a true story', 'a true story although', 'true story although many', 'story although many of', 'although many of the', 'many of the detail', 'of the detail of', 'the detail of the', 'detail of the real', 'of the real story', 'the real story be', 'real story be twist', 'story be twist for', 'be twist for the', 'twist for the movie', 'for the movie ie', 'the movie ie billy', 'movie ie billy sunday', \"ie billy sunday 's\", \"billy sunday 's character\", \"sunday 's character be\", \"'s character be three\", 'character be three or', 'be three or four', 'three or four people', 'or four people in', 'four people in the', 'people in the real', 'in the real story', 'the real story combine', 'real story combine together', 'story combine together robert', 'combine together robert deniro', 'together robert deniro be', 'robert deniro be of', 'deniro be of course', 'be of course good', 'of course good and', 'course good and cuba', 'good and cuba gooding', 'and cuba gooding jr.', 'cuba gooding jr. be', 'gooding jr. be also', 'jr. be also impressive']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tokens = tokenize(train_pos_clean[random_text], n_gram = 4)\n",
    "#tokens = tokenize(train_pos_clean[random_text])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_clean = train_pos_clean + train_neg_clean\n",
    "len(train_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_clean = test_pos_clean + test_neg_clean\n",
    "len(test_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training -> Training + Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be smaller than 25000\n",
    "training_size = 20000\n",
    "\n",
    "assert training_size < 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[15821 15685  4147 ... 18888 20316 23805]\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15821, 15685,  4147, ..., 17207, 22378, 14852])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_index[:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, n_gram):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], \n",
    "#                                                batch_size=512, \n",
    "#                                                n_threads=4)):\n",
    "\n",
    "    itr = 0\n",
    "    for sample in dataset:\n",
    "        \n",
    "        if itr % 50 == 0:\n",
    "            print(str(itr) + \" / \" + str(len(dataset)))\n",
    "        # unigram version\n",
    "        #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        # n-gram version\n",
    "        tokens = tokenize(sample,n_gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "        itr = itr + 1\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n"
     ]
    }
   ],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram = 2)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                     n_gram = 2)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n",
      "0 / 25000\n",
      "50 / 25000\n",
      "100 / 25000\n",
      "150 / 25000\n",
      "200 / 25000\n",
      "250 / 25000\n",
      "300 / 25000\n",
      "350 / 25000\n",
      "400 / 25000\n",
      "450 / 25000\n",
      "500 / 25000\n",
      "550 / 25000\n",
      "600 / 25000\n",
      "650 / 25000\n",
      "700 / 25000\n",
      "750 / 25000\n",
      "800 / 25000\n",
      "850 / 25000\n",
      "900 / 25000\n",
      "950 / 25000\n",
      "1000 / 25000\n",
      "1050 / 25000\n",
      "1100 / 25000\n",
      "1150 / 25000\n",
      "1200 / 25000\n",
      "1250 / 25000\n",
      "1300 / 25000\n",
      "1350 / 25000\n",
      "1400 / 25000\n",
      "1450 / 25000\n",
      "1500 / 25000\n",
      "1550 / 25000\n",
      "1600 / 25000\n",
      "1650 / 25000\n",
      "1700 / 25000\n",
      "1750 / 25000\n",
      "1800 / 25000\n",
      "1850 / 25000\n",
      "1900 / 25000\n",
      "1950 / 25000\n",
      "2000 / 25000\n",
      "2050 / 25000\n",
      "2100 / 25000\n",
      "2150 / 25000\n",
      "2200 / 25000\n",
      "2250 / 25000\n",
      "2300 / 25000\n",
      "2350 / 25000\n",
      "2400 / 25000\n",
      "2450 / 25000\n",
      "2500 / 25000\n",
      "2550 / 25000\n",
      "2600 / 25000\n",
      "2650 / 25000\n",
      "2700 / 25000\n",
      "2750 / 25000\n",
      "2800 / 25000\n",
      "2850 / 25000\n",
      "2900 / 25000\n",
      "2950 / 25000\n",
      "3000 / 25000\n",
      "3050 / 25000\n",
      "3100 / 25000\n",
      "3150 / 25000\n",
      "3200 / 25000\n",
      "3250 / 25000\n",
      "3300 / 25000\n",
      "3350 / 25000\n",
      "3400 / 25000\n",
      "3450 / 25000\n",
      "3500 / 25000\n",
      "3550 / 25000\n",
      "3600 / 25000\n",
      "3650 / 25000\n",
      "3700 / 25000\n",
      "3750 / 25000\n",
      "3800 / 25000\n",
      "3850 / 25000\n",
      "3900 / 25000\n",
      "3950 / 25000\n",
      "4000 / 25000\n",
      "4050 / 25000\n",
      "4100 / 25000\n",
      "4150 / 25000\n",
      "4200 / 25000\n",
      "4250 / 25000\n",
      "4300 / 25000\n",
      "4350 / 25000\n",
      "4400 / 25000\n",
      "4450 / 25000\n",
      "4500 / 25000\n",
      "4550 / 25000\n",
      "4600 / 25000\n",
      "4650 / 25000\n",
      "4700 / 25000\n",
      "4750 / 25000\n",
      "4800 / 25000\n",
      "4850 / 25000\n",
      "4900 / 25000\n",
      "4950 / 25000\n",
      "5000 / 25000\n",
      "5050 / 25000\n",
      "5100 / 25000\n",
      "5150 / 25000\n",
      "5200 / 25000\n",
      "5250 / 25000\n",
      "5300 / 25000\n",
      "5350 / 25000\n",
      "5400 / 25000\n",
      "5450 / 25000\n",
      "5500 / 25000\n",
      "5550 / 25000\n",
      "5600 / 25000\n",
      "5650 / 25000\n",
      "5700 / 25000\n",
      "5750 / 25000\n",
      "5800 / 25000\n",
      "5850 / 25000\n",
      "5900 / 25000\n",
      "5950 / 25000\n",
      "6000 / 25000\n",
      "6050 / 25000\n",
      "6100 / 25000\n",
      "6150 / 25000\n",
      "6200 / 25000\n",
      "6250 / 25000\n",
      "6300 / 25000\n",
      "6350 / 25000\n",
      "6400 / 25000\n",
      "6450 / 25000\n",
      "6500 / 25000\n",
      "6550 / 25000\n",
      "6600 / 25000\n",
      "6650 / 25000\n",
      "6700 / 25000\n",
      "6750 / 25000\n",
      "6800 / 25000\n",
      "6850 / 25000\n",
      "6900 / 25000\n",
      "6950 / 25000\n",
      "7000 / 25000\n",
      "7050 / 25000\n",
      "7100 / 25000\n",
      "7150 / 25000\n",
      "7200 / 25000\n",
      "7250 / 25000\n",
      "7300 / 25000\n",
      "7350 / 25000\n",
      "7400 / 25000\n",
      "7450 / 25000\n",
      "7500 / 25000\n",
      "7550 / 25000\n",
      "7600 / 25000\n",
      "7650 / 25000\n",
      "7700 / 25000\n",
      "7750 / 25000\n",
      "7800 / 25000\n",
      "7850 / 25000\n",
      "7900 / 25000\n",
      "7950 / 25000\n",
      "8000 / 25000\n",
      "8050 / 25000\n",
      "8100 / 25000\n",
      "8150 / 25000\n",
      "8200 / 25000\n",
      "8250 / 25000\n",
      "8300 / 25000\n",
      "8350 / 25000\n",
      "8400 / 25000\n",
      "8450 / 25000\n",
      "8500 / 25000\n",
      "8550 / 25000\n",
      "8600 / 25000\n",
      "8650 / 25000\n",
      "8700 / 25000\n",
      "8750 / 25000\n",
      "8800 / 25000\n",
      "8850 / 25000\n",
      "8900 / 25000\n",
      "8950 / 25000\n",
      "9000 / 25000\n",
      "9050 / 25000\n",
      "9100 / 25000\n",
      "9150 / 25000\n",
      "9200 / 25000\n",
      "9250 / 25000\n",
      "9300 / 25000\n",
      "9350 / 25000\n",
      "9400 / 25000\n",
      "9450 / 25000\n",
      "9500 / 25000\n",
      "9550 / 25000\n",
      "9600 / 25000\n",
      "9650 / 25000\n",
      "9700 / 25000\n",
      "9750 / 25000\n",
      "9800 / 25000\n",
      "9850 / 25000\n",
      "9900 / 25000\n",
      "9950 / 25000\n",
      "10000 / 25000\n",
      "10050 / 25000\n",
      "10100 / 25000\n",
      "10150 / 25000\n",
      "10200 / 25000\n",
      "10250 / 25000\n",
      "10300 / 25000\n",
      "10350 / 25000\n",
      "10400 / 25000\n",
      "10450 / 25000\n",
      "10500 / 25000\n",
      "10550 / 25000\n",
      "10600 / 25000\n",
      "10650 / 25000\n",
      "10700 / 25000\n",
      "10750 / 25000\n",
      "10800 / 25000\n",
      "10850 / 25000\n",
      "10900 / 25000\n",
      "10950 / 25000\n",
      "11000 / 25000\n",
      "11050 / 25000\n",
      "11100 / 25000\n",
      "11150 / 25000\n",
      "11200 / 25000\n",
      "11250 / 25000\n",
      "11300 / 25000\n",
      "11350 / 25000\n",
      "11400 / 25000\n",
      "11450 / 25000\n",
      "11500 / 25000\n",
      "11550 / 25000\n",
      "11600 / 25000\n",
      "11650 / 25000\n",
      "11700 / 25000\n",
      "11750 / 25000\n",
      "11800 / 25000\n",
      "11850 / 25000\n",
      "11900 / 25000\n",
      "11950 / 25000\n",
      "12000 / 25000\n",
      "12050 / 25000\n",
      "12100 / 25000\n",
      "12150 / 25000\n",
      "12200 / 25000\n",
      "12250 / 25000\n",
      "12300 / 25000\n",
      "12350 / 25000\n",
      "12400 / 25000\n",
      "12450 / 25000\n",
      "12500 / 25000\n",
      "12550 / 25000\n",
      "12600 / 25000\n",
      "12650 / 25000\n",
      "12700 / 25000\n",
      "12750 / 25000\n",
      "12800 / 25000\n",
      "12850 / 25000\n",
      "12900 / 25000\n",
      "12950 / 25000\n",
      "13000 / 25000\n",
      "13050 / 25000\n",
      "13100 / 25000\n",
      "13150 / 25000\n",
      "13200 / 25000\n",
      "13250 / 25000\n",
      "13300 / 25000\n",
      "13350 / 25000\n",
      "13400 / 25000\n",
      "13450 / 25000\n",
      "13500 / 25000\n",
      "13550 / 25000\n",
      "13600 / 25000\n",
      "13650 / 25000\n",
      "13700 / 25000\n",
      "13750 / 25000\n",
      "13800 / 25000\n",
      "13850 / 25000\n",
      "13900 / 25000\n",
      "13950 / 25000\n",
      "14000 / 25000\n",
      "14050 / 25000\n",
      "14100 / 25000\n",
      "14150 / 25000\n",
      "14200 / 25000\n",
      "14250 / 25000\n",
      "14300 / 25000\n",
      "14350 / 25000\n",
      "14400 / 25000\n",
      "14450 / 25000\n",
      "14500 / 25000\n",
      "14550 / 25000\n",
      "14600 / 25000\n",
      "14650 / 25000\n",
      "14700 / 25000\n",
      "14750 / 25000\n",
      "14800 / 25000\n",
      "14850 / 25000\n",
      "14900 / 25000\n",
      "14950 / 25000\n",
      "15000 / 25000\n",
      "15050 / 25000\n",
      "15100 / 25000\n",
      "15150 / 25000\n",
      "15200 / 25000\n",
      "15250 / 25000\n",
      "15300 / 25000\n",
      "15350 / 25000\n",
      "15400 / 25000\n",
      "15450 / 25000\n",
      "15500 / 25000\n",
      "15550 / 25000\n",
      "15600 / 25000\n",
      "15650 / 25000\n",
      "15700 / 25000\n",
      "15750 / 25000\n",
      "15800 / 25000\n",
      "15850 / 25000\n",
      "15900 / 25000\n",
      "15950 / 25000\n",
      "16000 / 25000\n",
      "16050 / 25000\n",
      "16100 / 25000\n",
      "16150 / 25000\n",
      "16200 / 25000\n",
      "16250 / 25000\n",
      "16300 / 25000\n",
      "16350 / 25000\n",
      "16400 / 25000\n",
      "16450 / 25000\n",
      "16500 / 25000\n",
      "16550 / 25000\n",
      "16600 / 25000\n",
      "16650 / 25000\n",
      "16700 / 25000\n",
      "16750 / 25000\n",
      "16800 / 25000\n",
      "16850 / 25000\n",
      "16900 / 25000\n",
      "16950 / 25000\n",
      "17000 / 25000\n",
      "17050 / 25000\n",
      "17100 / 25000\n",
      "17150 / 25000\n",
      "17200 / 25000\n",
      "17250 / 25000\n",
      "17300 / 25000\n",
      "17350 / 25000\n",
      "17400 / 25000\n",
      "17450 / 25000\n",
      "17500 / 25000\n",
      "17550 / 25000\n",
      "17600 / 25000\n",
      "17650 / 25000\n",
      "17700 / 25000\n",
      "17750 / 25000\n",
      "17800 / 25000\n",
      "17850 / 25000\n",
      "17900 / 25000\n",
      "17950 / 25000\n",
      "18000 / 25000\n",
      "18050 / 25000\n",
      "18100 / 25000\n",
      "18150 / 25000\n",
      "18200 / 25000\n",
      "18250 / 25000\n",
      "18300 / 25000\n",
      "18350 / 25000\n",
      "18400 / 25000\n",
      "18450 / 25000\n",
      "18500 / 25000\n",
      "18550 / 25000\n",
      "18600 / 25000\n",
      "18650 / 25000\n",
      "18700 / 25000\n",
      "18750 / 25000\n",
      "18800 / 25000\n",
      "18850 / 25000\n",
      "18900 / 25000\n",
      "18950 / 25000\n",
      "19000 / 25000\n",
      "19050 / 25000\n",
      "19100 / 25000\n",
      "19150 / 25000\n",
      "19200 / 25000\n",
      "19250 / 25000\n",
      "19300 / 25000\n",
      "19350 / 25000\n",
      "19400 / 25000\n",
      "19450 / 25000\n",
      "19500 / 25000\n",
      "19550 / 25000\n",
      "19600 / 25000\n",
      "19650 / 25000\n",
      "19700 / 25000\n",
      "19750 / 25000\n",
      "19800 / 25000\n",
      "19850 / 25000\n",
      "19900 / 25000\n",
      "19950 / 25000\n",
      "20000 / 25000\n",
      "20050 / 25000\n",
      "20100 / 25000\n",
      "20150 / 25000\n",
      "20200 / 25000\n",
      "20250 / 25000\n",
      "20300 / 25000\n",
      "20350 / 25000\n",
      "20400 / 25000\n",
      "20450 / 25000\n",
      "20500 / 25000\n",
      "20550 / 25000\n",
      "20600 / 25000\n",
      "20650 / 25000\n",
      "20700 / 25000\n",
      "20750 / 25000\n",
      "20800 / 25000\n",
      "20850 / 25000\n",
      "20900 / 25000\n",
      "20950 / 25000\n",
      "21000 / 25000\n",
      "21050 / 25000\n",
      "21100 / 25000\n",
      "21150 / 25000\n",
      "21200 / 25000\n",
      "21250 / 25000\n",
      "21300 / 25000\n",
      "21350 / 25000\n",
      "21400 / 25000\n",
      "21450 / 25000\n",
      "21500 / 25000\n",
      "21550 / 25000\n",
      "21600 / 25000\n",
      "21650 / 25000\n",
      "21700 / 25000\n",
      "21750 / 25000\n",
      "21800 / 25000\n",
      "21850 / 25000\n",
      "21900 / 25000\n",
      "21950 / 25000\n",
      "22000 / 25000\n",
      "22050 / 25000\n",
      "22100 / 25000\n",
      "22150 / 25000\n",
      "22200 / 25000\n",
      "22250 / 25000\n",
      "22300 / 25000\n",
      "22350 / 25000\n",
      "22400 / 25000\n",
      "22450 / 25000\n",
      "22500 / 25000\n",
      "22550 / 25000\n",
      "22600 / 25000\n",
      "22650 / 25000\n",
      "22700 / 25000\n",
      "22750 / 25000\n",
      "22800 / 25000\n",
      "22850 / 25000\n",
      "22900 / 25000\n",
      "22950 / 25000\n",
      "23000 / 25000\n",
      "23050 / 25000\n",
      "23100 / 25000\n",
      "23150 / 25000\n",
      "23200 / 25000\n",
      "23250 / 25000\n",
      "23300 / 25000\n",
      "23350 / 25000\n",
      "23400 / 25000\n",
      "23450 / 25000\n",
      "23500 / 25000\n",
      "23550 / 25000\n",
      "23600 / 25000\n",
      "23650 / 25000\n",
      "23700 / 25000\n",
      "23750 / 25000\n",
      "23800 / 25000\n",
      "23850 / 25000\n",
      "23900 / 25000\n",
      "23950 / 25000\n",
      "24000 / 25000\n",
      "24050 / 25000\n",
      "24100 / 25000\n",
      "24150 / 25000\n",
      "24200 / 25000\n",
      "24250 / 25000\n",
      "24300 / 25000\n",
      "24350 / 25000\n",
      "24400 / 25000\n",
      "24450 / 25000\n",
      "24500 / 25000\n",
      "24550 / 25000\n",
      "24600 / 25000\n",
      "24650 / 25000\n",
      "24700 / 25000\n",
      "24750 / 25000\n",
      "24800 / 25000\n",
      "24850 / 25000\n",
      "24900 / 25000\n",
      "24950 / 25000\n"
     ]
    }
   ],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_all_clean,\n",
    "                                      n_gram = 2)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'surely', 'one', 'of', 'the', 'worst', 'films', 'ever', 'made', 'and', 'released', 'by', 'a', 'major', 'hollywood', 'studio', 'the', 'plot', 'is', 'simply', 'stupid', 'the', 'dialog', 'is', 'written', 'in', 'clichs', 'you', 'can', 'complete', 'a', 'great', 'many', 'sentences', 'in', 'the', 'script', 'because', 'of', 'this', 'the', 'acting', 'is', 'ridiculously', 'bad', 'especially', 'that', 'of', 'rod', 'cameron', 'the', 'choreography', 'is', 'silly', 'and', 'wholly', 'unerotic', 'one', 'can', 'only', 'pity', 'the', 'reviewer', 'who', 'saw', '23-year', 'old', 'yvonne', \"'s\", 'dance', 'as', 'sexual', 'it', \"'s\", 'merely', 'very', 'bad', 'choreography', 'the', 'ballet', 'scene', 'in', 'the', 'film', \"'s\", 'beginning', 'is', 'especially', 'ludicrous', 'if', 'you', 'are', 'into', 'bad', 'movies', 'and', 'enjoy', 'laughing', 'at', 'some', 'of', 'hollywood', \"'s\", 'turkeys', 'this', 'is', 'for', 'you', 'i', 'bought', 'the', 'colorized', 'version', 'on', 'vhs', 'making', 'the', 'movie', 'even', 'worse', 'yvonne', \"'s\", 'heavy', 'makeup', 'when', 'colored', 'has', 'her', 'looking', 'like', 'a', 'clown', 'all', 'the', 'time', 'and', 'she', \"'s\", 'the', 'best', 'part', 'of', 'this', 'film', 'what', 'a', 'way', 'to', 'launch', 'a', 'career', 'this is', 'is surely', 'surely one', 'one of', 'of the', 'the worst', 'worst films', 'films ever', 'ever made', 'made and', 'and released', 'released by', 'by a', 'a major', 'major hollywood', 'hollywood studio', 'studio the', 'the plot', 'plot is', 'is simply', 'simply stupid', 'stupid the', 'the dialog', 'dialog is', 'is written', 'written in', 'in clichs', 'clichs you', 'you can', 'can complete', 'complete a', 'a great', 'great many', 'many sentences', 'sentences in', 'in the', 'the script', 'script because', 'because of', 'of this', 'this the', 'the acting', 'acting is', 'is ridiculously', 'ridiculously bad', 'bad especially', 'especially that', 'that of', 'of rod', 'rod cameron', 'cameron the', 'the choreography', 'choreography is', 'is silly', 'silly and', 'and wholly', 'wholly unerotic', 'unerotic one', 'one can', 'can only', 'only pity', 'pity the', 'the reviewer', 'reviewer who', 'who saw', 'saw 23-year', '23-year old', 'old yvonne', \"yvonne 's\", \"'s dance\", 'dance as', 'as sexual', 'sexual it', \"it 's\", \"'s merely\", 'merely very', 'very bad', 'bad choreography', 'choreography the', 'the ballet', 'ballet scene', 'scene in', 'in the', 'the film', \"film 's\", \"'s beginning\", 'beginning is', 'is especially', 'especially ludicrous', 'ludicrous if', 'if you', 'you are', 'are into', 'into bad', 'bad movies', 'movies and', 'and enjoy', 'enjoy laughing', 'laughing at', 'at some', 'some of', 'of hollywood', \"hollywood 's\", \"'s turkeys\", 'turkeys this', 'this is', 'is for', 'for you', 'you i', 'i bought', 'bought the', 'the colorized', 'colorized version', 'version on', 'on vhs', 'vhs making', 'making the', 'the movie', 'movie even', 'even worse', 'worse yvonne', \"yvonne 's\", \"'s heavy\", 'heavy makeup', 'makeup when', 'when colored', 'colored has', 'has her', 'her looking', 'looking like', 'like a', 'a clown', 'clown all', 'all the', 'the time', 'time and', 'and she', \"she 's\", \"'s the\", 'the best', 'best part', 'part of', 'of this', 'this film', 'film what', 'what a', 'a way', 'way to', 'to launch', 'launch a', 'a career'], ['mabel', 'at', 'the', 'wheel', 'is', 'one', 'of', 'those', 'movies', 'with', 'a', 'behind', 'the', 'scenes', 'story', 'that', \"'s\", 'more', 'interesting', 'than', 'the', 'movie', 'itself', 'this', 'was', 'chaplin', \"'s\", 'tenth', 'comedy', 'for', 'keystone', 'during', 'his', 'year', 'of', 'apprenticeship', 'and', 'his', 'first', 'two', 'reeler', 'here', 'he', 'played', 'one', 'of', 'his', 'last', 'out', 'and', 'out', 'villain', 'roles', 'although', 'the', 'feature', 'length', 'tillie', \"'s\", 'punctured', 'romance', 'was', 'yet', 'to', 'come', 'and', 'it', 'also', 'marked', 'one', 'of', 'the', 'last', 'times', 'he', 'would', 'work', 'for', 'a', 'director', 'other', 'than', 'himself', 'in', 'fact', 'chaplin', \"'s\", 'conflicts', 'with', 'director', 'and', 'co', 'star', 'mabel', 'normand', 'almost', 'got', 'him', 'fired', 'from', 'the', 'studio', 'chaplin', 'had', \"n't\", 'gotten', 'along', 'with', 'his', 'earlier', 'directors', 'henry', 'lehrman', 'and', 'george', 'nichols', 'but', 'according', 'to', 'his', 'autobiography', 'having', 'to', 'take', 'direction', 'from', 'a', 'mere', 'girl', 'was', 'the', 'last', 'straw', 'charlie', 'and', 'mabel', 'argued', 'bitterly', 'during', 'the', 'making', 'of', 'this', 'film', 'chaplin', 'was', 'still', 'a', 'newcomer', 'at', 'keystone', 'and', 'his', 'colleagues', 'did', \"n't\", 'know', 'what', 'to', 'make', 'of', 'him', 'but', 'everyone', 'loved', 'mabel', 'producer', 'mack', 'sennett', 'was', 'on', 'the', 'verge', 'of', 'firing', 'chaplin', 'when', 'he', 'learned', 'that', 'the', 'newcomer', \"'s\", 'films', 'were', 'catching', 'on', 'and', 'exhibitors', 'wanted', 'more', 'of', 'them', 'a.s.a.p.', 'so', 'chaplin', 'was', 'promised', 'the', 'chance', 'to', 'direct', 'himself', 'in', 'return', 'for', 'finishing', 'this', 'movie', 'the', 'way', 'mabel', 'wanted', 'it', 'unfortunately', 'none', 'of', 'that', 'drama', 'is', 'visible', 'on', 'screen', 'in', 'mabel', 'at', 'the', 'wheel', 'which', 'looks', 'like', 'typical', 'keystone', 'chaos', 'the', 'story', 'concerns', 'an', 'auto', 'race', 'in', 'which', 'mabel', \"'s\", 'beau', 'harry', 'mccoy', 'is', 'scheduled', 'to', 'compete', 'but', 'wicked', 'charlie', 'and', 'his', 'henchmen', 'abduct', 'the', 'lad', 'and', 'mabel', 'must', 'take', 'the', 'wheel', 'in', 'his', 'place', 'for', 'all', 'the', 'racing', 'around', 'brick', 'hurling', 'and', 'finger', 'biting', 'the', 'film', 'is', 'frankly', 'short', 'on', 'laughs', 'but', 'there', 'are', 'a', 'few', 'points', 'of', 'interest', 'there', \"'s\", 'some', 'good', 'cinematography', 'and', 'editing', 'in', 'the', 'race', 'sequence', 'though', 'there', 'are', \"n't\", 'really', 'any', 'gags', 'just', 'lots', 'of', 'frantic', 'activity', 'chaplin', 'himself', 'looks', 'odd', 'sporting', 'a', 'goat', 'like', 'beard', 'on', 'his', 'chin', 'and', 'wearing', 'the', 'top', 'hat', 'and', 'frock', 'coat', 'he', 'wore', 'in', 'his', 'very', 'first', 'film', 'appearance', 'making', 'a', 'living', 'but', 'the', 'outfit', 'suits', 'the', 'old', 'fashioned', 'villainy', 'he', 'displays', 'throughout', 'at', 'least', 'it', \"'s\", 'novel', 'to', 'watch', 'him', 'play', 'such', 'an', 'uncharacteristic', 'role', 'visible', 'in', 'the', 'stands', 'at', 'the', 'race', 'track', 'are', 'such', 'keystone', 'stalwarts', 'as', 'chester', 'conklin', 'edgar', 'kennedy', 'in', 'a', 'strangely', 'dandified', 'get', 'up', 'and', 'a', 'more', 'characteristic', 'mack', 'sennett', 'spitting', 'tobacco', 'and', 'doing', 'his', 'usual', 'mindless', 'rube', 'routine', 'as', 'a', 'performer', 'sennett', 'was', 'about', 'as', 'subtle', 'as', 'the', 'movies', 'he', 'produced', 'but', 'you', 'have', 'to', 'give', 'the', 'guy', 'credit', 'he', 'knew', 'what', 'people', 'liked', 'these', 'films', 'were', 'hugely', 'popular', 'in', 'their', 'day', 'mack', \"'s\", 'performance', 'does', \"n't\", 'add', 'much', 'to', 'mabel', 'at', 'the', 'wheel', 'but', 'he', 'probably', 'had', 'to', 'be', 'on', 'hand', 'for', 'the', 'filming', 'of', 'this', 'one', 'to', 'make', 'sure', 'his', 'stars', 'did', \"n't\", 'kill', 'each', 'other', 'mabel at', 'at the', 'the wheel', 'wheel is', 'is one', 'one of', 'of those', 'those movies', 'movies with', 'with a', 'a behind', 'behind the', 'the scenes', 'scenes story', 'story that', \"that 's\", \"'s more\", 'more interesting', 'interesting than', 'than the', 'the movie', 'movie itself', 'itself this', 'this was', 'was chaplin', \"chaplin 's\", \"'s tenth\", 'tenth comedy', 'comedy for', 'for keystone', 'keystone during', 'during his', 'his year', 'year of', 'of apprenticeship', 'apprenticeship and', 'and his', 'his first', 'first two', 'two reeler', 'reeler here', 'here he', 'he played', 'played one', 'one of', 'of his', 'his last', 'last out', 'out and', 'and out', 'out villain', 'villain roles', 'roles although', 'although the', 'the feature', 'feature length', 'length tillie', \"tillie 's\", \"'s punctured\", 'punctured romance', 'romance was', 'was yet', 'yet to', 'to come', 'come and', 'and it', 'it also', 'also marked', 'marked one', 'one of', 'of the', 'the last', 'last times', 'times he', 'he would', 'would work', 'work for', 'for a', 'a director', 'director other', 'other than', 'than himself', 'himself in', 'in fact', 'fact chaplin', \"chaplin 's\", \"'s conflicts\", 'conflicts with', 'with director', 'director and', 'and co', 'co star', 'star mabel', 'mabel normand', 'normand almost', 'almost got', 'got him', 'him fired', 'fired from', 'from the', 'the studio', 'studio chaplin', 'chaplin had', \"had n't\", \"n't gotten\", 'gotten along', 'along with', 'with his', 'his earlier', 'earlier directors', 'directors henry', 'henry lehrman', 'lehrman and', 'and george', 'george nichols', 'nichols but', 'but according', 'according to', 'to his', 'his autobiography', 'autobiography having', 'having to', 'to take', 'take direction', 'direction from', 'from a', 'a mere', 'mere girl', 'girl was', 'was the', 'the last', 'last straw', 'straw charlie', 'charlie and', 'and mabel', 'mabel argued', 'argued bitterly', 'bitterly during', 'during the', 'the making', 'making of', 'of this', 'this film', 'film chaplin', 'chaplin was', 'was still', 'still a', 'a newcomer', 'newcomer at', 'at keystone', 'keystone and', 'and his', 'his colleagues', 'colleagues did', \"did n't\", \"n't know\", 'know what', 'what to', 'to make', 'make of', 'of him', 'him but', 'but everyone', 'everyone loved', 'loved mabel', 'mabel producer', 'producer mack', 'mack sennett', 'sennett was', 'was on', 'on the', 'the verge', 'verge of', 'of firing', 'firing chaplin', 'chaplin when', 'when he', 'he learned', 'learned that', 'that the', 'the newcomer', \"newcomer 's\", \"'s films\", 'films were', 'were catching', 'catching on', 'on and', 'and exhibitors', 'exhibitors wanted', 'wanted more', 'more of', 'of them', 'them a.s.a.p.', 'a.s.a.p. so', 'so chaplin', 'chaplin was', 'was promised', 'promised the', 'the chance', 'chance to', 'to direct', 'direct himself', 'himself in', 'in return', 'return for', 'for finishing', 'finishing this', 'this movie', 'movie the', 'the way', 'way mabel', 'mabel wanted', 'wanted it', 'it unfortunately', 'unfortunately none', 'none of', 'of that', 'that drama', 'drama is', 'is visible', 'visible on', 'on screen', 'screen in', 'in mabel', 'mabel at', 'at the', 'the wheel', 'wheel which', 'which looks', 'looks like', 'like typical', 'typical keystone', 'keystone chaos', 'chaos the', 'the story', 'story concerns', 'concerns an', 'an auto', 'auto race', 'race in', 'in which', 'which mabel', \"mabel 's\", \"'s beau\", 'beau harry', 'harry mccoy', 'mccoy is', 'is scheduled', 'scheduled to', 'to compete', 'compete but', 'but wicked', 'wicked charlie', 'charlie and', 'and his', 'his henchmen', 'henchmen abduct', 'abduct the', 'the lad', 'lad and', 'and mabel', 'mabel must', 'must take', 'take the', 'the wheel', 'wheel in', 'in his', 'his place', 'place for', 'for all', 'all the', 'the racing', 'racing around', 'around brick', 'brick hurling', 'hurling and', 'and finger', 'finger biting', 'biting the', 'the film', 'film is', 'is frankly', 'frankly short', 'short on', 'on laughs', 'laughs but', 'but there', 'there are', 'are a', 'a few', 'few points', 'points of', 'of interest', 'interest there', \"there 's\", \"'s some\", 'some good', 'good cinematography', 'cinematography and', 'and editing', 'editing in', 'in the', 'the race', 'race sequence', 'sequence though', 'though there', 'there are', \"are n't\", \"n't really\", 'really any', 'any gags', 'gags just', 'just lots', 'lots of', 'of frantic', 'frantic activity', 'activity chaplin', 'chaplin himself', 'himself looks', 'looks odd', 'odd sporting', 'sporting a', 'a goat', 'goat like', 'like beard', 'beard on', 'on his', 'his chin', 'chin and', 'and wearing', 'wearing the', 'the top', 'top hat', 'hat and', 'and frock', 'frock coat', 'coat he', 'he wore', 'wore in', 'in his', 'his very', 'very first', 'first film', 'film appearance', 'appearance making', 'making a', 'a living', 'living but', 'but the', 'the outfit', 'outfit suits', 'suits the', 'the old', 'old fashioned', 'fashioned villainy', 'villainy he', 'he displays', 'displays throughout', 'throughout at', 'at least', 'least it', \"it 's\", \"'s novel\", 'novel to', 'to watch', 'watch him', 'him play', 'play such', 'such an', 'an uncharacteristic', 'uncharacteristic role', 'role visible', 'visible in', 'in the', 'the stands', 'stands at', 'at the', 'the race', 'race track', 'track are', 'are such', 'such keystone', 'keystone stalwarts', 'stalwarts as', 'as chester', 'chester conklin', 'conklin edgar', 'edgar kennedy', 'kennedy in', 'in a', 'a strangely', 'strangely dandified', 'dandified get', 'get up', 'up and', 'and a', 'a more', 'more characteristic', 'characteristic mack', 'mack sennett', 'sennett spitting', 'spitting tobacco', 'tobacco and', 'and doing', 'doing his', 'his usual', 'usual mindless', 'mindless rube', 'rube routine', 'routine as', 'as a', 'a performer', 'performer sennett', 'sennett was', 'was about', 'about as', 'as subtle', 'subtle as', 'as the', 'the movies', 'movies he', 'he produced', 'produced but', 'but you', 'you have', 'have to', 'to give', 'give the', 'the guy', 'guy credit', 'credit he', 'he knew', 'knew what', 'what people', 'people liked', 'liked these', 'these films', 'films were', 'were hugely', 'hugely popular', 'popular in', 'in their', 'their day', 'day mack', \"mack 's\", \"'s performance\", 'performance does', \"does n't\", \"n't add\", 'add much', 'much to', 'to mabel', 'mabel at', 'at the', 'the wheel', 'wheel but', 'but he', 'he probably', 'probably had', 'had to', 'to be', 'be on', 'on hand', 'hand for', 'for the', 'the filming', 'filming of', 'of this', 'this one', 'one to', 'to make', 'make sure', 'sure his', 'his stars', 'stars did', \"did n't\", \"n't kill\", 'kill each', 'each other']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'surely', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(all_train_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove blank space tokens\n",
    "\n",
    "In the above tokenization, some blankspace strings were observed, thus this section adresses that by deleting them from the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blankspaces = [\" \",\"  \",\"   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_blankspaces(review):\n",
    "    \n",
    "#     review = [x for x in review if x not in blankspaces] \n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(remove_blankspaces(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data_tokens_clean = [remove_blankspaces(token) for token in train_data_tokens]\n",
    "# len(train_data_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens_clean = remove_blankspaces(all_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9538806"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289344"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-365f383102c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     return token2id, id2token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtoken2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# max_vocab_size = 10000\n",
    "# # save index 0 for unk and 1 for pad\n",
    "# PAD_IDX = 0\n",
    "# UNK_IDX = 1\n",
    "\n",
    "# def build_vocab(all_tokens,vocab_size=max_vocab_size):\n",
    "#     # Returns:\n",
    "#     # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "#     # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "#     token_counter = Counter(all_tokens)\n",
    "#     vocab, count = zip(*token_counter.most_common(vocab_size))\n",
    "#     id2token = list(vocab)\n",
    "#     token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "#     id2token = ['<pad>', '<unk>'] + id2token\n",
    "#     token2id['<pad>'] = PAD_IDX \n",
    "#     token2id['<unk>'] = UNK_IDX\n",
    "#     return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8255 ; token my money\n",
      "Token my money; token id 8255\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDBDataset(train_data_indices, training_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, validation_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3c5a3717ed55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBagOfNgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mBagOfNgrams\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [51/625], Training Acc: 51.965,Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [101/625], Training Acc: 69.47,Validation Acc: 67.8\n",
      "Epoch: [1/10], Step: [151/625], Training Acc: 81.755,Validation Acc: 80.14\n",
      "Epoch: [1/10], Step: [201/625], Training Acc: 84.655,Validation Acc: 82.76\n",
      "Epoch: [1/10], Step: [251/625], Training Acc: 86.205,Validation Acc: 84.12\n",
      "Epoch: [1/10], Step: [301/625], Training Acc: 87.52,Validation Acc: 84.44\n",
      "Epoch: [1/10], Step: [351/625], Training Acc: 88.47,Validation Acc: 85.52\n",
      "Epoch: [1/10], Step: [401/625], Training Acc: 88.695,Validation Acc: 85.42\n",
      "Epoch: [1/10], Step: [451/625], Training Acc: 88.65,Validation Acc: 85.02\n",
      "Epoch: [1/10], Step: [501/625], Training Acc: 90.575,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [551/625], Training Acc: 90.955,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [601/625], Training Acc: 91.545,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [51/625], Training Acc: 91.66,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [101/625], Training Acc: 91.36,Validation Acc: 86.16\n",
      "Epoch: [2/10], Step: [151/625], Training Acc: 91.815,Validation Acc: 86.7\n",
      "Epoch: [2/10], Step: [201/625], Training Acc: 92.165,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [251/625], Training Acc: 92.52,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [301/625], Training Acc: 91.02,Validation Acc: 84.94\n",
      "Epoch: [2/10], Step: [351/625], Training Acc: 90.405,Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [401/625], Training Acc: 92.965,Validation Acc: 86.8\n",
      "Epoch: [2/10], Step: [451/625], Training Acc: 93.18,Validation Acc: 86.64\n",
      "Epoch: [2/10], Step: [501/625], Training Acc: 92.6,Validation Acc: 85.84\n",
      "Epoch: [2/10], Step: [551/625], Training Acc: 93.67,Validation Acc: 86.9\n",
      "Epoch: [2/10], Step: [601/625], Training Acc: 94.16,Validation Acc: 86.9\n",
      "Epoch: [3/10], Step: [51/625], Training Acc: 94.08,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [101/625], Training Acc: 94.445,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [151/625], Training Acc: 94.335,Validation Acc: 86.64\n",
      "Epoch: [3/10], Step: [201/625], Training Acc: 93.075,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [251/625], Training Acc: 94.62,Validation Acc: 86.42\n",
      "Epoch: [3/10], Step: [301/625], Training Acc: 94.36,Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [351/625], Training Acc: 94.85,Validation Acc: 86.4\n",
      "Epoch: [3/10], Step: [401/625], Training Acc: 94.705,Validation Acc: 85.98\n",
      "Epoch: [3/10], Step: [451/625], Training Acc: 95.19,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [501/625], Training Acc: 95.215,Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [551/625], Training Acc: 94.475,Validation Acc: 85.5\n",
      "Epoch: [3/10], Step: [601/625], Training Acc: 95.13,Validation Acc: 85.92\n",
      "Epoch: [4/10], Step: [51/625], Training Acc: 94.39,Validation Acc: 85.26\n",
      "Epoch: [4/10], Step: [101/625], Training Acc: 95.16,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [151/625], Training Acc: 95.31,Validation Acc: 85.86\n",
      "Epoch: [4/10], Step: [201/625], Training Acc: 94.805,Validation Acc: 85.22\n",
      "Epoch: [4/10], Step: [251/625], Training Acc: 95.23,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [301/625], Training Acc: 95.89,Validation Acc: 85.3\n",
      "Epoch: [4/10], Step: [351/625], Training Acc: 95.895,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [401/625], Training Acc: 95.84,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [451/625], Training Acc: 96.21,Validation Acc: 85.18\n",
      "Epoch: [4/10], Step: [501/625], Training Acc: 96.0,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [551/625], Training Acc: 95.8,Validation Acc: 85.42\n",
      "Epoch: [4/10], Step: [601/625], Training Acc: 96.345,Validation Acc: 85.62\n",
      "Epoch: [5/10], Step: [51/625], Training Acc: 96.41,Validation Acc: 85.5\n",
      "Epoch: [5/10], Step: [101/625], Training Acc: 96.415,Validation Acc: 85.32\n",
      "Epoch: [5/10], Step: [151/625], Training Acc: 96.14,Validation Acc: 84.98\n",
      "Epoch: [5/10], Step: [201/625], Training Acc: 95.265,Validation Acc: 84.26\n",
      "Epoch: [5/10], Step: [251/625], Training Acc: 95.785,Validation Acc: 84.56\n",
      "Epoch: [5/10], Step: [301/625], Training Acc: 96.415,Validation Acc: 85.36\n",
      "Epoch: [5/10], Step: [351/625], Training Acc: 96.61,Validation Acc: 85.48\n",
      "Epoch: [5/10], Step: [401/625], Training Acc: 95.835,Validation Acc: 84.32\n",
      "Epoch: [5/10], Step: [451/625], Training Acc: 96.675,Validation Acc: 85.12\n",
      "Epoch: [5/10], Step: [501/625], Training Acc: 96.98,Validation Acc: 85.3\n",
      "Epoch: [5/10], Step: [551/625], Training Acc: 96.965,Validation Acc: 85.08\n",
      "Epoch: [5/10], Step: [601/625], Training Acc: 96.755,Validation Acc: 84.64\n",
      "Epoch: [6/10], Step: [51/625], Training Acc: 97.115,Validation Acc: 85.04\n",
      "Epoch: [6/10], Step: [101/625], Training Acc: 97.185,Validation Acc: 84.78\n",
      "Epoch: [6/10], Step: [151/625], Training Acc: 96.855,Validation Acc: 84.54\n",
      "Epoch: [6/10], Step: [201/625], Training Acc: 96.615,Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [251/625], Training Acc: 95.92,Validation Acc: 83.54\n",
      "Epoch: [6/10], Step: [301/625], Training Acc: 96.895,Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [351/625], Training Acc: 96.995,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [401/625], Training Acc: 97.005,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [451/625], Training Acc: 96.785,Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [501/625], Training Acc: 97.03,Validation Acc: 84.74\n",
      "Epoch: [6/10], Step: [551/625], Training Acc: 95.895,Validation Acc: 83.78\n",
      "Epoch: [6/10], Step: [601/625], Training Acc: 96.885,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [51/625], Training Acc: 96.99,Validation Acc: 84.42\n",
      "Epoch: [7/10], Step: [101/625], Training Acc: 97.6,Validation Acc: 84.66\n",
      "Epoch: [7/10], Step: [151/625], Training Acc: 97.25,Validation Acc: 84.4\n",
      "Epoch: [7/10], Step: [201/625], Training Acc: 97.48,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [251/625], Training Acc: 97.555,Validation Acc: 84.32\n",
      "Epoch: [7/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 84.54\n",
      "Epoch: [7/10], Step: [351/625], Training Acc: 96.79,Validation Acc: 83.8\n",
      "Epoch: [7/10], Step: [401/625], Training Acc: 97.3,Validation Acc: 83.76\n",
      "Epoch: [7/10], Step: [451/625], Training Acc: 97.43,Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [501/625], Training Acc: 97.525,Validation Acc: 84.22\n",
      "Epoch: [7/10], Step: [551/625], Training Acc: 97.67,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [601/625], Training Acc: 97.295,Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [51/625], Training Acc: 97.83,Validation Acc: 84.56\n",
      "Epoch: [8/10], Step: [101/625], Training Acc: 97.805,Validation Acc: 84.52\n",
      "Epoch: [8/10], Step: [151/625], Training Acc: 97.855,Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [201/625], Training Acc: 97.81,Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [251/625], Training Acc: 97.655,Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [301/625], Training Acc: 97.075,Validation Acc: 84.06\n",
      "Epoch: [8/10], Step: [351/625], Training Acc: 97.505,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [401/625], Training Acc: 97.59,Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [451/625], Training Acc: 97.46,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [501/625], Training Acc: 97.655,Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [551/625], Training Acc: 96.37,Validation Acc: 82.9\n",
      "Epoch: [8/10], Step: [601/625], Training Acc: 96.815,Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [51/625], Training Acc: 97.91,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [101/625], Training Acc: 97.89,Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [151/625], Training Acc: 97.805,Validation Acc: 83.56\n",
      "Epoch: [9/10], Step: [201/625], Training Acc: 97.41,Validation Acc: 83.74\n",
      "Epoch: [9/10], Step: [251/625], Training Acc: 97.95,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 83.68\n",
      "Epoch: [9/10], Step: [351/625], Training Acc: 97.85,Validation Acc: 83.92\n",
      "Epoch: [9/10], Step: [401/625], Training Acc: 98.08,Validation Acc: 83.64\n",
      "Epoch: [9/10], Step: [451/625], Training Acc: 98.22,Validation Acc: 83.48\n",
      "Epoch: [9/10], Step: [501/625], Training Acc: 97.505,Validation Acc: 83.3\n",
      "Epoch: [9/10], Step: [551/625], Training Acc: 98.14,Validation Acc: 83.62\n",
      "Epoch: [9/10], Step: [601/625], Training Acc: 97.92,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [51/625], Training Acc: 98.1,Validation Acc: 83.78\n",
      "Epoch: [10/10], Step: [101/625], Training Acc: 96.255,Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [151/625], Training Acc: 97.645,Validation Acc: 83.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [201/625], Training Acc: 98.11,Validation Acc: 83.42\n",
      "Epoch: [10/10], Step: [251/625], Training Acc: 96.115,Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [301/625], Training Acc: 97.575,Validation Acc: 83.18\n",
      "Epoch: [10/10], Step: [351/625], Training Acc: 98.245,Validation Acc: 83.76\n",
      "Epoch: [10/10], Step: [401/625], Training Acc: 98.13,Validation Acc: 83.66\n",
      "Epoch: [10/10], Step: [451/625], Training Acc: 98.185,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [501/625], Training Acc: 98.445,Validation Acc: 83.36\n",
      "Epoch: [10/10], Step: [551/625], Training Acc: 97.95,Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [601/625], Training Acc: 97.66,Validation Acc: 83.54\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # check training score every 100 iterations\n",
    "        ## validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {},Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, \n",
    "                len(train_loader), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we are going to try to optimize are the following:\n",
    "\n",
    "* n-gram max length\n",
    "* optimizer choice\n",
    "* embedding size\n",
    "* vocab size\n",
    "* learning rate of the optimizer\n",
    "\n",
    "And maybe increase the batch size to speed up the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = [torch.optim.Adam(model.parameters(), \n",
    "                               lr=learning_rate),             \n",
    "              torch.optim.SGD(model.parameters(), \n",
    "                              lr=learning_rate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[14269 16316  4649 ...  8312 14965 23410]\n",
      "20000\n",
      "20000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)\n",
    "\n",
    "shuffled_index[:training_size]\n",
    "\n",
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))\n",
    "\n",
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all ngram tokens for easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams = params[1]\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "grams = 4\n",
    "\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram=grams)\n",
    "\n",
    "# Tokenize Validation\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                      n_gram=grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_4\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "grams = \"lemma_4\"\n",
    "print(grams)\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_\"+str(grams)+\".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(train_data_tokens[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(val_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params,\n",
    "                          epochs=5,\n",
    "                          optimizer_name = \"Adam\",\n",
    "                          lemmatize = False):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    # counter for progress\n",
    "    count = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Parameter Combination = \" + str(count+1) + \" / \" + str(len(param_space)))\n",
    "        count = count + 1      \n",
    "        \n",
    "        NUM_EPOCHS = epochs\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = int(param_comb[2]) # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        MAX_SENTENCE_LENGTH = param_comb[4] # max sentence length of data loader\n",
    "        BATCH_SIZE = param_comb[5]\n",
    "        \n",
    "        print(\"Learning Rate = \" + str(lr_rate))\n",
    "        print(\"Ngram = \" + str(grams))\n",
    "        print(\"Vocab Size = \" + str(max_vocab_size))\n",
    "        print(\"Embedding Dimension = \" + str(embed_dimension))\n",
    "        print(\"Max Sentence Length = \" + str(MAX_SENTENCE_LENGTH))\n",
    "        print(\"Batch Size = \" + str(BATCH_SIZE))\n",
    "\n",
    "        # Tokenization\n",
    "        # All tokens are created before the hyperparameter search loop\n",
    "        # Load the tokens here\n",
    "        if lemmatize == True:\n",
    "            grams = \"lemma_\" + str(grams)\n",
    "        \n",
    "        train_data_tokens = pkl.load(open(\"train_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        all_train_tokens = pkl.load(open(\"all_train_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "\n",
    "        val_data_tokens = pkl.load(open(\"val_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        \n",
    "        print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "        print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "        \n",
    "        # Building Vocabulary\n",
    "        # implicitly gets the max_vocab_size parameter\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                         max_vocab_size=max_vocab_size)\n",
    "        \n",
    "        # Lets check the dictionary by loading random token from it\n",
    "        random_token_id = random.randint(0, len(id2token)-1)\n",
    "        random_token = id2token[random_token_id]\n",
    "        print (\"Token id {} -> token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "        print (\"Token {} -> token id {}\".format(random_token, token2id[random_token]))\n",
    "        \n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "        # double checking\n",
    "        print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Load training and validation data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)  \n",
    "\n",
    "        # Initialize the N-gram Model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "        \n",
    "        # Both Adam and SGD will be tried\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "        else:\n",
    "            print(\"this optimizer is not implemented yet\")\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        # Caution: Two different optimizers\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "    #for optimizer in optimizers:\n",
    "        print(\"Optimization Start\")\n",
    "        print(optimizer)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Validate every 100 iterations\n",
    "                # Adjust it to accustom changing batch sizes\n",
    "                if i > 0 and i % (50 * (64 / BATCH_SIZE)) == 0:\n",
    "\n",
    "                    # Accuracy Calculations\n",
    "                    train_acc = test_model(train_loader, model)\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                    # Logging\n",
    "                    print('Epoch:[{}/{}],Step:[{}/{}],Training Acc:{},Validation Acc:{}'.format( \n",
    "                               epoch+1, NUM_EPOCHS, \n",
    "                                i+1, len(train_loader), \n",
    "                                train_acc, val_acc))\n",
    "                      \n",
    "    return val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.01, 1, 100000.0, 100, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 100, 128),\n",
       " (0.01, 1, 100000.0, 100, 200, 64),\n",
       " (0.01, 1, 100000.0, 100, 200, 128),\n",
       " (0.01, 1, 100000.0, 150, 100, 64),\n",
       " (0.01, 1, 100000.0, 150, 100, 128),\n",
       " (0.01, 1, 100000.0, 150, 200, 64),\n",
       " (0.01, 1, 100000.0, 150, 200, 128),\n",
       " (0.01, 1, 100000.0, 200, 100, 64),\n",
       " (0.01, 1, 100000.0, 200, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 100, 128),\n",
       " (0.01, 1, 1000000.0, 100, 200, 64),\n",
       " (0.01, 1, 1000000.0, 100, 200, 128),\n",
       " (0.01, 1, 1000000.0, 150, 100, 64),\n",
       " (0.01, 1, 1000000.0, 150, 100, 128),\n",
       " (0.01, 1, 1000000.0, 150, 200, 64),\n",
       " (0.01, 1, 1000000.0, 150, 200, 128),\n",
       " (0.01, 1, 1000000.0, 200, 100, 64),\n",
       " (0.01, 1, 1000000.0, 200, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 100, 128),\n",
       " (0.01, 2, 100000.0, 100, 200, 64),\n",
       " (0.01, 2, 100000.0, 100, 200, 128),\n",
       " (0.01, 2, 100000.0, 150, 100, 64),\n",
       " (0.01, 2, 100000.0, 150, 100, 128),\n",
       " (0.01, 2, 100000.0, 150, 200, 64),\n",
       " (0.01, 2, 100000.0, 150, 200, 128),\n",
       " (0.01, 2, 100000.0, 200, 100, 64),\n",
       " (0.01, 2, 100000.0, 200, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 100, 128),\n",
       " (0.01, 2, 1000000.0, 100, 200, 64),\n",
       " (0.01, 2, 1000000.0, 100, 200, 128),\n",
       " (0.01, 2, 1000000.0, 150, 100, 64),\n",
       " (0.01, 2, 1000000.0, 150, 100, 128),\n",
       " (0.01, 2, 1000000.0, 150, 200, 64),\n",
       " (0.01, 2, 1000000.0, 150, 200, 128),\n",
       " (0.01, 2, 1000000.0, 200, 100, 64),\n",
       " (0.01, 2, 1000000.0, 200, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 100, 128),\n",
       " (0.01, 3, 100000.0, 100, 200, 64),\n",
       " (0.01, 3, 100000.0, 100, 200, 128),\n",
       " (0.01, 3, 100000.0, 150, 100, 64),\n",
       " (0.01, 3, 100000.0, 150, 100, 128),\n",
       " (0.01, 3, 100000.0, 150, 200, 64),\n",
       " (0.01, 3, 100000.0, 150, 200, 128),\n",
       " (0.01, 3, 100000.0, 200, 100, 64),\n",
       " (0.01, 3, 100000.0, 200, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 100, 128),\n",
       " (0.01, 3, 1000000.0, 100, 200, 64),\n",
       " (0.01, 3, 1000000.0, 100, 200, 128),\n",
       " (0.01, 3, 1000000.0, 150, 100, 64),\n",
       " (0.01, 3, 1000000.0, 150, 100, 128),\n",
       " (0.01, 3, 1000000.0, 150, 200, 64),\n",
       " (0.01, 3, 1000000.0, 150, 200, 128),\n",
       " (0.01, 3, 1000000.0, 200, 100, 64),\n",
       " (0.01, 3, 1000000.0, 200, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 100, 128),\n",
       " (0.1, 1, 100000.0, 100, 200, 64),\n",
       " (0.1, 1, 100000.0, 100, 200, 128),\n",
       " (0.1, 1, 100000.0, 150, 100, 64),\n",
       " (0.1, 1, 100000.0, 150, 100, 128),\n",
       " (0.1, 1, 100000.0, 150, 200, 64),\n",
       " (0.1, 1, 100000.0, 150, 200, 128),\n",
       " (0.1, 1, 100000.0, 200, 100, 64),\n",
       " (0.1, 1, 100000.0, 200, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 100, 128),\n",
       " (0.1, 1, 1000000.0, 100, 200, 64),\n",
       " (0.1, 1, 1000000.0, 100, 200, 128),\n",
       " (0.1, 1, 1000000.0, 150, 100, 64),\n",
       " (0.1, 1, 1000000.0, 150, 100, 128),\n",
       " (0.1, 1, 1000000.0, 150, 200, 64),\n",
       " (0.1, 1, 1000000.0, 150, 200, 128),\n",
       " (0.1, 1, 1000000.0, 200, 100, 64),\n",
       " (0.1, 1, 1000000.0, 200, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 100, 128),\n",
       " (0.1, 2, 100000.0, 100, 200, 64),\n",
       " (0.1, 2, 100000.0, 100, 200, 128),\n",
       " (0.1, 2, 100000.0, 150, 100, 64),\n",
       " (0.1, 2, 100000.0, 150, 100, 128),\n",
       " (0.1, 2, 100000.0, 150, 200, 64),\n",
       " (0.1, 2, 100000.0, 150, 200, 128),\n",
       " (0.1, 2, 100000.0, 200, 100, 64),\n",
       " (0.1, 2, 100000.0, 200, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 100, 128),\n",
       " (0.1, 2, 1000000.0, 100, 200, 64),\n",
       " (0.1, 2, 1000000.0, 100, 200, 128),\n",
       " (0.1, 2, 1000000.0, 150, 100, 64),\n",
       " (0.1, 2, 1000000.0, 150, 100, 128),\n",
       " (0.1, 2, 1000000.0, 150, 200, 64),\n",
       " (0.1, 2, 1000000.0, 150, 200, 128),\n",
       " (0.1, 2, 1000000.0, 200, 100, 64),\n",
       " (0.1, 2, 1000000.0, 200, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 100, 128),\n",
       " (0.1, 3, 100000.0, 100, 200, 64),\n",
       " (0.1, 3, 100000.0, 100, 200, 128),\n",
       " (0.1, 3, 100000.0, 150, 100, 64),\n",
       " (0.1, 3, 100000.0, 150, 100, 128),\n",
       " (0.1, 3, 100000.0, 150, 200, 64),\n",
       " (0.1, 3, 100000.0, 150, 200, 128),\n",
       " (0.1, 3, 100000.0, 200, 100, 64),\n",
       " (0.1, 3, 100000.0, 200, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 100, 128),\n",
       " (0.1, 3, 1000000.0, 100, 200, 64),\n",
       " (0.1, 3, 1000000.0, 100, 200, 128),\n",
       " (0.1, 3, 1000000.0, 150, 100, 64),\n",
       " (0.1, 3, 1000000.0, 150, 100, 128),\n",
       " (0.1, 3, 1000000.0, 150, 200, 64),\n",
       " (0.1, 3, 1000000.0, 150, 200, 128),\n",
       " (0.1, 3, 1000000.0, 200, 100, 64),\n",
       " (0.1, 3, 1000000.0, 200, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 200, 128),\n",
       " (1, 1, 100000.0, 100, 100, 64),\n",
       " (1, 1, 100000.0, 100, 100, 128),\n",
       " (1, 1, 100000.0, 100, 200, 64),\n",
       " (1, 1, 100000.0, 100, 200, 128),\n",
       " (1, 1, 100000.0, 150, 100, 64),\n",
       " (1, 1, 100000.0, 150, 100, 128),\n",
       " (1, 1, 100000.0, 150, 200, 64),\n",
       " (1, 1, 100000.0, 150, 200, 128),\n",
       " (1, 1, 100000.0, 200, 100, 64),\n",
       " (1, 1, 100000.0, 200, 100, 128),\n",
       " (1, 1, 100000.0, 200, 200, 64),\n",
       " (1, 1, 100000.0, 200, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 100, 128),\n",
       " (1, 1, 1000000.0, 100, 200, 64),\n",
       " (1, 1, 1000000.0, 100, 200, 128),\n",
       " (1, 1, 1000000.0, 150, 100, 64),\n",
       " (1, 1, 1000000.0, 150, 100, 128),\n",
       " (1, 1, 1000000.0, 150, 200, 64),\n",
       " (1, 1, 1000000.0, 150, 200, 128),\n",
       " (1, 1, 1000000.0, 200, 100, 64),\n",
       " (1, 1, 1000000.0, 200, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 200, 128),\n",
       " (1, 2, 100000.0, 100, 100, 64),\n",
       " (1, 2, 100000.0, 100, 100, 128),\n",
       " (1, 2, 100000.0, 100, 200, 64),\n",
       " (1, 2, 100000.0, 100, 200, 128),\n",
       " (1, 2, 100000.0, 150, 100, 64),\n",
       " (1, 2, 100000.0, 150, 100, 128),\n",
       " (1, 2, 100000.0, 150, 200, 64),\n",
       " (1, 2, 100000.0, 150, 200, 128),\n",
       " (1, 2, 100000.0, 200, 100, 64),\n",
       " (1, 2, 100000.0, 200, 100, 128),\n",
       " (1, 2, 100000.0, 200, 200, 64),\n",
       " (1, 2, 100000.0, 200, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 100, 128),\n",
       " (1, 2, 1000000.0, 100, 200, 64),\n",
       " (1, 2, 1000000.0, 100, 200, 128),\n",
       " (1, 2, 1000000.0, 150, 100, 64),\n",
       " (1, 2, 1000000.0, 150, 100, 128),\n",
       " (1, 2, 1000000.0, 150, 200, 64),\n",
       " (1, 2, 1000000.0, 150, 200, 128),\n",
       " (1, 2, 1000000.0, 200, 100, 64),\n",
       " (1, 2, 1000000.0, 200, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 200, 128),\n",
       " (1, 3, 100000.0, 100, 100, 64),\n",
       " (1, 3, 100000.0, 100, 100, 128),\n",
       " (1, 3, 100000.0, 100, 200, 64),\n",
       " (1, 3, 100000.0, 100, 200, 128),\n",
       " (1, 3, 100000.0, 150, 100, 64),\n",
       " (1, 3, 100000.0, 150, 100, 128),\n",
       " (1, 3, 100000.0, 150, 200, 64),\n",
       " (1, 3, 100000.0, 150, 200, 128),\n",
       " (1, 3, 100000.0, 200, 100, 64),\n",
       " (1, 3, 100000.0, 200, 100, 128),\n",
       " (1, 3, 100000.0, 200, 200, 64),\n",
       " (1, 3, 100000.0, 200, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 100, 128),\n",
       " (1, 3, 1000000.0, 100, 200, 64),\n",
       " (1, 3, 1000000.0, 100, 200, 128),\n",
       " (1, 3, 1000000.0, 150, 100, 64),\n",
       " (1, 3, 1000000.0, 150, 100, 128),\n",
       " (1, 3, 1000000.0, 150, 200, 64),\n",
       " (1, 3, 1000000.0, 150, 200, 128),\n",
       " (1, 3, 1000000.0, 200, 100, 64),\n",
       " (1, 3, 1000000.0, 200, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 200, 128),\n",
       " (2, 1, 100000.0, 100, 100, 64),\n",
       " (2, 1, 100000.0, 100, 100, 128),\n",
       " (2, 1, 100000.0, 100, 200, 64),\n",
       " (2, 1, 100000.0, 100, 200, 128),\n",
       " (2, 1, 100000.0, 150, 100, 64),\n",
       " (2, 1, 100000.0, 150, 100, 128),\n",
       " (2, 1, 100000.0, 150, 200, 64),\n",
       " (2, 1, 100000.0, 150, 200, 128),\n",
       " (2, 1, 100000.0, 200, 100, 64),\n",
       " (2, 1, 100000.0, 200, 100, 128),\n",
       " (2, 1, 100000.0, 200, 200, 64),\n",
       " (2, 1, 100000.0, 200, 200, 128),\n",
       " (2, 1, 1000000.0, 100, 100, 64),\n",
       " (2, 1, 1000000.0, 100, 100, 128),\n",
       " (2, 1, 1000000.0, 100, 200, 64),\n",
       " (2, 1, 1000000.0, 100, 200, 128),\n",
       " (2, 1, 1000000.0, 150, 100, 64),\n",
       " (2, 1, 1000000.0, 150, 100, 128),\n",
       " (2, 1, 1000000.0, 150, 200, 64),\n",
       " (2, 1, 1000000.0, 150, 200, 128),\n",
       " (2, 1, 1000000.0, 200, 100, 64),\n",
       " (2, 1, 1000000.0, 200, 100, 128),\n",
       " (2, 1, 1000000.0, 200, 200, 64),\n",
       " (2, 1, 1000000.0, 200, 200, 128),\n",
       " (2, 2, 100000.0, 100, 100, 64),\n",
       " (2, 2, 100000.0, 100, 100, 128),\n",
       " (2, 2, 100000.0, 100, 200, 64),\n",
       " (2, 2, 100000.0, 100, 200, 128),\n",
       " (2, 2, 100000.0, 150, 100, 64),\n",
       " (2, 2, 100000.0, 150, 100, 128),\n",
       " (2, 2, 100000.0, 150, 200, 64),\n",
       " (2, 2, 100000.0, 150, 200, 128),\n",
       " (2, 2, 100000.0, 200, 100, 64),\n",
       " (2, 2, 100000.0, 200, 100, 128),\n",
       " (2, 2, 100000.0, 200, 200, 64),\n",
       " (2, 2, 100000.0, 200, 200, 128),\n",
       " (2, 2, 1000000.0, 100, 100, 64),\n",
       " (2, 2, 1000000.0, 100, 100, 128),\n",
       " (2, 2, 1000000.0, 100, 200, 64),\n",
       " (2, 2, 1000000.0, 100, 200, 128),\n",
       " (2, 2, 1000000.0, 150, 100, 64),\n",
       " (2, 2, 1000000.0, 150, 100, 128),\n",
       " (2, 2, 1000000.0, 150, 200, 64),\n",
       " (2, 2, 1000000.0, 150, 200, 128),\n",
       " (2, 2, 1000000.0, 200, 100, 64),\n",
       " (2, 2, 1000000.0, 200, 100, 128),\n",
       " (2, 2, 1000000.0, 200, 200, 64),\n",
       " (2, 2, 1000000.0, 200, 200, 128),\n",
       " (2, 3, 100000.0, 100, 100, 64),\n",
       " (2, 3, 100000.0, 100, 100, 128),\n",
       " (2, 3, 100000.0, 100, 200, 64),\n",
       " (2, 3, 100000.0, 100, 200, 128),\n",
       " (2, 3, 100000.0, 150, 100, 64),\n",
       " (2, 3, 100000.0, 150, 100, 128),\n",
       " (2, 3, 100000.0, 150, 200, 64),\n",
       " (2, 3, 100000.0, 150, 200, 128),\n",
       " (2, 3, 100000.0, 200, 100, 64),\n",
       " (2, 3, 100000.0, 200, 100, 128),\n",
       " (2, 3, 100000.0, 200, 200, 64),\n",
       " (2, 3, 100000.0, 200, 200, 128),\n",
       " (2, 3, 1000000.0, 100, 100, 64),\n",
       " (2, 3, 1000000.0, 100, 100, 128),\n",
       " (2, 3, 1000000.0, 100, 200, 64),\n",
       " (2, 3, 1000000.0, 100, 200, 128),\n",
       " (2, 3, 1000000.0, 150, 100, 64),\n",
       " (2, 3, 1000000.0, 150, 100, 128),\n",
       " (2, 3, 1000000.0, 150, 200, 64),\n",
       " (2, 3, 1000000.0, 150, 200, 128),\n",
       " (2, 3, 1000000.0, 200, 100, 64),\n",
       " (2, 3, 1000000.0, 200, 100, 128),\n",
       " (2, 3, 1000000.0, 200, 200, 64),\n",
       " (2, 3, 1000000.0, 200, 200, 128)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [[1e-2,1e-1,1,2], ## learning rates\n",
    "          list(range(1,4)), ## ngrams\n",
    "          [1e5,1e6], ## vocab size\n",
    "          [100,150,200], ## embedding size\n",
    "          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "# params = [[1e-1,1,2,5], ## learning rates\n",
    "#           list(range(1,2)), ## ngrams\n",
    "#           [1e5], ## vocab size\n",
    "#           [100], ## embedding size\n",
    "#           [100], ## max sentence length\n",
    "#           [64] ## batch size\n",
    "#          ]\n",
    "\n",
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 288\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 56609 -> token verifies\n",
      "Token verifies -> token id 56609\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token2id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-369e3b971405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0moptimizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                           lemmatize = False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-dbb916a178f6>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(hyperparameter_space, epochs, optimizer_name, lemmatize)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Token {} -> token id {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtrain_data_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken2index_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mval_data_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken2index_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# double checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-866ed63b363d>\u001b[0m in \u001b[0;36mtoken2index_dataset\u001b[0;34m(tokens_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mindex_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken2id\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mUNK_IDX\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mindices_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-866ed63b363d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mindex_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken2id\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mUNK_IDX\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mindices_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token2id' is not defined"
     ]
    }
   ],
   "source": [
    "param_val_losses_adam = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"Adam\",\n",
    "                                          lemmatize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 288\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 23185 -> token togetherness\n",
      "Token togetherness -> token id 23185\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/10],Step:[51/313],Training Acc:50.615,Validation Acc:50.52\n",
      "Epoch:[1/10],Step:[101/313],Training Acc:51.515,Validation Acc:51.28\n",
      "Epoch:[1/10],Step:[151/313],Training Acc:54.055,Validation Acc:53.14\n",
      "Epoch:[1/10],Step:[201/313],Training Acc:53.91,Validation Acc:52.62\n",
      "Epoch:[1/10],Step:[251/313],Training Acc:52.5,Validation Acc:52.2\n",
      "Epoch:[1/10],Step:[301/313],Training Acc:55.425,Validation Acc:54.36\n",
      "Epoch:[2/10],Step:[51/313],Training Acc:56.83,Validation Acc:55.24\n",
      "Epoch:[2/10],Step:[101/313],Training Acc:55.03,Validation Acc:54.42\n",
      "Epoch:[2/10],Step:[151/313],Training Acc:56.895,Validation Acc:55.96\n",
      "Epoch:[2/10],Step:[201/313],Training Acc:57.72,Validation Acc:56.86\n",
      "Epoch:[2/10],Step:[251/313],Training Acc:58.44,Validation Acc:57.42\n",
      "Epoch:[2/10],Step:[301/313],Training Acc:58.355,Validation Acc:57.46\n",
      "Epoch:[3/10],Step:[51/313],Training Acc:58.615,Validation Acc:57.66\n",
      "Epoch:[3/10],Step:[101/313],Training Acc:58.505,Validation Acc:57.78\n",
      "Epoch:[3/10],Step:[151/313],Training Acc:58.88,Validation Acc:58.22\n",
      "Epoch:[3/10],Step:[201/313],Training Acc:59.23,Validation Acc:58.3\n",
      "Epoch:[3/10],Step:[251/313],Training Acc:59.595,Validation Acc:58.74\n",
      "Epoch:[3/10],Step:[301/313],Training Acc:59.185,Validation Acc:58.3\n",
      "Epoch:[4/10],Step:[51/313],Training Acc:59.995,Validation Acc:59.48\n",
      "Epoch:[4/10],Step:[101/313],Training Acc:59.865,Validation Acc:59.12\n",
      "Epoch:[4/10],Step:[151/313],Training Acc:59.795,Validation Acc:59.22\n",
      "Epoch:[4/10],Step:[201/313],Training Acc:59.69,Validation Acc:58.78\n",
      "Epoch:[4/10],Step:[251/313],Training Acc:60.39,Validation Acc:59.8\n",
      "Epoch:[4/10],Step:[301/313],Training Acc:59.87,Validation Acc:59.18\n",
      "Epoch:[5/10],Step:[51/313],Training Acc:60.595,Validation Acc:60.14\n",
      "Epoch:[5/10],Step:[101/313],Training Acc:60.86,Validation Acc:59.98\n",
      "Epoch:[5/10],Step:[151/313],Training Acc:60.54,Validation Acc:60.14\n",
      "Epoch:[5/10],Step:[201/313],Training Acc:60.265,Validation Acc:59.48\n",
      "Epoch:[5/10],Step:[251/313],Training Acc:60.69,Validation Acc:60.28\n",
      "Epoch:[5/10],Step:[301/313],Training Acc:61.055,Validation Acc:60.66\n",
      "Epoch:[6/10],Step:[51/313],Training Acc:61.04,Validation Acc:60.7\n",
      "Epoch:[6/10],Step:[101/313],Training Acc:60.79,Validation Acc:60.44\n",
      "Epoch:[6/10],Step:[151/313],Training Acc:61.13,Validation Acc:60.86\n",
      "Epoch:[6/10],Step:[201/313],Training Acc:61.32,Validation Acc:60.98\n",
      "Epoch:[6/10],Step:[251/313],Training Acc:61.25,Validation Acc:61.02\n",
      "Epoch:[6/10],Step:[301/313],Training Acc:61.47,Validation Acc:61.12\n",
      "Epoch:[7/10],Step:[51/313],Training Acc:61.55,Validation Acc:61.04\n",
      "Epoch:[7/10],Step:[101/313],Training Acc:61.57,Validation Acc:61.14\n",
      "Epoch:[7/10],Step:[151/313],Training Acc:61.525,Validation Acc:60.96\n",
      "Epoch:[7/10],Step:[201/313],Training Acc:61.75,Validation Acc:61.56\n",
      "Epoch:[7/10],Step:[251/313],Training Acc:61.85,Validation Acc:61.58\n",
      "Epoch:[7/10],Step:[301/313],Training Acc:61.825,Validation Acc:61.5\n",
      "Epoch:[8/10],Step:[51/313],Training Acc:61.975,Validation Acc:61.56\n",
      "Epoch:[8/10],Step:[101/313],Training Acc:61.94,Validation Acc:61.4\n",
      "Epoch:[8/10],Step:[151/313],Training Acc:61.995,Validation Acc:61.18\n",
      "Epoch:[8/10],Step:[201/313],Training Acc:62.025,Validation Acc:61.64\n",
      "Epoch:[8/10],Step:[251/313],Training Acc:62.09,Validation Acc:61.7\n",
      "Epoch:[8/10],Step:[301/313],Training Acc:62.195,Validation Acc:61.76\n",
      "Epoch:[9/10],Step:[51/313],Training Acc:62.29,Validation Acc:61.76\n",
      "Epoch:[9/10],Step:[101/313],Training Acc:62.325,Validation Acc:61.82\n",
      "Epoch:[9/10],Step:[151/313],Training Acc:62.31,Validation Acc:61.44\n",
      "Epoch:[9/10],Step:[201/313],Training Acc:62.19,Validation Acc:61.7\n",
      "Epoch:[9/10],Step:[251/313],Training Acc:62.24,Validation Acc:61.48\n",
      "Epoch:[9/10],Step:[301/313],Training Acc:62.525,Validation Acc:61.98\n",
      "Epoch:[10/10],Step:[51/313],Training Acc:62.29,Validation Acc:61.66\n",
      "Epoch:[10/10],Step:[101/313],Training Acc:62.525,Validation Acc:61.96\n",
      "Epoch:[10/10],Step:[151/313],Training Acc:62.655,Validation Acc:62.08\n",
      "Epoch:[10/10],Step:[201/313],Training Acc:62.44,Validation Acc:61.88\n",
      "Epoch:[10/10],Step:[251/313],Training Acc:62.395,Validation Acc:61.56\n",
      "Epoch:[10/10],Step:[301/313],Training Acc:62.67,Validation Acc:62.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 2 / 288\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 40239 -> token mnard\n",
      "Token mnard -> token id 40239\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-32cab302fb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0moptimizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SGD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                           lemmatize = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-356-dbb916a178f6>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(hyperparameter_space, epochs, optimizer_name, lemmatize)\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0;31m# Accuracy Calculations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                     \u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_comb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-7e5a5651d850>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-3c5a3717ed55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, length)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m         return F.embedding(\n\u001b[1;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_val_losses_sgd = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"SGD\",\n",
    "                                          lemmatize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 1, 100000.0, 100, 100, 64) [82.26, 83.48, 84.34, 85.2, 83.78, 85.28, 84.88, 82.8, 84.84, 84.4, 83.16, 84.8]\n",
      "(0.1, 1, 100000.0, 100, 100, 128) [83.24, 78.62, 84.92, 85.38, 85.7, 85.4, 85.28, 84.44, 85.2, 84.86, 85.4, 85.96]\n",
      "(1, 1, 100000.0, 100, 100, 64) [78.52, 79.34, 64.98, 82.32, 79.98, 79.36, 75.4, 79.26, 80.8, 75.32, 80.42, 81.56]\n",
      "(1, 1, 100000.0, 100, 100, 128) [51.9, 52.48, 75.2, 82.9, 75.08, 73.48, 84.02, 84.46, 80.56, 83.08, 83.2, 80.08]\n",
      "(2, 1, 100000.0, 100, 100, 64) [78.58, 52.7, 79.3, 77.6, 69.44, 78.26, 81.92, 81.8, 73.88, 74.9, 79.58, 79.26]\n",
      "(2, 1, 100000.0, 100, 100, 128) [68.06, 67.5, 80.56, 68.62, 82.84, 83.54, 80.02, 82.04, 82.1, 81.46, 83.42, 83.66]\n",
      "(5, 1, 100000.0, 100, 100, 64) [81.5, 76.28, 70.52, 56.0, 74.78, 72.3, 71.08, 79.52, 82.86, 81.08, 74.76, 77.86]\n",
      "(5, 1, 100000.0, 100, 100, 128) [77.3, 71.52, 82.48, 81.4, 75.44, 82.62, 78.06, 79.76, 81.08, 83.5, 74.34, 81.64]\n"
     ]
    }
   ],
   "source": [
    "for key, value in param_val_losses_adam.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in param_val_losses_sgd.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
