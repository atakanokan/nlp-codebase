{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses bag-of-ngrams approach to sentiment classification using the IMDB review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"data/imdb_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "    file_list = os.listdir(folder_path)\n",
    "    # for debugging, printing out the folder path and some files in it\n",
    "    print(folder_path)\n",
    "    print(file_list[:10])\n",
    "    \n",
    "    all_reviews = []\n",
    "    for file_path in file_list:\n",
    "        f = open(folder_path + file_path,\"r\")\n",
    "        all_reviews.append(f.readline())\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_reviews/train/pos/\n",
      "['4715_9.txt', '12390_8.txt', '8329_7.txt', '9063_8.txt', '3092_10.txt', '9865_8.txt', '6639_10.txt', '10460_10.txt', '10331_10.txt', '11606_10.txt']\n",
      "12500\n",
      "data/imdb_reviews/train/neg/\n",
      "['1821_4.txt', '10402_1.txt', '1062_4.txt', '9056_1.txt', '5392_3.txt', '2682_3.txt', '3351_4.txt', '399_2.txt', '10447_1.txt', '10096_1.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/pos/\n",
      "['4715_9.txt', '1930_9.txt', '3205_9.txt', '10186_10.txt', '147_10.txt', '7511_7.txt', '616_10.txt', '10460_10.txt', '3240_9.txt', '1975_9.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/neg/\n",
      "['1821_4.txt', '9487_1.txt', '4604_4.txt', '2828_2.txt', '10890_1.txt', '3351_4.txt', '8070_2.txt', '1027_4.txt', '8248_3.txt', '4290_4.txt']\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "print(len(train_pos))\n",
    "train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "print(len(train_neg))\n",
    "test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "print(len(test_pos))\n",
    "test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)\n",
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive examples = 12500\n",
      "Train Negative examples = 12500\n",
      "Test Positive examples = 12500\n",
      "Test Negative examples = 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "train_pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the labels of the test set for Test Error Measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "print(len(test_data_labels))\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "test_neg_clean = [cleanhtml(x) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing dots & question marks & paranthesis with space\n",
    "\n",
    "It seems that punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"asdasdasds.asdasda\".replace(\".\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_dqmp(review):\n",
    "    \n",
    "#     review = review.replace(\".\",\" \")\n",
    "#     review = review.replace(\"?\",\" \")\n",
    "#     review = review.replace(\")\",\" \")\n",
    "#     review = review.replace(\"(\",\" \")\n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove_dqmp(train_pos_clean[random_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_clean = [remove_dqmp(x) for x in train_pos_clean]\n",
    "# train_neg_clean = [remove_dqmp(x) for x in train_neg_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# This is word tokenizer\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# Modified for n-grams\n",
    "def tokenize(sent, n_gram = 0):\n",
    "    \n",
    "    tokens = tokenizer(sent)\n",
    "    \n",
    "    # unigrams\n",
    "    unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    output = []\n",
    "    output.extend(unigrams)\n",
    "    \n",
    "    n = 2\n",
    "    while n <= n_gram:\n",
    "        ngram_tokens = [\" \".join(unigrams[x:x+n]) \\\n",
    "                            for x in range(len(unigrams)-n+1)]\n",
    "        output.extend(ngram_tokens)\n",
    "        n = n + 1\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9262\n"
     ]
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maybe i identify with this film cause i live in nyc and suffer from bad insomnia but whatever it is, i must praise the filmmaker on a most amazing job. to do what she did with no budget...wow, thats all i can say. really, really good. like no money was spent on this film and it still blew me away. i definitley suggest checking it out if you can. great directing, fantastic score and of course a script that will knock you on your arse. see it.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'identify', 'with', 'this', 'film', 'cause', 'i', 'live', 'in', 'nyc', 'and', 'suffer', 'from', 'bad', 'insomnia', 'but', 'whatever', 'it', 'is', 'i', 'must', 'praise', 'the', 'filmmaker', 'on', 'a', 'most', 'amazing', 'job', 'to', 'do', 'what', 'she', 'did', 'with', 'no', 'budget', '...', 'wow', 'that', 's', 'all', 'i', 'can', 'say', 'really', 'really', 'good', 'like', 'no', 'money', 'was', 'spent', 'on', 'this', 'film', 'and', 'it', 'still', 'blew', 'me', 'away', 'i', 'definitley', 'suggest', 'checking', 'it', 'out', 'if', 'you', 'can', 'great', 'directing', 'fantastic', 'score', 'and', 'of', 'course', 'a', 'script', 'that', 'will', 'knock', 'you', 'on', 'your', 'arse', 'see', 'it', 'maybe i', 'i identify', 'identify with', 'with this', 'this film', 'film cause', 'cause i', 'i live', 'live in', 'in nyc', 'nyc and', 'and suffer', 'suffer from', 'from bad', 'bad insomnia', 'insomnia but', 'but whatever', 'whatever it', 'it is', 'is i', 'i must', 'must praise', 'praise the', 'the filmmaker', 'filmmaker on', 'on a', 'a most', 'most amazing', 'amazing job', 'job to', 'to do', 'do what', 'what she', 'she did', 'did with', 'with no', 'no budget', 'budget ...', '... wow', 'wow that', 'that s', 's all', 'all i', 'i can', 'can say', 'say really', 'really really', 'really good', 'good like', 'like no', 'no money', 'money was', 'was spent', 'spent on', 'on this', 'this film', 'film and', 'and it', 'it still', 'still blew', 'blew me', 'me away', 'away i', 'i definitley', 'definitley suggest', 'suggest checking', 'checking it', 'it out', 'out if', 'if you', 'you can', 'can great', 'great directing', 'directing fantastic', 'fantastic score', 'score and', 'and of', 'of course', 'course a', 'a script', 'script that', 'that will', 'will knock', 'knock you', 'you on', 'on your', 'your arse', 'arse see', 'see it', 'maybe i identify', 'i identify with', 'identify with this', 'with this film', 'this film cause', 'film cause i', 'cause i live', 'i live in', 'live in nyc', 'in nyc and', 'nyc and suffer', 'and suffer from', 'suffer from bad', 'from bad insomnia', 'bad insomnia but', 'insomnia but whatever', 'but whatever it', 'whatever it is', 'it is i', 'is i must', 'i must praise', 'must praise the', 'praise the filmmaker', 'the filmmaker on', 'filmmaker on a', 'on a most', 'a most amazing', 'most amazing job', 'amazing job to', 'job to do', 'to do what', 'do what she', 'what she did', 'she did with', 'did with no', 'with no budget', 'no budget ...', 'budget ... wow', '... wow that', 'wow that s', 'that s all', 's all i', 'all i can', 'i can say', 'can say really', 'say really really', 'really really good', 'really good like', 'good like no', 'like no money', 'no money was', 'money was spent', 'was spent on', 'spent on this', 'on this film', 'this film and', 'film and it', 'and it still', 'it still blew', 'still blew me', 'blew me away', 'me away i', 'away i definitley', 'i definitley suggest', 'definitley suggest checking', 'suggest checking it', 'checking it out', 'it out if', 'out if you', 'if you can', 'you can great', 'can great directing', 'great directing fantastic', 'directing fantastic score', 'fantastic score and', 'score and of', 'and of course', 'of course a', 'course a script', 'a script that', 'script that will', 'that will knock', 'will knock you', 'knock you on', 'you on your', 'on your arse', 'your arse see', 'arse see it', 'maybe i identify with', 'i identify with this', 'identify with this film', 'with this film cause', 'this film cause i', 'film cause i live', 'cause i live in', 'i live in nyc', 'live in nyc and', 'in nyc and suffer', 'nyc and suffer from', 'and suffer from bad', 'suffer from bad insomnia', 'from bad insomnia but', 'bad insomnia but whatever', 'insomnia but whatever it', 'but whatever it is', 'whatever it is i', 'it is i must', 'is i must praise', 'i must praise the', 'must praise the filmmaker', 'praise the filmmaker on', 'the filmmaker on a', 'filmmaker on a most', 'on a most amazing', 'a most amazing job', 'most amazing job to', 'amazing job to do', 'job to do what', 'to do what she', 'do what she did', 'what she did with', 'she did with no', 'did with no budget', 'with no budget ...', 'no budget ... wow', 'budget ... wow that', '... wow that s', 'wow that s all', 'that s all i', 's all i can', 'all i can say', 'i can say really', 'can say really really', 'say really really good', 'really really good like', 'really good like no', 'good like no money', 'like no money was', 'no money was spent', 'money was spent on', 'was spent on this', 'spent on this film', 'on this film and', 'this film and it', 'film and it still', 'and it still blew', 'it still blew me', 'still blew me away', 'blew me away i', 'me away i definitley', 'away i definitley suggest', 'i definitley suggest checking', 'definitley suggest checking it', 'suggest checking it out', 'checking it out if', 'it out if you', 'out if you can', 'if you can great', 'you can great directing', 'can great directing fantastic', 'great directing fantastic score', 'directing fantastic score and', 'fantastic score and of', 'score and of course', 'and of course a', 'of course a script', 'course a script that', 'a script that will', 'script that will knock', 'that will knock you', 'will knock you on', 'knock you on your', 'you on your arse', 'on your arse see', 'your arse see it']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tokens = tokenize(train_pos_clean[random_text], n_gram = 4)\n",
    "#tokens = tokenize(train_pos_clean[random_text])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_clean = train_pos_clean + train_neg_clean\n",
    "len(train_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_clean = test_pos_clean + test_neg_clean\n",
    "len(test_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training -> Training + Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be smaller than 25000\n",
    "training_size = 20000\n",
    "\n",
    "assert training_size < 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[15821 15685  4147 ... 18888 20316 23805]\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15821, 15685,  4147, ..., 17207, 22378, 14852])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_index[:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, n_gram):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], \n",
    "#                                                batch_size=512, \n",
    "#                                                n_threads=4)):\n",
    "\n",
    "    itr = 0\n",
    "    for sample in dataset:\n",
    "        \n",
    "        if itr % 50 == 0:\n",
    "            print(str(itr) + \" / \" + str(len(dataset)))\n",
    "        # unigram version\n",
    "        #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        # n-gram version\n",
    "        tokens = tokenize(sample,n_gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "        itr = itr + 1\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n"
     ]
    }
   ],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram = 2)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                     n_gram = 2)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n",
      "0 / 25000\n",
      "50 / 25000\n",
      "100 / 25000\n",
      "150 / 25000\n",
      "200 / 25000\n",
      "250 / 25000\n",
      "300 / 25000\n",
      "350 / 25000\n",
      "400 / 25000\n",
      "450 / 25000\n",
      "500 / 25000\n",
      "550 / 25000\n",
      "600 / 25000\n",
      "650 / 25000\n",
      "700 / 25000\n",
      "750 / 25000\n",
      "800 / 25000\n",
      "850 / 25000\n",
      "900 / 25000\n",
      "950 / 25000\n",
      "1000 / 25000\n",
      "1050 / 25000\n",
      "1100 / 25000\n",
      "1150 / 25000\n",
      "1200 / 25000\n",
      "1250 / 25000\n",
      "1300 / 25000\n",
      "1350 / 25000\n",
      "1400 / 25000\n",
      "1450 / 25000\n",
      "1500 / 25000\n",
      "1550 / 25000\n",
      "1600 / 25000\n",
      "1650 / 25000\n",
      "1700 / 25000\n",
      "1750 / 25000\n",
      "1800 / 25000\n",
      "1850 / 25000\n",
      "1900 / 25000\n",
      "1950 / 25000\n",
      "2000 / 25000\n",
      "2050 / 25000\n",
      "2100 / 25000\n",
      "2150 / 25000\n",
      "2200 / 25000\n",
      "2250 / 25000\n",
      "2300 / 25000\n",
      "2350 / 25000\n",
      "2400 / 25000\n",
      "2450 / 25000\n",
      "2500 / 25000\n",
      "2550 / 25000\n",
      "2600 / 25000\n",
      "2650 / 25000\n",
      "2700 / 25000\n",
      "2750 / 25000\n",
      "2800 / 25000\n",
      "2850 / 25000\n",
      "2900 / 25000\n",
      "2950 / 25000\n",
      "3000 / 25000\n",
      "3050 / 25000\n",
      "3100 / 25000\n",
      "3150 / 25000\n",
      "3200 / 25000\n",
      "3250 / 25000\n",
      "3300 / 25000\n",
      "3350 / 25000\n",
      "3400 / 25000\n",
      "3450 / 25000\n",
      "3500 / 25000\n",
      "3550 / 25000\n",
      "3600 / 25000\n",
      "3650 / 25000\n",
      "3700 / 25000\n",
      "3750 / 25000\n",
      "3800 / 25000\n",
      "3850 / 25000\n",
      "3900 / 25000\n",
      "3950 / 25000\n",
      "4000 / 25000\n",
      "4050 / 25000\n",
      "4100 / 25000\n",
      "4150 / 25000\n",
      "4200 / 25000\n",
      "4250 / 25000\n",
      "4300 / 25000\n",
      "4350 / 25000\n",
      "4400 / 25000\n",
      "4450 / 25000\n",
      "4500 / 25000\n",
      "4550 / 25000\n",
      "4600 / 25000\n",
      "4650 / 25000\n",
      "4700 / 25000\n",
      "4750 / 25000\n",
      "4800 / 25000\n",
      "4850 / 25000\n",
      "4900 / 25000\n",
      "4950 / 25000\n",
      "5000 / 25000\n",
      "5050 / 25000\n",
      "5100 / 25000\n",
      "5150 / 25000\n",
      "5200 / 25000\n",
      "5250 / 25000\n",
      "5300 / 25000\n",
      "5350 / 25000\n",
      "5400 / 25000\n",
      "5450 / 25000\n",
      "5500 / 25000\n",
      "5550 / 25000\n",
      "5600 / 25000\n",
      "5650 / 25000\n",
      "5700 / 25000\n",
      "5750 / 25000\n",
      "5800 / 25000\n",
      "5850 / 25000\n",
      "5900 / 25000\n",
      "5950 / 25000\n",
      "6000 / 25000\n",
      "6050 / 25000\n",
      "6100 / 25000\n",
      "6150 / 25000\n",
      "6200 / 25000\n",
      "6250 / 25000\n",
      "6300 / 25000\n",
      "6350 / 25000\n",
      "6400 / 25000\n",
      "6450 / 25000\n",
      "6500 / 25000\n",
      "6550 / 25000\n",
      "6600 / 25000\n",
      "6650 / 25000\n",
      "6700 / 25000\n",
      "6750 / 25000\n",
      "6800 / 25000\n",
      "6850 / 25000\n",
      "6900 / 25000\n",
      "6950 / 25000\n",
      "7000 / 25000\n",
      "7050 / 25000\n",
      "7100 / 25000\n",
      "7150 / 25000\n",
      "7200 / 25000\n",
      "7250 / 25000\n",
      "7300 / 25000\n",
      "7350 / 25000\n",
      "7400 / 25000\n",
      "7450 / 25000\n",
      "7500 / 25000\n",
      "7550 / 25000\n",
      "7600 / 25000\n",
      "7650 / 25000\n",
      "7700 / 25000\n",
      "7750 / 25000\n",
      "7800 / 25000\n",
      "7850 / 25000\n",
      "7900 / 25000\n",
      "7950 / 25000\n",
      "8000 / 25000\n",
      "8050 / 25000\n",
      "8100 / 25000\n",
      "8150 / 25000\n",
      "8200 / 25000\n",
      "8250 / 25000\n",
      "8300 / 25000\n",
      "8350 / 25000\n",
      "8400 / 25000\n",
      "8450 / 25000\n",
      "8500 / 25000\n",
      "8550 / 25000\n",
      "8600 / 25000\n",
      "8650 / 25000\n",
      "8700 / 25000\n",
      "8750 / 25000\n",
      "8800 / 25000\n",
      "8850 / 25000\n",
      "8900 / 25000\n",
      "8950 / 25000\n",
      "9000 / 25000\n",
      "9050 / 25000\n",
      "9100 / 25000\n",
      "9150 / 25000\n",
      "9200 / 25000\n",
      "9250 / 25000\n",
      "9300 / 25000\n",
      "9350 / 25000\n",
      "9400 / 25000\n",
      "9450 / 25000\n",
      "9500 / 25000\n",
      "9550 / 25000\n",
      "9600 / 25000\n",
      "9650 / 25000\n",
      "9700 / 25000\n",
      "9750 / 25000\n",
      "9800 / 25000\n",
      "9850 / 25000\n",
      "9900 / 25000\n",
      "9950 / 25000\n",
      "10000 / 25000\n",
      "10050 / 25000\n",
      "10100 / 25000\n",
      "10150 / 25000\n",
      "10200 / 25000\n",
      "10250 / 25000\n",
      "10300 / 25000\n",
      "10350 / 25000\n",
      "10400 / 25000\n",
      "10450 / 25000\n",
      "10500 / 25000\n",
      "10550 / 25000\n",
      "10600 / 25000\n",
      "10650 / 25000\n",
      "10700 / 25000\n",
      "10750 / 25000\n",
      "10800 / 25000\n",
      "10850 / 25000\n",
      "10900 / 25000\n",
      "10950 / 25000\n",
      "11000 / 25000\n",
      "11050 / 25000\n",
      "11100 / 25000\n",
      "11150 / 25000\n",
      "11200 / 25000\n",
      "11250 / 25000\n",
      "11300 / 25000\n",
      "11350 / 25000\n",
      "11400 / 25000\n",
      "11450 / 25000\n",
      "11500 / 25000\n",
      "11550 / 25000\n",
      "11600 / 25000\n",
      "11650 / 25000\n",
      "11700 / 25000\n",
      "11750 / 25000\n",
      "11800 / 25000\n",
      "11850 / 25000\n",
      "11900 / 25000\n",
      "11950 / 25000\n",
      "12000 / 25000\n",
      "12050 / 25000\n",
      "12100 / 25000\n",
      "12150 / 25000\n",
      "12200 / 25000\n",
      "12250 / 25000\n",
      "12300 / 25000\n",
      "12350 / 25000\n",
      "12400 / 25000\n",
      "12450 / 25000\n",
      "12500 / 25000\n",
      "12550 / 25000\n",
      "12600 / 25000\n",
      "12650 / 25000\n",
      "12700 / 25000\n",
      "12750 / 25000\n",
      "12800 / 25000\n",
      "12850 / 25000\n",
      "12900 / 25000\n",
      "12950 / 25000\n",
      "13000 / 25000\n",
      "13050 / 25000\n",
      "13100 / 25000\n",
      "13150 / 25000\n",
      "13200 / 25000\n",
      "13250 / 25000\n",
      "13300 / 25000\n",
      "13350 / 25000\n",
      "13400 / 25000\n",
      "13450 / 25000\n",
      "13500 / 25000\n",
      "13550 / 25000\n",
      "13600 / 25000\n",
      "13650 / 25000\n",
      "13700 / 25000\n",
      "13750 / 25000\n",
      "13800 / 25000\n",
      "13850 / 25000\n",
      "13900 / 25000\n",
      "13950 / 25000\n",
      "14000 / 25000\n",
      "14050 / 25000\n",
      "14100 / 25000\n",
      "14150 / 25000\n",
      "14200 / 25000\n",
      "14250 / 25000\n",
      "14300 / 25000\n",
      "14350 / 25000\n",
      "14400 / 25000\n",
      "14450 / 25000\n",
      "14500 / 25000\n",
      "14550 / 25000\n",
      "14600 / 25000\n",
      "14650 / 25000\n",
      "14700 / 25000\n",
      "14750 / 25000\n",
      "14800 / 25000\n",
      "14850 / 25000\n",
      "14900 / 25000\n",
      "14950 / 25000\n",
      "15000 / 25000\n",
      "15050 / 25000\n",
      "15100 / 25000\n",
      "15150 / 25000\n",
      "15200 / 25000\n",
      "15250 / 25000\n",
      "15300 / 25000\n",
      "15350 / 25000\n",
      "15400 / 25000\n",
      "15450 / 25000\n",
      "15500 / 25000\n",
      "15550 / 25000\n",
      "15600 / 25000\n",
      "15650 / 25000\n",
      "15700 / 25000\n",
      "15750 / 25000\n",
      "15800 / 25000\n",
      "15850 / 25000\n",
      "15900 / 25000\n",
      "15950 / 25000\n",
      "16000 / 25000\n",
      "16050 / 25000\n",
      "16100 / 25000\n",
      "16150 / 25000\n",
      "16200 / 25000\n",
      "16250 / 25000\n",
      "16300 / 25000\n",
      "16350 / 25000\n",
      "16400 / 25000\n",
      "16450 / 25000\n",
      "16500 / 25000\n",
      "16550 / 25000\n",
      "16600 / 25000\n",
      "16650 / 25000\n",
      "16700 / 25000\n",
      "16750 / 25000\n",
      "16800 / 25000\n",
      "16850 / 25000\n",
      "16900 / 25000\n",
      "16950 / 25000\n",
      "17000 / 25000\n",
      "17050 / 25000\n",
      "17100 / 25000\n",
      "17150 / 25000\n",
      "17200 / 25000\n",
      "17250 / 25000\n",
      "17300 / 25000\n",
      "17350 / 25000\n",
      "17400 / 25000\n",
      "17450 / 25000\n",
      "17500 / 25000\n",
      "17550 / 25000\n",
      "17600 / 25000\n",
      "17650 / 25000\n",
      "17700 / 25000\n",
      "17750 / 25000\n",
      "17800 / 25000\n",
      "17850 / 25000\n",
      "17900 / 25000\n",
      "17950 / 25000\n",
      "18000 / 25000\n",
      "18050 / 25000\n",
      "18100 / 25000\n",
      "18150 / 25000\n",
      "18200 / 25000\n",
      "18250 / 25000\n",
      "18300 / 25000\n",
      "18350 / 25000\n",
      "18400 / 25000\n",
      "18450 / 25000\n",
      "18500 / 25000\n",
      "18550 / 25000\n",
      "18600 / 25000\n",
      "18650 / 25000\n",
      "18700 / 25000\n",
      "18750 / 25000\n",
      "18800 / 25000\n",
      "18850 / 25000\n",
      "18900 / 25000\n",
      "18950 / 25000\n",
      "19000 / 25000\n",
      "19050 / 25000\n",
      "19100 / 25000\n",
      "19150 / 25000\n",
      "19200 / 25000\n",
      "19250 / 25000\n",
      "19300 / 25000\n",
      "19350 / 25000\n",
      "19400 / 25000\n",
      "19450 / 25000\n",
      "19500 / 25000\n",
      "19550 / 25000\n",
      "19600 / 25000\n",
      "19650 / 25000\n",
      "19700 / 25000\n",
      "19750 / 25000\n",
      "19800 / 25000\n",
      "19850 / 25000\n",
      "19900 / 25000\n",
      "19950 / 25000\n",
      "20000 / 25000\n",
      "20050 / 25000\n",
      "20100 / 25000\n",
      "20150 / 25000\n",
      "20200 / 25000\n",
      "20250 / 25000\n",
      "20300 / 25000\n",
      "20350 / 25000\n",
      "20400 / 25000\n",
      "20450 / 25000\n",
      "20500 / 25000\n",
      "20550 / 25000\n",
      "20600 / 25000\n",
      "20650 / 25000\n",
      "20700 / 25000\n",
      "20750 / 25000\n",
      "20800 / 25000\n",
      "20850 / 25000\n",
      "20900 / 25000\n",
      "20950 / 25000\n",
      "21000 / 25000\n",
      "21050 / 25000\n",
      "21100 / 25000\n",
      "21150 / 25000\n",
      "21200 / 25000\n",
      "21250 / 25000\n",
      "21300 / 25000\n",
      "21350 / 25000\n",
      "21400 / 25000\n",
      "21450 / 25000\n",
      "21500 / 25000\n",
      "21550 / 25000\n",
      "21600 / 25000\n",
      "21650 / 25000\n",
      "21700 / 25000\n",
      "21750 / 25000\n",
      "21800 / 25000\n",
      "21850 / 25000\n",
      "21900 / 25000\n",
      "21950 / 25000\n",
      "22000 / 25000\n",
      "22050 / 25000\n",
      "22100 / 25000\n",
      "22150 / 25000\n",
      "22200 / 25000\n",
      "22250 / 25000\n",
      "22300 / 25000\n",
      "22350 / 25000\n",
      "22400 / 25000\n",
      "22450 / 25000\n",
      "22500 / 25000\n",
      "22550 / 25000\n",
      "22600 / 25000\n",
      "22650 / 25000\n",
      "22700 / 25000\n",
      "22750 / 25000\n",
      "22800 / 25000\n",
      "22850 / 25000\n",
      "22900 / 25000\n",
      "22950 / 25000\n",
      "23000 / 25000\n",
      "23050 / 25000\n",
      "23100 / 25000\n",
      "23150 / 25000\n",
      "23200 / 25000\n",
      "23250 / 25000\n",
      "23300 / 25000\n",
      "23350 / 25000\n",
      "23400 / 25000\n",
      "23450 / 25000\n",
      "23500 / 25000\n",
      "23550 / 25000\n",
      "23600 / 25000\n",
      "23650 / 25000\n",
      "23700 / 25000\n",
      "23750 / 25000\n",
      "23800 / 25000\n",
      "23850 / 25000\n",
      "23900 / 25000\n",
      "23950 / 25000\n",
      "24000 / 25000\n",
      "24050 / 25000\n",
      "24100 / 25000\n",
      "24150 / 25000\n",
      "24200 / 25000\n",
      "24250 / 25000\n",
      "24300 / 25000\n",
      "24350 / 25000\n",
      "24400 / 25000\n",
      "24450 / 25000\n",
      "24500 / 25000\n",
      "24550 / 25000\n",
      "24600 / 25000\n",
      "24650 / 25000\n",
      "24700 / 25000\n",
      "24750 / 25000\n",
      "24800 / 25000\n",
      "24850 / 25000\n",
      "24900 / 25000\n",
      "24950 / 25000\n"
     ]
    }
   ],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_all_clean,\n",
    "                                      n_gram = 2)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'surely', 'one', 'of', 'the', 'worst', 'films', 'ever', 'made', 'and', 'released', 'by', 'a', 'major', 'hollywood', 'studio', 'the', 'plot', 'is', 'simply', 'stupid', 'the', 'dialog', 'is', 'written', 'in', 'clichés', 'you', 'can', 'complete', 'a', 'great', 'many', 'sentences', 'in', 'the', 'script', 'because', 'of', 'this', 'the', 'acting', 'is', 'ridiculously', 'bad', 'especially', 'that', 'of', 'rod', 'cameron', 'the', 'choreography', 'is', 'silly', 'and', 'wholly', 'unerotic', 'one', 'can', 'only', 'pity', 'the', 'reviewer', 'who', 'saw', '23-year', 'old', 'yvonne', \"'s\", 'dance', 'as', 'sexual', 'it', \"'s\", 'merely', 'very', 'bad', 'choreography', 'the', 'ballet', 'scene', 'in', 'the', 'film', \"'s\", 'beginning', 'is', 'especially', 'ludicrous', 'if', 'you', 'are', 'into', 'bad', 'movies', 'and', 'enjoy', 'laughing', 'at', 'some', 'of', 'hollywood', \"'s\", 'turkeys', 'this', 'is', 'for', 'you', 'i', 'bought', 'the', 'colorized', 'version', 'on', 'vhs', 'making', 'the', 'movie', 'even', 'worse', 'yvonne', \"'s\", 'heavy', 'makeup', 'when', 'colored', 'has', 'her', 'looking', 'like', 'a', 'clown', 'all', 'the', 'time', 'and', 'she', \"'s\", 'the', 'best', 'part', 'of', 'this', 'film', 'what', 'a', 'way', 'to', 'launch', 'a', 'career', 'this is', 'is surely', 'surely one', 'one of', 'of the', 'the worst', 'worst films', 'films ever', 'ever made', 'made and', 'and released', 'released by', 'by a', 'a major', 'major hollywood', 'hollywood studio', 'studio the', 'the plot', 'plot is', 'is simply', 'simply stupid', 'stupid the', 'the dialog', 'dialog is', 'is written', 'written in', 'in clichés', 'clichés you', 'you can', 'can complete', 'complete a', 'a great', 'great many', 'many sentences', 'sentences in', 'in the', 'the script', 'script because', 'because of', 'of this', 'this the', 'the acting', 'acting is', 'is ridiculously', 'ridiculously bad', 'bad especially', 'especially that', 'that of', 'of rod', 'rod cameron', 'cameron the', 'the choreography', 'choreography is', 'is silly', 'silly and', 'and wholly', 'wholly unerotic', 'unerotic one', 'one can', 'can only', 'only pity', 'pity the', 'the reviewer', 'reviewer who', 'who saw', 'saw 23-year', '23-year old', 'old yvonne', \"yvonne 's\", \"'s dance\", 'dance as', 'as sexual', 'sexual it', \"it 's\", \"'s merely\", 'merely very', 'very bad', 'bad choreography', 'choreography the', 'the ballet', 'ballet scene', 'scene in', 'in the', 'the film', \"film 's\", \"'s beginning\", 'beginning is', 'is especially', 'especially ludicrous', 'ludicrous if', 'if you', 'you are', 'are into', 'into bad', 'bad movies', 'movies and', 'and enjoy', 'enjoy laughing', 'laughing at', 'at some', 'some of', 'of hollywood', \"hollywood 's\", \"'s turkeys\", 'turkeys this', 'this is', 'is for', 'for you', 'you i', 'i bought', 'bought the', 'the colorized', 'colorized version', 'version on', 'on vhs', 'vhs making', 'making the', 'the movie', 'movie even', 'even worse', 'worse yvonne', \"yvonne 's\", \"'s heavy\", 'heavy makeup', 'makeup when', 'when colored', 'colored has', 'has her', 'her looking', 'looking like', 'like a', 'a clown', 'clown all', 'all the', 'the time', 'time and', 'and she', \"she 's\", \"'s the\", 'the best', 'best part', 'part of', 'of this', 'this film', 'film what', 'what a', 'a way', 'way to', 'to launch', 'launch a', 'a career'], ['mabel', 'at', 'the', 'wheel', 'is', 'one', 'of', 'those', 'movies', 'with', 'a', 'behind', 'the', 'scenes', 'story', 'that', \"'s\", 'more', 'interesting', 'than', 'the', 'movie', 'itself', 'this', 'was', 'chaplin', \"'s\", 'tenth', 'comedy', 'for', 'keystone', 'during', 'his', 'year', 'of', 'apprenticeship', 'and', 'his', 'first', 'two', 'reeler', 'here', 'he', 'played', 'one', 'of', 'his', 'last', 'out', 'and', 'out', 'villain', 'roles', 'although', 'the', 'feature', 'length', 'tillie', \"'s\", 'punctured', 'romance', 'was', 'yet', 'to', 'come', 'and', 'it', 'also', 'marked', 'one', 'of', 'the', 'last', 'times', 'he', 'would', 'work', 'for', 'a', 'director', 'other', 'than', 'himself', 'in', 'fact', 'chaplin', \"'s\", 'conflicts', 'with', 'director', 'and', 'co', 'star', 'mabel', 'normand', 'almost', 'got', 'him', 'fired', 'from', 'the', 'studio', 'chaplin', 'had', \"n't\", 'gotten', 'along', 'with', 'his', 'earlier', 'directors', 'henry', 'lehrman', 'and', 'george', 'nichols', 'but', 'according', 'to', 'his', 'autobiography', 'having', 'to', 'take', 'direction', 'from', 'a', 'mere', 'girl', 'was', 'the', 'last', 'straw', 'charlie', 'and', 'mabel', 'argued', 'bitterly', 'during', 'the', 'making', 'of', 'this', 'film', 'chaplin', 'was', 'still', 'a', 'newcomer', 'at', 'keystone', 'and', 'his', 'colleagues', 'did', \"n't\", 'know', 'what', 'to', 'make', 'of', 'him', 'but', 'everyone', 'loved', 'mabel', 'producer', 'mack', 'sennett', 'was', 'on', 'the', 'verge', 'of', 'firing', 'chaplin', 'when', 'he', 'learned', 'that', 'the', 'newcomer', \"'s\", 'films', 'were', 'catching', 'on', 'and', 'exhibitors', 'wanted', 'more', 'of', 'them', 'a.s.a.p.', 'so', 'chaplin', 'was', 'promised', 'the', 'chance', 'to', 'direct', 'himself', 'in', 'return', 'for', 'finishing', 'this', 'movie', 'the', 'way', 'mabel', 'wanted', 'it', 'unfortunately', 'none', 'of', 'that', 'drama', 'is', 'visible', 'on', 'screen', 'in', 'mabel', 'at', 'the', 'wheel', 'which', 'looks', 'like', 'typical', 'keystone', 'chaos', 'the', 'story', 'concerns', 'an', 'auto', 'race', 'in', 'which', 'mabel', \"'s\", 'beau', 'harry', 'mccoy', 'is', 'scheduled', 'to', 'compete', 'but', 'wicked', 'charlie', 'and', 'his', 'henchmen', 'abduct', 'the', 'lad', 'and', 'mabel', 'must', 'take', 'the', 'wheel', 'in', 'his', 'place', 'for', 'all', 'the', 'racing', 'around', 'brick', 'hurling', 'and', 'finger', 'biting', 'the', 'film', 'is', 'frankly', 'short', 'on', 'laughs', 'but', 'there', 'are', 'a', 'few', 'points', 'of', 'interest', 'there', \"'s\", 'some', 'good', 'cinematography', 'and', 'editing', 'in', 'the', 'race', 'sequence', 'though', 'there', 'are', \"n't\", 'really', 'any', 'gags', 'just', 'lots', 'of', 'frantic', 'activity', 'chaplin', 'himself', 'looks', 'odd', 'sporting', 'a', 'goat', 'like', 'beard', 'on', 'his', 'chin', 'and', 'wearing', 'the', 'top', 'hat', 'and', 'frock', 'coat', 'he', 'wore', 'in', 'his', 'very', 'first', 'film', 'appearance', 'making', 'a', 'living', 'but', 'the', 'outfit', 'suits', 'the', 'old', 'fashioned', 'villainy', 'he', 'displays', 'throughout', 'at', 'least', 'it', \"'s\", 'novel', 'to', 'watch', 'him', 'play', 'such', 'an', 'uncharacteristic', 'role', 'visible', 'in', 'the', 'stands', 'at', 'the', 'race', 'track', 'are', 'such', 'keystone', 'stalwarts', 'as', 'chester', 'conklin', 'edgar', 'kennedy', 'in', 'a', 'strangely', 'dandified', 'get', 'up', 'and', 'a', 'more', 'characteristic', 'mack', 'sennett', 'spitting', 'tobacco', 'and', 'doing', 'his', 'usual', 'mindless', 'rube', 'routine', 'as', 'a', 'performer', 'sennett', 'was', 'about', 'as', 'subtle', 'as', 'the', 'movies', 'he', 'produced', 'but', 'you', 'have', 'to', 'give', 'the', 'guy', 'credit', 'he', 'knew', 'what', 'people', 'liked', 'these', 'films', 'were', 'hugely', 'popular', 'in', 'their', 'day', 'mack', \"'s\", 'performance', 'does', \"n't\", 'add', 'much', 'to', 'mabel', 'at', 'the', 'wheel', 'but', 'he', 'probably', 'had', 'to', 'be', 'on', 'hand', 'for', 'the', 'filming', 'of', 'this', 'one', 'to', 'make', 'sure', 'his', 'stars', 'did', \"n't\", 'kill', 'each', 'other', 'mabel at', 'at the', 'the wheel', 'wheel is', 'is one', 'one of', 'of those', 'those movies', 'movies with', 'with a', 'a behind', 'behind the', 'the scenes', 'scenes story', 'story that', \"that 's\", \"'s more\", 'more interesting', 'interesting than', 'than the', 'the movie', 'movie itself', 'itself this', 'this was', 'was chaplin', \"chaplin 's\", \"'s tenth\", 'tenth comedy', 'comedy for', 'for keystone', 'keystone during', 'during his', 'his year', 'year of', 'of apprenticeship', 'apprenticeship and', 'and his', 'his first', 'first two', 'two reeler', 'reeler here', 'here he', 'he played', 'played one', 'one of', 'of his', 'his last', 'last out', 'out and', 'and out', 'out villain', 'villain roles', 'roles although', 'although the', 'the feature', 'feature length', 'length tillie', \"tillie 's\", \"'s punctured\", 'punctured romance', 'romance was', 'was yet', 'yet to', 'to come', 'come and', 'and it', 'it also', 'also marked', 'marked one', 'one of', 'of the', 'the last', 'last times', 'times he', 'he would', 'would work', 'work for', 'for a', 'a director', 'director other', 'other than', 'than himself', 'himself in', 'in fact', 'fact chaplin', \"chaplin 's\", \"'s conflicts\", 'conflicts with', 'with director', 'director and', 'and co', 'co star', 'star mabel', 'mabel normand', 'normand almost', 'almost got', 'got him', 'him fired', 'fired from', 'from the', 'the studio', 'studio chaplin', 'chaplin had', \"had n't\", \"n't gotten\", 'gotten along', 'along with', 'with his', 'his earlier', 'earlier directors', 'directors henry', 'henry lehrman', 'lehrman and', 'and george', 'george nichols', 'nichols but', 'but according', 'according to', 'to his', 'his autobiography', 'autobiography having', 'having to', 'to take', 'take direction', 'direction from', 'from a', 'a mere', 'mere girl', 'girl was', 'was the', 'the last', 'last straw', 'straw charlie', 'charlie and', 'and mabel', 'mabel argued', 'argued bitterly', 'bitterly during', 'during the', 'the making', 'making of', 'of this', 'this film', 'film chaplin', 'chaplin was', 'was still', 'still a', 'a newcomer', 'newcomer at', 'at keystone', 'keystone and', 'and his', 'his colleagues', 'colleagues did', \"did n't\", \"n't know\", 'know what', 'what to', 'to make', 'make of', 'of him', 'him but', 'but everyone', 'everyone loved', 'loved mabel', 'mabel producer', 'producer mack', 'mack sennett', 'sennett was', 'was on', 'on the', 'the verge', 'verge of', 'of firing', 'firing chaplin', 'chaplin when', 'when he', 'he learned', 'learned that', 'that the', 'the newcomer', \"newcomer 's\", \"'s films\", 'films were', 'were catching', 'catching on', 'on and', 'and exhibitors', 'exhibitors wanted', 'wanted more', 'more of', 'of them', 'them a.s.a.p.', 'a.s.a.p. so', 'so chaplin', 'chaplin was', 'was promised', 'promised the', 'the chance', 'chance to', 'to direct', 'direct himself', 'himself in', 'in return', 'return for', 'for finishing', 'finishing this', 'this movie', 'movie the', 'the way', 'way mabel', 'mabel wanted', 'wanted it', 'it unfortunately', 'unfortunately none', 'none of', 'of that', 'that drama', 'drama is', 'is visible', 'visible on', 'on screen', 'screen in', 'in mabel', 'mabel at', 'at the', 'the wheel', 'wheel which', 'which looks', 'looks like', 'like typical', 'typical keystone', 'keystone chaos', 'chaos the', 'the story', 'story concerns', 'concerns an', 'an auto', 'auto race', 'race in', 'in which', 'which mabel', \"mabel 's\", \"'s beau\", 'beau harry', 'harry mccoy', 'mccoy is', 'is scheduled', 'scheduled to', 'to compete', 'compete but', 'but wicked', 'wicked charlie', 'charlie and', 'and his', 'his henchmen', 'henchmen abduct', 'abduct the', 'the lad', 'lad and', 'and mabel', 'mabel must', 'must take', 'take the', 'the wheel', 'wheel in', 'in his', 'his place', 'place for', 'for all', 'all the', 'the racing', 'racing around', 'around brick', 'brick hurling', 'hurling and', 'and finger', 'finger biting', 'biting the', 'the film', 'film is', 'is frankly', 'frankly short', 'short on', 'on laughs', 'laughs but', 'but there', 'there are', 'are a', 'a few', 'few points', 'points of', 'of interest', 'interest there', \"there 's\", \"'s some\", 'some good', 'good cinematography', 'cinematography and', 'and editing', 'editing in', 'in the', 'the race', 'race sequence', 'sequence though', 'though there', 'there are', \"are n't\", \"n't really\", 'really any', 'any gags', 'gags just', 'just lots', 'lots of', 'of frantic', 'frantic activity', 'activity chaplin', 'chaplin himself', 'himself looks', 'looks odd', 'odd sporting', 'sporting a', 'a goat', 'goat like', 'like beard', 'beard on', 'on his', 'his chin', 'chin and', 'and wearing', 'wearing the', 'the top', 'top hat', 'hat and', 'and frock', 'frock coat', 'coat he', 'he wore', 'wore in', 'in his', 'his very', 'very first', 'first film', 'film appearance', 'appearance making', 'making a', 'a living', 'living but', 'but the', 'the outfit', 'outfit suits', 'suits the', 'the old', 'old fashioned', 'fashioned villainy', 'villainy he', 'he displays', 'displays throughout', 'throughout at', 'at least', 'least it', \"it 's\", \"'s novel\", 'novel to', 'to watch', 'watch him', 'him play', 'play such', 'such an', 'an uncharacteristic', 'uncharacteristic role', 'role visible', 'visible in', 'in the', 'the stands', 'stands at', 'at the', 'the race', 'race track', 'track are', 'are such', 'such keystone', 'keystone stalwarts', 'stalwarts as', 'as chester', 'chester conklin', 'conklin edgar', 'edgar kennedy', 'kennedy in', 'in a', 'a strangely', 'strangely dandified', 'dandified get', 'get up', 'up and', 'and a', 'a more', 'more characteristic', 'characteristic mack', 'mack sennett', 'sennett spitting', 'spitting tobacco', 'tobacco and', 'and doing', 'doing his', 'his usual', 'usual mindless', 'mindless rube', 'rube routine', 'routine as', 'as a', 'a performer', 'performer sennett', 'sennett was', 'was about', 'about as', 'as subtle', 'subtle as', 'as the', 'the movies', 'movies he', 'he produced', 'produced but', 'but you', 'you have', 'have to', 'to give', 'give the', 'the guy', 'guy credit', 'credit he', 'he knew', 'knew what', 'what people', 'people liked', 'liked these', 'these films', 'films were', 'were hugely', 'hugely popular', 'popular in', 'in their', 'their day', 'day mack', \"mack 's\", \"'s performance\", 'performance does', \"does n't\", \"n't add\", 'add much', 'much to', 'to mabel', 'mabel at', 'at the', 'the wheel', 'wheel but', 'but he', 'he probably', 'probably had', 'had to', 'to be', 'be on', 'on hand', 'hand for', 'for the', 'the filming', 'filming of', 'of this', 'this one', 'one to', 'to make', 'make sure', 'sure his', 'his stars', 'stars did', \"did n't\", \"n't kill\", 'kill each', 'each other']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'surely', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(all_train_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove blank space tokens\n",
    "\n",
    "In the above tokenization, some blankspace strings were observed, thus this section adresses that by deleting them from the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blankspaces = [\" \",\"  \",\"   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_blankspaces(review):\n",
    "    \n",
    "#     review = [x for x in review if x not in blankspaces] \n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(remove_blankspaces(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data_tokens_clean = [remove_blankspaces(token) for token in train_data_tokens]\n",
    "# len(train_data_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens_clean = remove_blankspaces(all_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9538806"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289344"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,vocab_size=max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8255 ; token my money\n",
      "Token my money; token id 8255\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDBDataset(train_data_indices, training_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, validation_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [51/625], Training Acc: 51.965,Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [101/625], Training Acc: 69.47,Validation Acc: 67.8\n",
      "Epoch: [1/10], Step: [151/625], Training Acc: 81.755,Validation Acc: 80.14\n",
      "Epoch: [1/10], Step: [201/625], Training Acc: 84.655,Validation Acc: 82.76\n",
      "Epoch: [1/10], Step: [251/625], Training Acc: 86.205,Validation Acc: 84.12\n",
      "Epoch: [1/10], Step: [301/625], Training Acc: 87.52,Validation Acc: 84.44\n",
      "Epoch: [1/10], Step: [351/625], Training Acc: 88.47,Validation Acc: 85.52\n",
      "Epoch: [1/10], Step: [401/625], Training Acc: 88.695,Validation Acc: 85.42\n",
      "Epoch: [1/10], Step: [451/625], Training Acc: 88.65,Validation Acc: 85.02\n",
      "Epoch: [1/10], Step: [501/625], Training Acc: 90.575,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [551/625], Training Acc: 90.955,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [601/625], Training Acc: 91.545,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [51/625], Training Acc: 91.66,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [101/625], Training Acc: 91.36,Validation Acc: 86.16\n",
      "Epoch: [2/10], Step: [151/625], Training Acc: 91.815,Validation Acc: 86.7\n",
      "Epoch: [2/10], Step: [201/625], Training Acc: 92.165,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [251/625], Training Acc: 92.52,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [301/625], Training Acc: 91.02,Validation Acc: 84.94\n",
      "Epoch: [2/10], Step: [351/625], Training Acc: 90.405,Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [401/625], Training Acc: 92.965,Validation Acc: 86.8\n",
      "Epoch: [2/10], Step: [451/625], Training Acc: 93.18,Validation Acc: 86.64\n",
      "Epoch: [2/10], Step: [501/625], Training Acc: 92.6,Validation Acc: 85.84\n",
      "Epoch: [2/10], Step: [551/625], Training Acc: 93.67,Validation Acc: 86.9\n",
      "Epoch: [2/10], Step: [601/625], Training Acc: 94.16,Validation Acc: 86.9\n",
      "Epoch: [3/10], Step: [51/625], Training Acc: 94.08,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [101/625], Training Acc: 94.445,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [151/625], Training Acc: 94.335,Validation Acc: 86.64\n",
      "Epoch: [3/10], Step: [201/625], Training Acc: 93.075,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [251/625], Training Acc: 94.62,Validation Acc: 86.42\n",
      "Epoch: [3/10], Step: [301/625], Training Acc: 94.36,Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [351/625], Training Acc: 94.85,Validation Acc: 86.4\n",
      "Epoch: [3/10], Step: [401/625], Training Acc: 94.705,Validation Acc: 85.98\n",
      "Epoch: [3/10], Step: [451/625], Training Acc: 95.19,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [501/625], Training Acc: 95.215,Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [551/625], Training Acc: 94.475,Validation Acc: 85.5\n",
      "Epoch: [3/10], Step: [601/625], Training Acc: 95.13,Validation Acc: 85.92\n",
      "Epoch: [4/10], Step: [51/625], Training Acc: 94.39,Validation Acc: 85.26\n",
      "Epoch: [4/10], Step: [101/625], Training Acc: 95.16,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [151/625], Training Acc: 95.31,Validation Acc: 85.86\n",
      "Epoch: [4/10], Step: [201/625], Training Acc: 94.805,Validation Acc: 85.22\n",
      "Epoch: [4/10], Step: [251/625], Training Acc: 95.23,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [301/625], Training Acc: 95.89,Validation Acc: 85.3\n",
      "Epoch: [4/10], Step: [351/625], Training Acc: 95.895,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [401/625], Training Acc: 95.84,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [451/625], Training Acc: 96.21,Validation Acc: 85.18\n",
      "Epoch: [4/10], Step: [501/625], Training Acc: 96.0,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [551/625], Training Acc: 95.8,Validation Acc: 85.42\n",
      "Epoch: [4/10], Step: [601/625], Training Acc: 96.345,Validation Acc: 85.62\n",
      "Epoch: [5/10], Step: [51/625], Training Acc: 96.41,Validation Acc: 85.5\n",
      "Epoch: [5/10], Step: [101/625], Training Acc: 96.415,Validation Acc: 85.32\n",
      "Epoch: [5/10], Step: [151/625], Training Acc: 96.14,Validation Acc: 84.98\n",
      "Epoch: [5/10], Step: [201/625], Training Acc: 95.265,Validation Acc: 84.26\n",
      "Epoch: [5/10], Step: [251/625], Training Acc: 95.785,Validation Acc: 84.56\n",
      "Epoch: [5/10], Step: [301/625], Training Acc: 96.415,Validation Acc: 85.36\n",
      "Epoch: [5/10], Step: [351/625], Training Acc: 96.61,Validation Acc: 85.48\n",
      "Epoch: [5/10], Step: [401/625], Training Acc: 95.835,Validation Acc: 84.32\n",
      "Epoch: [5/10], Step: [451/625], Training Acc: 96.675,Validation Acc: 85.12\n",
      "Epoch: [5/10], Step: [501/625], Training Acc: 96.98,Validation Acc: 85.3\n",
      "Epoch: [5/10], Step: [551/625], Training Acc: 96.965,Validation Acc: 85.08\n",
      "Epoch: [5/10], Step: [601/625], Training Acc: 96.755,Validation Acc: 84.64\n",
      "Epoch: [6/10], Step: [51/625], Training Acc: 97.115,Validation Acc: 85.04\n",
      "Epoch: [6/10], Step: [101/625], Training Acc: 97.185,Validation Acc: 84.78\n",
      "Epoch: [6/10], Step: [151/625], Training Acc: 96.855,Validation Acc: 84.54\n",
      "Epoch: [6/10], Step: [201/625], Training Acc: 96.615,Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [251/625], Training Acc: 95.92,Validation Acc: 83.54\n",
      "Epoch: [6/10], Step: [301/625], Training Acc: 96.895,Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [351/625], Training Acc: 96.995,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [401/625], Training Acc: 97.005,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [451/625], Training Acc: 96.785,Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [501/625], Training Acc: 97.03,Validation Acc: 84.74\n",
      "Epoch: [6/10], Step: [551/625], Training Acc: 95.895,Validation Acc: 83.78\n",
      "Epoch: [6/10], Step: [601/625], Training Acc: 96.885,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [51/625], Training Acc: 96.99,Validation Acc: 84.42\n",
      "Epoch: [7/10], Step: [101/625], Training Acc: 97.6,Validation Acc: 84.66\n",
      "Epoch: [7/10], Step: [151/625], Training Acc: 97.25,Validation Acc: 84.4\n",
      "Epoch: [7/10], Step: [201/625], Training Acc: 97.48,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [251/625], Training Acc: 97.555,Validation Acc: 84.32\n",
      "Epoch: [7/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 84.54\n",
      "Epoch: [7/10], Step: [351/625], Training Acc: 96.79,Validation Acc: 83.8\n",
      "Epoch: [7/10], Step: [401/625], Training Acc: 97.3,Validation Acc: 83.76\n",
      "Epoch: [7/10], Step: [451/625], Training Acc: 97.43,Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [501/625], Training Acc: 97.525,Validation Acc: 84.22\n",
      "Epoch: [7/10], Step: [551/625], Training Acc: 97.67,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [601/625], Training Acc: 97.295,Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [51/625], Training Acc: 97.83,Validation Acc: 84.56\n",
      "Epoch: [8/10], Step: [101/625], Training Acc: 97.805,Validation Acc: 84.52\n",
      "Epoch: [8/10], Step: [151/625], Training Acc: 97.855,Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [201/625], Training Acc: 97.81,Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [251/625], Training Acc: 97.655,Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [301/625], Training Acc: 97.075,Validation Acc: 84.06\n",
      "Epoch: [8/10], Step: [351/625], Training Acc: 97.505,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [401/625], Training Acc: 97.59,Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [451/625], Training Acc: 97.46,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [501/625], Training Acc: 97.655,Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [551/625], Training Acc: 96.37,Validation Acc: 82.9\n",
      "Epoch: [8/10], Step: [601/625], Training Acc: 96.815,Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [51/625], Training Acc: 97.91,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [101/625], Training Acc: 97.89,Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [151/625], Training Acc: 97.805,Validation Acc: 83.56\n",
      "Epoch: [9/10], Step: [201/625], Training Acc: 97.41,Validation Acc: 83.74\n",
      "Epoch: [9/10], Step: [251/625], Training Acc: 97.95,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 83.68\n",
      "Epoch: [9/10], Step: [351/625], Training Acc: 97.85,Validation Acc: 83.92\n",
      "Epoch: [9/10], Step: [401/625], Training Acc: 98.08,Validation Acc: 83.64\n",
      "Epoch: [9/10], Step: [451/625], Training Acc: 98.22,Validation Acc: 83.48\n",
      "Epoch: [9/10], Step: [501/625], Training Acc: 97.505,Validation Acc: 83.3\n",
      "Epoch: [9/10], Step: [551/625], Training Acc: 98.14,Validation Acc: 83.62\n",
      "Epoch: [9/10], Step: [601/625], Training Acc: 97.92,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [51/625], Training Acc: 98.1,Validation Acc: 83.78\n",
      "Epoch: [10/10], Step: [101/625], Training Acc: 96.255,Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [151/625], Training Acc: 97.645,Validation Acc: 83.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [201/625], Training Acc: 98.11,Validation Acc: 83.42\n",
      "Epoch: [10/10], Step: [251/625], Training Acc: 96.115,Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [301/625], Training Acc: 97.575,Validation Acc: 83.18\n",
      "Epoch: [10/10], Step: [351/625], Training Acc: 98.245,Validation Acc: 83.76\n",
      "Epoch: [10/10], Step: [401/625], Training Acc: 98.13,Validation Acc: 83.66\n",
      "Epoch: [10/10], Step: [451/625], Training Acc: 98.185,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [501/625], Training Acc: 98.445,Validation Acc: 83.36\n",
      "Epoch: [10/10], Step: [551/625], Training Acc: 97.95,Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [601/625], Training Acc: 97.66,Validation Acc: 83.54\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # check training score every 100 iterations\n",
    "        ## validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {},Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, \n",
    "                len(train_loader), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we are going to try to optimize are the following:\n",
    "\n",
    "* n-gram max length\n",
    "* optimizer choice\n",
    "* embedding size\n",
    "* vocab size\n",
    "* learning rate of the optimizer\n",
    "\n",
    "And maybe increase the batch size to speed up the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "params = [[1e-3,1e-2,1e-1,5e-1,1,10,1e2], ## learning rates\n",
    "          list(range(1,5)), ## ngrams\n",
    "          [1e3,1e4,1e5,1e6], ## vocab size\n",
    "          [100,150,200], ## embedding size\n",
    "          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.001, 1, 1000.0, 100, 100, 64),\n",
       " (0.001, 1, 1000.0, 100, 100, 128),\n",
       " (0.001, 1, 1000.0, 100, 200, 64),\n",
       " (0.001, 1, 1000.0, 100, 200, 128),\n",
       " (0.001, 1, 1000.0, 150, 100, 64),\n",
       " (0.001, 1, 1000.0, 150, 100, 128),\n",
       " (0.001, 1, 1000.0, 150, 200, 64),\n",
       " (0.001, 1, 1000.0, 150, 200, 128),\n",
       " (0.001, 1, 1000.0, 200, 100, 64),\n",
       " (0.001, 1, 1000.0, 200, 100, 128),\n",
       " (0.001, 1, 1000.0, 200, 200, 64),\n",
       " (0.001, 1, 1000.0, 200, 200, 128),\n",
       " (0.001, 1, 10000.0, 100, 100, 64),\n",
       " (0.001, 1, 10000.0, 100, 100, 128),\n",
       " (0.001, 1, 10000.0, 100, 200, 64),\n",
       " (0.001, 1, 10000.0, 100, 200, 128),\n",
       " (0.001, 1, 10000.0, 150, 100, 64),\n",
       " (0.001, 1, 10000.0, 150, 100, 128),\n",
       " (0.001, 1, 10000.0, 150, 200, 64),\n",
       " (0.001, 1, 10000.0, 150, 200, 128),\n",
       " (0.001, 1, 10000.0, 200, 100, 64),\n",
       " (0.001, 1, 10000.0, 200, 100, 128),\n",
       " (0.001, 1, 10000.0, 200, 200, 64),\n",
       " (0.001, 1, 10000.0, 200, 200, 128),\n",
       " (0.001, 1, 100000.0, 100, 100, 64),\n",
       " (0.001, 1, 100000.0, 100, 100, 128),\n",
       " (0.001, 1, 100000.0, 100, 200, 64),\n",
       " (0.001, 1, 100000.0, 100, 200, 128),\n",
       " (0.001, 1, 100000.0, 150, 100, 64),\n",
       " (0.001, 1, 100000.0, 150, 100, 128),\n",
       " (0.001, 1, 100000.0, 150, 200, 64),\n",
       " (0.001, 1, 100000.0, 150, 200, 128),\n",
       " (0.001, 1, 100000.0, 200, 100, 64),\n",
       " (0.001, 1, 100000.0, 200, 100, 128),\n",
       " (0.001, 1, 100000.0, 200, 200, 64),\n",
       " (0.001, 1, 100000.0, 200, 200, 128),\n",
       " (0.001, 1, 1000000.0, 100, 100, 64),\n",
       " (0.001, 1, 1000000.0, 100, 100, 128),\n",
       " (0.001, 1, 1000000.0, 100, 200, 64),\n",
       " (0.001, 1, 1000000.0, 100, 200, 128),\n",
       " (0.001, 1, 1000000.0, 150, 100, 64),\n",
       " (0.001, 1, 1000000.0, 150, 100, 128),\n",
       " (0.001, 1, 1000000.0, 150, 200, 64),\n",
       " (0.001, 1, 1000000.0, 150, 200, 128),\n",
       " (0.001, 1, 1000000.0, 200, 100, 64),\n",
       " (0.001, 1, 1000000.0, 200, 100, 128),\n",
       " (0.001, 1, 1000000.0, 200, 200, 64),\n",
       " (0.001, 1, 1000000.0, 200, 200, 128),\n",
       " (0.001, 2, 1000.0, 100, 100, 64),\n",
       " (0.001, 2, 1000.0, 100, 100, 128),\n",
       " (0.001, 2, 1000.0, 100, 200, 64),\n",
       " (0.001, 2, 1000.0, 100, 200, 128),\n",
       " (0.001, 2, 1000.0, 150, 100, 64),\n",
       " (0.001, 2, 1000.0, 150, 100, 128),\n",
       " (0.001, 2, 1000.0, 150, 200, 64),\n",
       " (0.001, 2, 1000.0, 150, 200, 128),\n",
       " (0.001, 2, 1000.0, 200, 100, 64),\n",
       " (0.001, 2, 1000.0, 200, 100, 128),\n",
       " (0.001, 2, 1000.0, 200, 200, 64),\n",
       " (0.001, 2, 1000.0, 200, 200, 128),\n",
       " (0.001, 2, 10000.0, 100, 100, 64),\n",
       " (0.001, 2, 10000.0, 100, 100, 128),\n",
       " (0.001, 2, 10000.0, 100, 200, 64),\n",
       " (0.001, 2, 10000.0, 100, 200, 128),\n",
       " (0.001, 2, 10000.0, 150, 100, 64),\n",
       " (0.001, 2, 10000.0, 150, 100, 128),\n",
       " (0.001, 2, 10000.0, 150, 200, 64),\n",
       " (0.001, 2, 10000.0, 150, 200, 128),\n",
       " (0.001, 2, 10000.0, 200, 100, 64),\n",
       " (0.001, 2, 10000.0, 200, 100, 128),\n",
       " (0.001, 2, 10000.0, 200, 200, 64),\n",
       " (0.001, 2, 10000.0, 200, 200, 128),\n",
       " (0.001, 2, 100000.0, 100, 100, 64),\n",
       " (0.001, 2, 100000.0, 100, 100, 128),\n",
       " (0.001, 2, 100000.0, 100, 200, 64),\n",
       " (0.001, 2, 100000.0, 100, 200, 128),\n",
       " (0.001, 2, 100000.0, 150, 100, 64),\n",
       " (0.001, 2, 100000.0, 150, 100, 128),\n",
       " (0.001, 2, 100000.0, 150, 200, 64),\n",
       " (0.001, 2, 100000.0, 150, 200, 128),\n",
       " (0.001, 2, 100000.0, 200, 100, 64),\n",
       " (0.001, 2, 100000.0, 200, 100, 128),\n",
       " (0.001, 2, 100000.0, 200, 200, 64),\n",
       " (0.001, 2, 100000.0, 200, 200, 128),\n",
       " (0.001, 2, 1000000.0, 100, 100, 64),\n",
       " (0.001, 2, 1000000.0, 100, 100, 128),\n",
       " (0.001, 2, 1000000.0, 100, 200, 64),\n",
       " (0.001, 2, 1000000.0, 100, 200, 128),\n",
       " (0.001, 2, 1000000.0, 150, 100, 64),\n",
       " (0.001, 2, 1000000.0, 150, 100, 128),\n",
       " (0.001, 2, 1000000.0, 150, 200, 64),\n",
       " (0.001, 2, 1000000.0, 150, 200, 128),\n",
       " (0.001, 2, 1000000.0, 200, 100, 64),\n",
       " (0.001, 2, 1000000.0, 200, 100, 128),\n",
       " (0.001, 2, 1000000.0, 200, 200, 64),\n",
       " (0.001, 2, 1000000.0, 200, 200, 128),\n",
       " (0.001, 3, 1000.0, 100, 100, 64),\n",
       " (0.001, 3, 1000.0, 100, 100, 128),\n",
       " (0.001, 3, 1000.0, 100, 200, 64),\n",
       " (0.001, 3, 1000.0, 100, 200, 128),\n",
       " (0.001, 3, 1000.0, 150, 100, 64),\n",
       " (0.001, 3, 1000.0, 150, 100, 128),\n",
       " (0.001, 3, 1000.0, 150, 200, 64),\n",
       " (0.001, 3, 1000.0, 150, 200, 128),\n",
       " (0.001, 3, 1000.0, 200, 100, 64),\n",
       " (0.001, 3, 1000.0, 200, 100, 128),\n",
       " (0.001, 3, 1000.0, 200, 200, 64),\n",
       " (0.001, 3, 1000.0, 200, 200, 128),\n",
       " (0.001, 3, 10000.0, 100, 100, 64),\n",
       " (0.001, 3, 10000.0, 100, 100, 128),\n",
       " (0.001, 3, 10000.0, 100, 200, 64),\n",
       " (0.001, 3, 10000.0, 100, 200, 128),\n",
       " (0.001, 3, 10000.0, 150, 100, 64),\n",
       " (0.001, 3, 10000.0, 150, 100, 128),\n",
       " (0.001, 3, 10000.0, 150, 200, 64),\n",
       " (0.001, 3, 10000.0, 150, 200, 128),\n",
       " (0.001, 3, 10000.0, 200, 100, 64),\n",
       " (0.001, 3, 10000.0, 200, 100, 128),\n",
       " (0.001, 3, 10000.0, 200, 200, 64),\n",
       " (0.001, 3, 10000.0, 200, 200, 128),\n",
       " (0.001, 3, 100000.0, 100, 100, 64),\n",
       " (0.001, 3, 100000.0, 100, 100, 128),\n",
       " (0.001, 3, 100000.0, 100, 200, 64),\n",
       " (0.001, 3, 100000.0, 100, 200, 128),\n",
       " (0.001, 3, 100000.0, 150, 100, 64),\n",
       " (0.001, 3, 100000.0, 150, 100, 128),\n",
       " (0.001, 3, 100000.0, 150, 200, 64),\n",
       " (0.001, 3, 100000.0, 150, 200, 128),\n",
       " (0.001, 3, 100000.0, 200, 100, 64),\n",
       " (0.001, 3, 100000.0, 200, 100, 128),\n",
       " (0.001, 3, 100000.0, 200, 200, 64),\n",
       " (0.001, 3, 100000.0, 200, 200, 128),\n",
       " (0.001, 3, 1000000.0, 100, 100, 64),\n",
       " (0.001, 3, 1000000.0, 100, 100, 128),\n",
       " (0.001, 3, 1000000.0, 100, 200, 64),\n",
       " (0.001, 3, 1000000.0, 100, 200, 128),\n",
       " (0.001, 3, 1000000.0, 150, 100, 64),\n",
       " (0.001, 3, 1000000.0, 150, 100, 128),\n",
       " (0.001, 3, 1000000.0, 150, 200, 64),\n",
       " (0.001, 3, 1000000.0, 150, 200, 128),\n",
       " (0.001, 3, 1000000.0, 200, 100, 64),\n",
       " (0.001, 3, 1000000.0, 200, 100, 128),\n",
       " (0.001, 3, 1000000.0, 200, 200, 64),\n",
       " (0.001, 3, 1000000.0, 200, 200, 128),\n",
       " (0.001, 4, 1000.0, 100, 100, 64),\n",
       " (0.001, 4, 1000.0, 100, 100, 128),\n",
       " (0.001, 4, 1000.0, 100, 200, 64),\n",
       " (0.001, 4, 1000.0, 100, 200, 128),\n",
       " (0.001, 4, 1000.0, 150, 100, 64),\n",
       " (0.001, 4, 1000.0, 150, 100, 128),\n",
       " (0.001, 4, 1000.0, 150, 200, 64),\n",
       " (0.001, 4, 1000.0, 150, 200, 128),\n",
       " (0.001, 4, 1000.0, 200, 100, 64),\n",
       " (0.001, 4, 1000.0, 200, 100, 128),\n",
       " (0.001, 4, 1000.0, 200, 200, 64),\n",
       " (0.001, 4, 1000.0, 200, 200, 128),\n",
       " (0.001, 4, 10000.0, 100, 100, 64),\n",
       " (0.001, 4, 10000.0, 100, 100, 128),\n",
       " (0.001, 4, 10000.0, 100, 200, 64),\n",
       " (0.001, 4, 10000.0, 100, 200, 128),\n",
       " (0.001, 4, 10000.0, 150, 100, 64),\n",
       " (0.001, 4, 10000.0, 150, 100, 128),\n",
       " (0.001, 4, 10000.0, 150, 200, 64),\n",
       " (0.001, 4, 10000.0, 150, 200, 128),\n",
       " (0.001, 4, 10000.0, 200, 100, 64),\n",
       " (0.001, 4, 10000.0, 200, 100, 128),\n",
       " (0.001, 4, 10000.0, 200, 200, 64),\n",
       " (0.001, 4, 10000.0, 200, 200, 128),\n",
       " (0.001, 4, 100000.0, 100, 100, 64),\n",
       " (0.001, 4, 100000.0, 100, 100, 128),\n",
       " (0.001, 4, 100000.0, 100, 200, 64),\n",
       " (0.001, 4, 100000.0, 100, 200, 128),\n",
       " (0.001, 4, 100000.0, 150, 100, 64),\n",
       " (0.001, 4, 100000.0, 150, 100, 128),\n",
       " (0.001, 4, 100000.0, 150, 200, 64),\n",
       " (0.001, 4, 100000.0, 150, 200, 128),\n",
       " (0.001, 4, 100000.0, 200, 100, 64),\n",
       " (0.001, 4, 100000.0, 200, 100, 128),\n",
       " (0.001, 4, 100000.0, 200, 200, 64),\n",
       " (0.001, 4, 100000.0, 200, 200, 128),\n",
       " (0.001, 4, 1000000.0, 100, 100, 64),\n",
       " (0.001, 4, 1000000.0, 100, 100, 128),\n",
       " (0.001, 4, 1000000.0, 100, 200, 64),\n",
       " (0.001, 4, 1000000.0, 100, 200, 128),\n",
       " (0.001, 4, 1000000.0, 150, 100, 64),\n",
       " (0.001, 4, 1000000.0, 150, 100, 128),\n",
       " (0.001, 4, 1000000.0, 150, 200, 64),\n",
       " (0.001, 4, 1000000.0, 150, 200, 128),\n",
       " (0.001, 4, 1000000.0, 200, 100, 64),\n",
       " (0.001, 4, 1000000.0, 200, 100, 128),\n",
       " (0.001, 4, 1000000.0, 200, 200, 64),\n",
       " (0.001, 4, 1000000.0, 200, 200, 128),\n",
       " (0.01, 1, 1000.0, 100, 100, 64),\n",
       " (0.01, 1, 1000.0, 100, 100, 128),\n",
       " (0.01, 1, 1000.0, 100, 200, 64),\n",
       " (0.01, 1, 1000.0, 100, 200, 128),\n",
       " (0.01, 1, 1000.0, 150, 100, 64),\n",
       " (0.01, 1, 1000.0, 150, 100, 128),\n",
       " (0.01, 1, 1000.0, 150, 200, 64),\n",
       " (0.01, 1, 1000.0, 150, 200, 128),\n",
       " (0.01, 1, 1000.0, 200, 100, 64),\n",
       " (0.01, 1, 1000.0, 200, 100, 128),\n",
       " (0.01, 1, 1000.0, 200, 200, 64),\n",
       " (0.01, 1, 1000.0, 200, 200, 128),\n",
       " (0.01, 1, 10000.0, 100, 100, 64),\n",
       " (0.01, 1, 10000.0, 100, 100, 128),\n",
       " (0.01, 1, 10000.0, 100, 200, 64),\n",
       " (0.01, 1, 10000.0, 100, 200, 128),\n",
       " (0.01, 1, 10000.0, 150, 100, 64),\n",
       " (0.01, 1, 10000.0, 150, 100, 128),\n",
       " (0.01, 1, 10000.0, 150, 200, 64),\n",
       " (0.01, 1, 10000.0, 150, 200, 128),\n",
       " (0.01, 1, 10000.0, 200, 100, 64),\n",
       " (0.01, 1, 10000.0, 200, 100, 128),\n",
       " (0.01, 1, 10000.0, 200, 200, 64),\n",
       " (0.01, 1, 10000.0, 200, 200, 128),\n",
       " (0.01, 1, 100000.0, 100, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 100, 128),\n",
       " (0.01, 1, 100000.0, 100, 200, 64),\n",
       " (0.01, 1, 100000.0, 100, 200, 128),\n",
       " (0.01, 1, 100000.0, 150, 100, 64),\n",
       " (0.01, 1, 100000.0, 150, 100, 128),\n",
       " (0.01, 1, 100000.0, 150, 200, 64),\n",
       " (0.01, 1, 100000.0, 150, 200, 128),\n",
       " (0.01, 1, 100000.0, 200, 100, 64),\n",
       " (0.01, 1, 100000.0, 200, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 100, 128),\n",
       " (0.01, 1, 1000000.0, 100, 200, 64),\n",
       " (0.01, 1, 1000000.0, 100, 200, 128),\n",
       " (0.01, 1, 1000000.0, 150, 100, 64),\n",
       " (0.01, 1, 1000000.0, 150, 100, 128),\n",
       " (0.01, 1, 1000000.0, 150, 200, 64),\n",
       " (0.01, 1, 1000000.0, 150, 200, 128),\n",
       " (0.01, 1, 1000000.0, 200, 100, 64),\n",
       " (0.01, 1, 1000000.0, 200, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 200, 128),\n",
       " (0.01, 2, 1000.0, 100, 100, 64),\n",
       " (0.01, 2, 1000.0, 100, 100, 128),\n",
       " (0.01, 2, 1000.0, 100, 200, 64),\n",
       " (0.01, 2, 1000.0, 100, 200, 128),\n",
       " (0.01, 2, 1000.0, 150, 100, 64),\n",
       " (0.01, 2, 1000.0, 150, 100, 128),\n",
       " (0.01, 2, 1000.0, 150, 200, 64),\n",
       " (0.01, 2, 1000.0, 150, 200, 128),\n",
       " (0.01, 2, 1000.0, 200, 100, 64),\n",
       " (0.01, 2, 1000.0, 200, 100, 128),\n",
       " (0.01, 2, 1000.0, 200, 200, 64),\n",
       " (0.01, 2, 1000.0, 200, 200, 128),\n",
       " (0.01, 2, 10000.0, 100, 100, 64),\n",
       " (0.01, 2, 10000.0, 100, 100, 128),\n",
       " (0.01, 2, 10000.0, 100, 200, 64),\n",
       " (0.01, 2, 10000.0, 100, 200, 128),\n",
       " (0.01, 2, 10000.0, 150, 100, 64),\n",
       " (0.01, 2, 10000.0, 150, 100, 128),\n",
       " (0.01, 2, 10000.0, 150, 200, 64),\n",
       " (0.01, 2, 10000.0, 150, 200, 128),\n",
       " (0.01, 2, 10000.0, 200, 100, 64),\n",
       " (0.01, 2, 10000.0, 200, 100, 128),\n",
       " (0.01, 2, 10000.0, 200, 200, 64),\n",
       " (0.01, 2, 10000.0, 200, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 100, 128),\n",
       " (0.01, 2, 100000.0, 100, 200, 64),\n",
       " (0.01, 2, 100000.0, 100, 200, 128),\n",
       " (0.01, 2, 100000.0, 150, 100, 64),\n",
       " (0.01, 2, 100000.0, 150, 100, 128),\n",
       " (0.01, 2, 100000.0, 150, 200, 64),\n",
       " (0.01, 2, 100000.0, 150, 200, 128),\n",
       " (0.01, 2, 100000.0, 200, 100, 64),\n",
       " (0.01, 2, 100000.0, 200, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 100, 128),\n",
       " (0.01, 2, 1000000.0, 100, 200, 64),\n",
       " (0.01, 2, 1000000.0, 100, 200, 128),\n",
       " (0.01, 2, 1000000.0, 150, 100, 64),\n",
       " (0.01, 2, 1000000.0, 150, 100, 128),\n",
       " (0.01, 2, 1000000.0, 150, 200, 64),\n",
       " (0.01, 2, 1000000.0, 150, 200, 128),\n",
       " (0.01, 2, 1000000.0, 200, 100, 64),\n",
       " (0.01, 2, 1000000.0, 200, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 200, 128),\n",
       " (0.01, 3, 1000.0, 100, 100, 64),\n",
       " (0.01, 3, 1000.0, 100, 100, 128),\n",
       " (0.01, 3, 1000.0, 100, 200, 64),\n",
       " (0.01, 3, 1000.0, 100, 200, 128),\n",
       " (0.01, 3, 1000.0, 150, 100, 64),\n",
       " (0.01, 3, 1000.0, 150, 100, 128),\n",
       " (0.01, 3, 1000.0, 150, 200, 64),\n",
       " (0.01, 3, 1000.0, 150, 200, 128),\n",
       " (0.01, 3, 1000.0, 200, 100, 64),\n",
       " (0.01, 3, 1000.0, 200, 100, 128),\n",
       " (0.01, 3, 1000.0, 200, 200, 64),\n",
       " (0.01, 3, 1000.0, 200, 200, 128),\n",
       " (0.01, 3, 10000.0, 100, 100, 64),\n",
       " (0.01, 3, 10000.0, 100, 100, 128),\n",
       " (0.01, 3, 10000.0, 100, 200, 64),\n",
       " (0.01, 3, 10000.0, 100, 200, 128),\n",
       " (0.01, 3, 10000.0, 150, 100, 64),\n",
       " (0.01, 3, 10000.0, 150, 100, 128),\n",
       " (0.01, 3, 10000.0, 150, 200, 64),\n",
       " (0.01, 3, 10000.0, 150, 200, 128),\n",
       " (0.01, 3, 10000.0, 200, 100, 64),\n",
       " (0.01, 3, 10000.0, 200, 100, 128),\n",
       " (0.01, 3, 10000.0, 200, 200, 64),\n",
       " (0.01, 3, 10000.0, 200, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 100, 128),\n",
       " (0.01, 3, 100000.0, 100, 200, 64),\n",
       " (0.01, 3, 100000.0, 100, 200, 128),\n",
       " (0.01, 3, 100000.0, 150, 100, 64),\n",
       " (0.01, 3, 100000.0, 150, 100, 128),\n",
       " (0.01, 3, 100000.0, 150, 200, 64),\n",
       " (0.01, 3, 100000.0, 150, 200, 128),\n",
       " (0.01, 3, 100000.0, 200, 100, 64),\n",
       " (0.01, 3, 100000.0, 200, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 100, 128),\n",
       " (0.01, 3, 1000000.0, 100, 200, 64),\n",
       " (0.01, 3, 1000000.0, 100, 200, 128),\n",
       " (0.01, 3, 1000000.0, 150, 100, 64),\n",
       " (0.01, 3, 1000000.0, 150, 100, 128),\n",
       " (0.01, 3, 1000000.0, 150, 200, 64),\n",
       " (0.01, 3, 1000000.0, 150, 200, 128),\n",
       " (0.01, 3, 1000000.0, 200, 100, 64),\n",
       " (0.01, 3, 1000000.0, 200, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 200, 128),\n",
       " (0.01, 4, 1000.0, 100, 100, 64),\n",
       " (0.01, 4, 1000.0, 100, 100, 128),\n",
       " (0.01, 4, 1000.0, 100, 200, 64),\n",
       " (0.01, 4, 1000.0, 100, 200, 128),\n",
       " (0.01, 4, 1000.0, 150, 100, 64),\n",
       " (0.01, 4, 1000.0, 150, 100, 128),\n",
       " (0.01, 4, 1000.0, 150, 200, 64),\n",
       " (0.01, 4, 1000.0, 150, 200, 128),\n",
       " (0.01, 4, 1000.0, 200, 100, 64),\n",
       " (0.01, 4, 1000.0, 200, 100, 128),\n",
       " (0.01, 4, 1000.0, 200, 200, 64),\n",
       " (0.01, 4, 1000.0, 200, 200, 128),\n",
       " (0.01, 4, 10000.0, 100, 100, 64),\n",
       " (0.01, 4, 10000.0, 100, 100, 128),\n",
       " (0.01, 4, 10000.0, 100, 200, 64),\n",
       " (0.01, 4, 10000.0, 100, 200, 128),\n",
       " (0.01, 4, 10000.0, 150, 100, 64),\n",
       " (0.01, 4, 10000.0, 150, 100, 128),\n",
       " (0.01, 4, 10000.0, 150, 200, 64),\n",
       " (0.01, 4, 10000.0, 150, 200, 128),\n",
       " (0.01, 4, 10000.0, 200, 100, 64),\n",
       " (0.01, 4, 10000.0, 200, 100, 128),\n",
       " (0.01, 4, 10000.0, 200, 200, 64),\n",
       " (0.01, 4, 10000.0, 200, 200, 128),\n",
       " (0.01, 4, 100000.0, 100, 100, 64),\n",
       " (0.01, 4, 100000.0, 100, 100, 128),\n",
       " (0.01, 4, 100000.0, 100, 200, 64),\n",
       " (0.01, 4, 100000.0, 100, 200, 128),\n",
       " (0.01, 4, 100000.0, 150, 100, 64),\n",
       " (0.01, 4, 100000.0, 150, 100, 128),\n",
       " (0.01, 4, 100000.0, 150, 200, 64),\n",
       " (0.01, 4, 100000.0, 150, 200, 128),\n",
       " (0.01, 4, 100000.0, 200, 100, 64),\n",
       " (0.01, 4, 100000.0, 200, 100, 128),\n",
       " (0.01, 4, 100000.0, 200, 200, 64),\n",
       " (0.01, 4, 100000.0, 200, 200, 128),\n",
       " (0.01, 4, 1000000.0, 100, 100, 64),\n",
       " (0.01, 4, 1000000.0, 100, 100, 128),\n",
       " (0.01, 4, 1000000.0, 100, 200, 64),\n",
       " (0.01, 4, 1000000.0, 100, 200, 128),\n",
       " (0.01, 4, 1000000.0, 150, 100, 64),\n",
       " (0.01, 4, 1000000.0, 150, 100, 128),\n",
       " (0.01, 4, 1000000.0, 150, 200, 64),\n",
       " (0.01, 4, 1000000.0, 150, 200, 128),\n",
       " (0.01, 4, 1000000.0, 200, 100, 64),\n",
       " (0.01, 4, 1000000.0, 200, 100, 128),\n",
       " (0.01, 4, 1000000.0, 200, 200, 64),\n",
       " (0.01, 4, 1000000.0, 200, 200, 128),\n",
       " (0.1, 1, 1000.0, 100, 100, 64),\n",
       " (0.1, 1, 1000.0, 100, 100, 128),\n",
       " (0.1, 1, 1000.0, 100, 200, 64),\n",
       " (0.1, 1, 1000.0, 100, 200, 128),\n",
       " (0.1, 1, 1000.0, 150, 100, 64),\n",
       " (0.1, 1, 1000.0, 150, 100, 128),\n",
       " (0.1, 1, 1000.0, 150, 200, 64),\n",
       " (0.1, 1, 1000.0, 150, 200, 128),\n",
       " (0.1, 1, 1000.0, 200, 100, 64),\n",
       " (0.1, 1, 1000.0, 200, 100, 128),\n",
       " (0.1, 1, 1000.0, 200, 200, 64),\n",
       " (0.1, 1, 1000.0, 200, 200, 128),\n",
       " (0.1, 1, 10000.0, 100, 100, 64),\n",
       " (0.1, 1, 10000.0, 100, 100, 128),\n",
       " (0.1, 1, 10000.0, 100, 200, 64),\n",
       " (0.1, 1, 10000.0, 100, 200, 128),\n",
       " (0.1, 1, 10000.0, 150, 100, 64),\n",
       " (0.1, 1, 10000.0, 150, 100, 128),\n",
       " (0.1, 1, 10000.0, 150, 200, 64),\n",
       " (0.1, 1, 10000.0, 150, 200, 128),\n",
       " (0.1, 1, 10000.0, 200, 100, 64),\n",
       " (0.1, 1, 10000.0, 200, 100, 128),\n",
       " (0.1, 1, 10000.0, 200, 200, 64),\n",
       " (0.1, 1, 10000.0, 200, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 100, 128),\n",
       " (0.1, 1, 100000.0, 100, 200, 64),\n",
       " (0.1, 1, 100000.0, 100, 200, 128),\n",
       " (0.1, 1, 100000.0, 150, 100, 64),\n",
       " (0.1, 1, 100000.0, 150, 100, 128),\n",
       " (0.1, 1, 100000.0, 150, 200, 64),\n",
       " (0.1, 1, 100000.0, 150, 200, 128),\n",
       " (0.1, 1, 100000.0, 200, 100, 64),\n",
       " (0.1, 1, 100000.0, 200, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 100, 128),\n",
       " (0.1, 1, 1000000.0, 100, 200, 64),\n",
       " (0.1, 1, 1000000.0, 100, 200, 128),\n",
       " (0.1, 1, 1000000.0, 150, 100, 64),\n",
       " (0.1, 1, 1000000.0, 150, 100, 128),\n",
       " (0.1, 1, 1000000.0, 150, 200, 64),\n",
       " (0.1, 1, 1000000.0, 150, 200, 128),\n",
       " (0.1, 1, 1000000.0, 200, 100, 64),\n",
       " (0.1, 1, 1000000.0, 200, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 200, 128),\n",
       " (0.1, 2, 1000.0, 100, 100, 64),\n",
       " (0.1, 2, 1000.0, 100, 100, 128),\n",
       " (0.1, 2, 1000.0, 100, 200, 64),\n",
       " (0.1, 2, 1000.0, 100, 200, 128),\n",
       " (0.1, 2, 1000.0, 150, 100, 64),\n",
       " (0.1, 2, 1000.0, 150, 100, 128),\n",
       " (0.1, 2, 1000.0, 150, 200, 64),\n",
       " (0.1, 2, 1000.0, 150, 200, 128),\n",
       " (0.1, 2, 1000.0, 200, 100, 64),\n",
       " (0.1, 2, 1000.0, 200, 100, 128),\n",
       " (0.1, 2, 1000.0, 200, 200, 64),\n",
       " (0.1, 2, 1000.0, 200, 200, 128),\n",
       " (0.1, 2, 10000.0, 100, 100, 64),\n",
       " (0.1, 2, 10000.0, 100, 100, 128),\n",
       " (0.1, 2, 10000.0, 100, 200, 64),\n",
       " (0.1, 2, 10000.0, 100, 200, 128),\n",
       " (0.1, 2, 10000.0, 150, 100, 64),\n",
       " (0.1, 2, 10000.0, 150, 100, 128),\n",
       " (0.1, 2, 10000.0, 150, 200, 64),\n",
       " (0.1, 2, 10000.0, 150, 200, 128),\n",
       " (0.1, 2, 10000.0, 200, 100, 64),\n",
       " (0.1, 2, 10000.0, 200, 100, 128),\n",
       " (0.1, 2, 10000.0, 200, 200, 64),\n",
       " (0.1, 2, 10000.0, 200, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 100, 128),\n",
       " (0.1, 2, 100000.0, 100, 200, 64),\n",
       " (0.1, 2, 100000.0, 100, 200, 128),\n",
       " (0.1, 2, 100000.0, 150, 100, 64),\n",
       " (0.1, 2, 100000.0, 150, 100, 128),\n",
       " (0.1, 2, 100000.0, 150, 200, 64),\n",
       " (0.1, 2, 100000.0, 150, 200, 128),\n",
       " (0.1, 2, 100000.0, 200, 100, 64),\n",
       " (0.1, 2, 100000.0, 200, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 100, 128),\n",
       " (0.1, 2, 1000000.0, 100, 200, 64),\n",
       " (0.1, 2, 1000000.0, 100, 200, 128),\n",
       " (0.1, 2, 1000000.0, 150, 100, 64),\n",
       " (0.1, 2, 1000000.0, 150, 100, 128),\n",
       " (0.1, 2, 1000000.0, 150, 200, 64),\n",
       " (0.1, 2, 1000000.0, 150, 200, 128),\n",
       " (0.1, 2, 1000000.0, 200, 100, 64),\n",
       " (0.1, 2, 1000000.0, 200, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 200, 128),\n",
       " (0.1, 3, 1000.0, 100, 100, 64),\n",
       " (0.1, 3, 1000.0, 100, 100, 128),\n",
       " (0.1, 3, 1000.0, 100, 200, 64),\n",
       " (0.1, 3, 1000.0, 100, 200, 128),\n",
       " (0.1, 3, 1000.0, 150, 100, 64),\n",
       " (0.1, 3, 1000.0, 150, 100, 128),\n",
       " (0.1, 3, 1000.0, 150, 200, 64),\n",
       " (0.1, 3, 1000.0, 150, 200, 128),\n",
       " (0.1, 3, 1000.0, 200, 100, 64),\n",
       " (0.1, 3, 1000.0, 200, 100, 128),\n",
       " (0.1, 3, 1000.0, 200, 200, 64),\n",
       " (0.1, 3, 1000.0, 200, 200, 128),\n",
       " (0.1, 3, 10000.0, 100, 100, 64),\n",
       " (0.1, 3, 10000.0, 100, 100, 128),\n",
       " (0.1, 3, 10000.0, 100, 200, 64),\n",
       " (0.1, 3, 10000.0, 100, 200, 128),\n",
       " (0.1, 3, 10000.0, 150, 100, 64),\n",
       " (0.1, 3, 10000.0, 150, 100, 128),\n",
       " (0.1, 3, 10000.0, 150, 200, 64),\n",
       " (0.1, 3, 10000.0, 150, 200, 128),\n",
       " (0.1, 3, 10000.0, 200, 100, 64),\n",
       " (0.1, 3, 10000.0, 200, 100, 128),\n",
       " (0.1, 3, 10000.0, 200, 200, 64),\n",
       " (0.1, 3, 10000.0, 200, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 100, 128),\n",
       " (0.1, 3, 100000.0, 100, 200, 64),\n",
       " (0.1, 3, 100000.0, 100, 200, 128),\n",
       " (0.1, 3, 100000.0, 150, 100, 64),\n",
       " (0.1, 3, 100000.0, 150, 100, 128),\n",
       " (0.1, 3, 100000.0, 150, 200, 64),\n",
       " (0.1, 3, 100000.0, 150, 200, 128),\n",
       " (0.1, 3, 100000.0, 200, 100, 64),\n",
       " (0.1, 3, 100000.0, 200, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 100, 128),\n",
       " (0.1, 3, 1000000.0, 100, 200, 64),\n",
       " (0.1, 3, 1000000.0, 100, 200, 128),\n",
       " (0.1, 3, 1000000.0, 150, 100, 64),\n",
       " (0.1, 3, 1000000.0, 150, 100, 128),\n",
       " (0.1, 3, 1000000.0, 150, 200, 64),\n",
       " (0.1, 3, 1000000.0, 150, 200, 128),\n",
       " (0.1, 3, 1000000.0, 200, 100, 64),\n",
       " (0.1, 3, 1000000.0, 200, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 200, 128),\n",
       " (0.1, 4, 1000.0, 100, 100, 64),\n",
       " (0.1, 4, 1000.0, 100, 100, 128),\n",
       " (0.1, 4, 1000.0, 100, 200, 64),\n",
       " (0.1, 4, 1000.0, 100, 200, 128),\n",
       " (0.1, 4, 1000.0, 150, 100, 64),\n",
       " (0.1, 4, 1000.0, 150, 100, 128),\n",
       " (0.1, 4, 1000.0, 150, 200, 64),\n",
       " (0.1, 4, 1000.0, 150, 200, 128),\n",
       " (0.1, 4, 1000.0, 200, 100, 64),\n",
       " (0.1, 4, 1000.0, 200, 100, 128),\n",
       " (0.1, 4, 1000.0, 200, 200, 64),\n",
       " (0.1, 4, 1000.0, 200, 200, 128),\n",
       " (0.1, 4, 10000.0, 100, 100, 64),\n",
       " (0.1, 4, 10000.0, 100, 100, 128),\n",
       " (0.1, 4, 10000.0, 100, 200, 64),\n",
       " (0.1, 4, 10000.0, 100, 200, 128),\n",
       " (0.1, 4, 10000.0, 150, 100, 64),\n",
       " (0.1, 4, 10000.0, 150, 100, 128),\n",
       " (0.1, 4, 10000.0, 150, 200, 64),\n",
       " (0.1, 4, 10000.0, 150, 200, 128),\n",
       " (0.1, 4, 10000.0, 200, 100, 64),\n",
       " (0.1, 4, 10000.0, 200, 100, 128),\n",
       " (0.1, 4, 10000.0, 200, 200, 64),\n",
       " (0.1, 4, 10000.0, 200, 200, 128),\n",
       " (0.1, 4, 100000.0, 100, 100, 64),\n",
       " (0.1, 4, 100000.0, 100, 100, 128),\n",
       " (0.1, 4, 100000.0, 100, 200, 64),\n",
       " (0.1, 4, 100000.0, 100, 200, 128),\n",
       " (0.1, 4, 100000.0, 150, 100, 64),\n",
       " (0.1, 4, 100000.0, 150, 100, 128),\n",
       " (0.1, 4, 100000.0, 150, 200, 64),\n",
       " (0.1, 4, 100000.0, 150, 200, 128),\n",
       " (0.1, 4, 100000.0, 200, 100, 64),\n",
       " (0.1, 4, 100000.0, 200, 100, 128),\n",
       " (0.1, 4, 100000.0, 200, 200, 64),\n",
       " (0.1, 4, 100000.0, 200, 200, 128),\n",
       " (0.1, 4, 1000000.0, 100, 100, 64),\n",
       " (0.1, 4, 1000000.0, 100, 100, 128),\n",
       " (0.1, 4, 1000000.0, 100, 200, 64),\n",
       " (0.1, 4, 1000000.0, 100, 200, 128),\n",
       " (0.1, 4, 1000000.0, 150, 100, 64),\n",
       " (0.1, 4, 1000000.0, 150, 100, 128),\n",
       " (0.1, 4, 1000000.0, 150, 200, 64),\n",
       " (0.1, 4, 1000000.0, 150, 200, 128),\n",
       " (0.1, 4, 1000000.0, 200, 100, 64),\n",
       " (0.1, 4, 1000000.0, 200, 100, 128),\n",
       " (0.1, 4, 1000000.0, 200, 200, 64),\n",
       " (0.1, 4, 1000000.0, 200, 200, 128),\n",
       " (0.5, 1, 1000.0, 100, 100, 64),\n",
       " (0.5, 1, 1000.0, 100, 100, 128),\n",
       " (0.5, 1, 1000.0, 100, 200, 64),\n",
       " (0.5, 1, 1000.0, 100, 200, 128),\n",
       " (0.5, 1, 1000.0, 150, 100, 64),\n",
       " (0.5, 1, 1000.0, 150, 100, 128),\n",
       " (0.5, 1, 1000.0, 150, 200, 64),\n",
       " (0.5, 1, 1000.0, 150, 200, 128),\n",
       " (0.5, 1, 1000.0, 200, 100, 64),\n",
       " (0.5, 1, 1000.0, 200, 100, 128),\n",
       " (0.5, 1, 1000.0, 200, 200, 64),\n",
       " (0.5, 1, 1000.0, 200, 200, 128),\n",
       " (0.5, 1, 10000.0, 100, 100, 64),\n",
       " (0.5, 1, 10000.0, 100, 100, 128),\n",
       " (0.5, 1, 10000.0, 100, 200, 64),\n",
       " (0.5, 1, 10000.0, 100, 200, 128),\n",
       " (0.5, 1, 10000.0, 150, 100, 64),\n",
       " (0.5, 1, 10000.0, 150, 100, 128),\n",
       " (0.5, 1, 10000.0, 150, 200, 64),\n",
       " (0.5, 1, 10000.0, 150, 200, 128),\n",
       " (0.5, 1, 10000.0, 200, 100, 64),\n",
       " (0.5, 1, 10000.0, 200, 100, 128),\n",
       " (0.5, 1, 10000.0, 200, 200, 64),\n",
       " (0.5, 1, 10000.0, 200, 200, 128),\n",
       " (0.5, 1, 100000.0, 100, 100, 64),\n",
       " (0.5, 1, 100000.0, 100, 100, 128),\n",
       " (0.5, 1, 100000.0, 100, 200, 64),\n",
       " (0.5, 1, 100000.0, 100, 200, 128),\n",
       " (0.5, 1, 100000.0, 150, 100, 64),\n",
       " (0.5, 1, 100000.0, 150, 100, 128),\n",
       " (0.5, 1, 100000.0, 150, 200, 64),\n",
       " (0.5, 1, 100000.0, 150, 200, 128),\n",
       " (0.5, 1, 100000.0, 200, 100, 64),\n",
       " (0.5, 1, 100000.0, 200, 100, 128),\n",
       " (0.5, 1, 100000.0, 200, 200, 64),\n",
       " (0.5, 1, 100000.0, 200, 200, 128),\n",
       " (0.5, 1, 1000000.0, 100, 100, 64),\n",
       " (0.5, 1, 1000000.0, 100, 100, 128),\n",
       " (0.5, 1, 1000000.0, 100, 200, 64),\n",
       " (0.5, 1, 1000000.0, 100, 200, 128),\n",
       " (0.5, 1, 1000000.0, 150, 100, 64),\n",
       " (0.5, 1, 1000000.0, 150, 100, 128),\n",
       " (0.5, 1, 1000000.0, 150, 200, 64),\n",
       " (0.5, 1, 1000000.0, 150, 200, 128),\n",
       " (0.5, 1, 1000000.0, 200, 100, 64),\n",
       " (0.5, 1, 1000000.0, 200, 100, 128),\n",
       " (0.5, 1, 1000000.0, 200, 200, 64),\n",
       " (0.5, 1, 1000000.0, 200, 200, 128),\n",
       " (0.5, 2, 1000.0, 100, 100, 64),\n",
       " (0.5, 2, 1000.0, 100, 100, 128),\n",
       " (0.5, 2, 1000.0, 100, 200, 64),\n",
       " (0.5, 2, 1000.0, 100, 200, 128),\n",
       " (0.5, 2, 1000.0, 150, 100, 64),\n",
       " (0.5, 2, 1000.0, 150, 100, 128),\n",
       " (0.5, 2, 1000.0, 150, 200, 64),\n",
       " (0.5, 2, 1000.0, 150, 200, 128),\n",
       " (0.5, 2, 1000.0, 200, 100, 64),\n",
       " (0.5, 2, 1000.0, 200, 100, 128),\n",
       " (0.5, 2, 1000.0, 200, 200, 64),\n",
       " (0.5, 2, 1000.0, 200, 200, 128),\n",
       " (0.5, 2, 10000.0, 100, 100, 64),\n",
       " (0.5, 2, 10000.0, 100, 100, 128),\n",
       " (0.5, 2, 10000.0, 100, 200, 64),\n",
       " (0.5, 2, 10000.0, 100, 200, 128),\n",
       " (0.5, 2, 10000.0, 150, 100, 64),\n",
       " (0.5, 2, 10000.0, 150, 100, 128),\n",
       " (0.5, 2, 10000.0, 150, 200, 64),\n",
       " (0.5, 2, 10000.0, 150, 200, 128),\n",
       " (0.5, 2, 10000.0, 200, 100, 64),\n",
       " (0.5, 2, 10000.0, 200, 100, 128),\n",
       " (0.5, 2, 10000.0, 200, 200, 64),\n",
       " (0.5, 2, 10000.0, 200, 200, 128),\n",
       " (0.5, 2, 100000.0, 100, 100, 64),\n",
       " (0.5, 2, 100000.0, 100, 100, 128),\n",
       " (0.5, 2, 100000.0, 100, 200, 64),\n",
       " (0.5, 2, 100000.0, 100, 200, 128),\n",
       " (0.5, 2, 100000.0, 150, 100, 64),\n",
       " (0.5, 2, 100000.0, 150, 100, 128),\n",
       " (0.5, 2, 100000.0, 150, 200, 64),\n",
       " (0.5, 2, 100000.0, 150, 200, 128),\n",
       " (0.5, 2, 100000.0, 200, 100, 64),\n",
       " (0.5, 2, 100000.0, 200, 100, 128),\n",
       " (0.5, 2, 100000.0, 200, 200, 64),\n",
       " (0.5, 2, 100000.0, 200, 200, 128),\n",
       " (0.5, 2, 1000000.0, 100, 100, 64),\n",
       " (0.5, 2, 1000000.0, 100, 100, 128),\n",
       " (0.5, 2, 1000000.0, 100, 200, 64),\n",
       " (0.5, 2, 1000000.0, 100, 200, 128),\n",
       " (0.5, 2, 1000000.0, 150, 100, 64),\n",
       " (0.5, 2, 1000000.0, 150, 100, 128),\n",
       " (0.5, 2, 1000000.0, 150, 200, 64),\n",
       " (0.5, 2, 1000000.0, 150, 200, 128),\n",
       " (0.5, 2, 1000000.0, 200, 100, 64),\n",
       " (0.5, 2, 1000000.0, 200, 100, 128),\n",
       " (0.5, 2, 1000000.0, 200, 200, 64),\n",
       " (0.5, 2, 1000000.0, 200, 200, 128),\n",
       " (0.5, 3, 1000.0, 100, 100, 64),\n",
       " (0.5, 3, 1000.0, 100, 100, 128),\n",
       " (0.5, 3, 1000.0, 100, 200, 64),\n",
       " (0.5, 3, 1000.0, 100, 200, 128),\n",
       " (0.5, 3, 1000.0, 150, 100, 64),\n",
       " (0.5, 3, 1000.0, 150, 100, 128),\n",
       " (0.5, 3, 1000.0, 150, 200, 64),\n",
       " (0.5, 3, 1000.0, 150, 200, 128),\n",
       " (0.5, 3, 1000.0, 200, 100, 64),\n",
       " (0.5, 3, 1000.0, 200, 100, 128),\n",
       " (0.5, 3, 1000.0, 200, 200, 64),\n",
       " (0.5, 3, 1000.0, 200, 200, 128),\n",
       " (0.5, 3, 10000.0, 100, 100, 64),\n",
       " (0.5, 3, 10000.0, 100, 100, 128),\n",
       " (0.5, 3, 10000.0, 100, 200, 64),\n",
       " (0.5, 3, 10000.0, 100, 200, 128),\n",
       " (0.5, 3, 10000.0, 150, 100, 64),\n",
       " (0.5, 3, 10000.0, 150, 100, 128),\n",
       " (0.5, 3, 10000.0, 150, 200, 64),\n",
       " (0.5, 3, 10000.0, 150, 200, 128),\n",
       " (0.5, 3, 10000.0, 200, 100, 64),\n",
       " (0.5, 3, 10000.0, 200, 100, 128),\n",
       " (0.5, 3, 10000.0, 200, 200, 64),\n",
       " (0.5, 3, 10000.0, 200, 200, 128),\n",
       " (0.5, 3, 100000.0, 100, 100, 64),\n",
       " (0.5, 3, 100000.0, 100, 100, 128),\n",
       " (0.5, 3, 100000.0, 100, 200, 64),\n",
       " (0.5, 3, 100000.0, 100, 200, 128),\n",
       " (0.5, 3, 100000.0, 150, 100, 64),\n",
       " (0.5, 3, 100000.0, 150, 100, 128),\n",
       " (0.5, 3, 100000.0, 150, 200, 64),\n",
       " (0.5, 3, 100000.0, 150, 200, 128),\n",
       " (0.5, 3, 100000.0, 200, 100, 64),\n",
       " (0.5, 3, 100000.0, 200, 100, 128),\n",
       " (0.5, 3, 100000.0, 200, 200, 64),\n",
       " (0.5, 3, 100000.0, 200, 200, 128),\n",
       " (0.5, 3, 1000000.0, 100, 100, 64),\n",
       " (0.5, 3, 1000000.0, 100, 100, 128),\n",
       " (0.5, 3, 1000000.0, 100, 200, 64),\n",
       " (0.5, 3, 1000000.0, 100, 200, 128),\n",
       " (0.5, 3, 1000000.0, 150, 100, 64),\n",
       " (0.5, 3, 1000000.0, 150, 100, 128),\n",
       " (0.5, 3, 1000000.0, 150, 200, 64),\n",
       " (0.5, 3, 1000000.0, 150, 200, 128),\n",
       " (0.5, 3, 1000000.0, 200, 100, 64),\n",
       " (0.5, 3, 1000000.0, 200, 100, 128),\n",
       " (0.5, 3, 1000000.0, 200, 200, 64),\n",
       " (0.5, 3, 1000000.0, 200, 200, 128),\n",
       " (0.5, 4, 1000.0, 100, 100, 64),\n",
       " (0.5, 4, 1000.0, 100, 100, 128),\n",
       " (0.5, 4, 1000.0, 100, 200, 64),\n",
       " (0.5, 4, 1000.0, 100, 200, 128),\n",
       " (0.5, 4, 1000.0, 150, 100, 64),\n",
       " (0.5, 4, 1000.0, 150, 100, 128),\n",
       " (0.5, 4, 1000.0, 150, 200, 64),\n",
       " (0.5, 4, 1000.0, 150, 200, 128),\n",
       " (0.5, 4, 1000.0, 200, 100, 64),\n",
       " (0.5, 4, 1000.0, 200, 100, 128),\n",
       " (0.5, 4, 1000.0, 200, 200, 64),\n",
       " (0.5, 4, 1000.0, 200, 200, 128),\n",
       " (0.5, 4, 10000.0, 100, 100, 64),\n",
       " (0.5, 4, 10000.0, 100, 100, 128),\n",
       " (0.5, 4, 10000.0, 100, 200, 64),\n",
       " (0.5, 4, 10000.0, 100, 200, 128),\n",
       " (0.5, 4, 10000.0, 150, 100, 64),\n",
       " (0.5, 4, 10000.0, 150, 100, 128),\n",
       " (0.5, 4, 10000.0, 150, 200, 64),\n",
       " (0.5, 4, 10000.0, 150, 200, 128),\n",
       " (0.5, 4, 10000.0, 200, 100, 64),\n",
       " (0.5, 4, 10000.0, 200, 100, 128),\n",
       " (0.5, 4, 10000.0, 200, 200, 64),\n",
       " (0.5, 4, 10000.0, 200, 200, 128),\n",
       " (0.5, 4, 100000.0, 100, 100, 64),\n",
       " (0.5, 4, 100000.0, 100, 100, 128),\n",
       " (0.5, 4, 100000.0, 100, 200, 64),\n",
       " (0.5, 4, 100000.0, 100, 200, 128),\n",
       " (0.5, 4, 100000.0, 150, 100, 64),\n",
       " (0.5, 4, 100000.0, 150, 100, 128),\n",
       " (0.5, 4, 100000.0, 150, 200, 64),\n",
       " (0.5, 4, 100000.0, 150, 200, 128),\n",
       " (0.5, 4, 100000.0, 200, 100, 64),\n",
       " (0.5, 4, 100000.0, 200, 100, 128),\n",
       " (0.5, 4, 100000.0, 200, 200, 64),\n",
       " (0.5, 4, 100000.0, 200, 200, 128),\n",
       " (0.5, 4, 1000000.0, 100, 100, 64),\n",
       " (0.5, 4, 1000000.0, 100, 100, 128),\n",
       " (0.5, 4, 1000000.0, 100, 200, 64),\n",
       " (0.5, 4, 1000000.0, 100, 200, 128),\n",
       " (0.5, 4, 1000000.0, 150, 100, 64),\n",
       " (0.5, 4, 1000000.0, 150, 100, 128),\n",
       " (0.5, 4, 1000000.0, 150, 200, 64),\n",
       " (0.5, 4, 1000000.0, 150, 200, 128),\n",
       " (0.5, 4, 1000000.0, 200, 100, 64),\n",
       " (0.5, 4, 1000000.0, 200, 100, 128),\n",
       " (0.5, 4, 1000000.0, 200, 200, 64),\n",
       " (0.5, 4, 1000000.0, 200, 200, 128),\n",
       " (1, 1, 1000.0, 100, 100, 64),\n",
       " (1, 1, 1000.0, 100, 100, 128),\n",
       " (1, 1, 1000.0, 100, 200, 64),\n",
       " (1, 1, 1000.0, 100, 200, 128),\n",
       " (1, 1, 1000.0, 150, 100, 64),\n",
       " (1, 1, 1000.0, 150, 100, 128),\n",
       " (1, 1, 1000.0, 150, 200, 64),\n",
       " (1, 1, 1000.0, 150, 200, 128),\n",
       " (1, 1, 1000.0, 200, 100, 64),\n",
       " (1, 1, 1000.0, 200, 100, 128),\n",
       " (1, 1, 1000.0, 200, 200, 64),\n",
       " (1, 1, 1000.0, 200, 200, 128),\n",
       " (1, 1, 10000.0, 100, 100, 64),\n",
       " (1, 1, 10000.0, 100, 100, 128),\n",
       " (1, 1, 10000.0, 100, 200, 64),\n",
       " (1, 1, 10000.0, 100, 200, 128),\n",
       " (1, 1, 10000.0, 150, 100, 64),\n",
       " (1, 1, 10000.0, 150, 100, 128),\n",
       " (1, 1, 10000.0, 150, 200, 64),\n",
       " (1, 1, 10000.0, 150, 200, 128),\n",
       " (1, 1, 10000.0, 200, 100, 64),\n",
       " (1, 1, 10000.0, 200, 100, 128),\n",
       " (1, 1, 10000.0, 200, 200, 64),\n",
       " (1, 1, 10000.0, 200, 200, 128),\n",
       " (1, 1, 100000.0, 100, 100, 64),\n",
       " (1, 1, 100000.0, 100, 100, 128),\n",
       " (1, 1, 100000.0, 100, 200, 64),\n",
       " (1, 1, 100000.0, 100, 200, 128),\n",
       " (1, 1, 100000.0, 150, 100, 64),\n",
       " (1, 1, 100000.0, 150, 100, 128),\n",
       " (1, 1, 100000.0, 150, 200, 64),\n",
       " (1, 1, 100000.0, 150, 200, 128),\n",
       " (1, 1, 100000.0, 200, 100, 64),\n",
       " (1, 1, 100000.0, 200, 100, 128),\n",
       " (1, 1, 100000.0, 200, 200, 64),\n",
       " (1, 1, 100000.0, 200, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 100, 128),\n",
       " (1, 1, 1000000.0, 100, 200, 64),\n",
       " (1, 1, 1000000.0, 100, 200, 128),\n",
       " (1, 1, 1000000.0, 150, 100, 64),\n",
       " (1, 1, 1000000.0, 150, 100, 128),\n",
       " (1, 1, 1000000.0, 150, 200, 64),\n",
       " (1, 1, 1000000.0, 150, 200, 128),\n",
       " (1, 1, 1000000.0, 200, 100, 64),\n",
       " (1, 1, 1000000.0, 200, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 200, 128),\n",
       " (1, 2, 1000.0, 100, 100, 64),\n",
       " (1, 2, 1000.0, 100, 100, 128),\n",
       " (1, 2, 1000.0, 100, 200, 64),\n",
       " (1, 2, 1000.0, 100, 200, 128),\n",
       " (1, 2, 1000.0, 150, 100, 64),\n",
       " (1, 2, 1000.0, 150, 100, 128),\n",
       " (1, 2, 1000.0, 150, 200, 64),\n",
       " (1, 2, 1000.0, 150, 200, 128),\n",
       " (1, 2, 1000.0, 200, 100, 64),\n",
       " (1, 2, 1000.0, 200, 100, 128),\n",
       " (1, 2, 1000.0, 200, 200, 64),\n",
       " (1, 2, 1000.0, 200, 200, 128),\n",
       " (1, 2, 10000.0, 100, 100, 64),\n",
       " (1, 2, 10000.0, 100, 100, 128),\n",
       " (1, 2, 10000.0, 100, 200, 64),\n",
       " (1, 2, 10000.0, 100, 200, 128),\n",
       " (1, 2, 10000.0, 150, 100, 64),\n",
       " (1, 2, 10000.0, 150, 100, 128),\n",
       " (1, 2, 10000.0, 150, 200, 64),\n",
       " (1, 2, 10000.0, 150, 200, 128),\n",
       " (1, 2, 10000.0, 200, 100, 64),\n",
       " (1, 2, 10000.0, 200, 100, 128),\n",
       " (1, 2, 10000.0, 200, 200, 64),\n",
       " (1, 2, 10000.0, 200, 200, 128),\n",
       " (1, 2, 100000.0, 100, 100, 64),\n",
       " (1, 2, 100000.0, 100, 100, 128),\n",
       " (1, 2, 100000.0, 100, 200, 64),\n",
       " (1, 2, 100000.0, 100, 200, 128),\n",
       " (1, 2, 100000.0, 150, 100, 64),\n",
       " (1, 2, 100000.0, 150, 100, 128),\n",
       " (1, 2, 100000.0, 150, 200, 64),\n",
       " (1, 2, 100000.0, 150, 200, 128),\n",
       " (1, 2, 100000.0, 200, 100, 64),\n",
       " (1, 2, 100000.0, 200, 100, 128),\n",
       " (1, 2, 100000.0, 200, 200, 64),\n",
       " (1, 2, 100000.0, 200, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 100, 128),\n",
       " (1, 2, 1000000.0, 100, 200, 64),\n",
       " (1, 2, 1000000.0, 100, 200, 128),\n",
       " (1, 2, 1000000.0, 150, 100, 64),\n",
       " (1, 2, 1000000.0, 150, 100, 128),\n",
       " (1, 2, 1000000.0, 150, 200, 64),\n",
       " (1, 2, 1000000.0, 150, 200, 128),\n",
       " (1, 2, 1000000.0, 200, 100, 64),\n",
       " (1, 2, 1000000.0, 200, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 200, 128),\n",
       " (1, 3, 1000.0, 100, 100, 64),\n",
       " (1, 3, 1000.0, 100, 100, 128),\n",
       " (1, 3, 1000.0, 100, 200, 64),\n",
       " (1, 3, 1000.0, 100, 200, 128),\n",
       " (1, 3, 1000.0, 150, 100, 64),\n",
       " (1, 3, 1000.0, 150, 100, 128),\n",
       " (1, 3, 1000.0, 150, 200, 64),\n",
       " (1, 3, 1000.0, 150, 200, 128),\n",
       " (1, 3, 1000.0, 200, 100, 64),\n",
       " (1, 3, 1000.0, 200, 100, 128),\n",
       " (1, 3, 1000.0, 200, 200, 64),\n",
       " (1, 3, 1000.0, 200, 200, 128),\n",
       " (1, 3, 10000.0, 100, 100, 64),\n",
       " (1, 3, 10000.0, 100, 100, 128),\n",
       " (1, 3, 10000.0, 100, 200, 64),\n",
       " (1, 3, 10000.0, 100, 200, 128),\n",
       " (1, 3, 10000.0, 150, 100, 64),\n",
       " (1, 3, 10000.0, 150, 100, 128),\n",
       " (1, 3, 10000.0, 150, 200, 64),\n",
       " (1, 3, 10000.0, 150, 200, 128),\n",
       " (1, 3, 10000.0, 200, 100, 64),\n",
       " (1, 3, 10000.0, 200, 100, 128),\n",
       " (1, 3, 10000.0, 200, 200, 64),\n",
       " (1, 3, 10000.0, 200, 200, 128),\n",
       " (1, 3, 100000.0, 100, 100, 64),\n",
       " (1, 3, 100000.0, 100, 100, 128),\n",
       " (1, 3, 100000.0, 100, 200, 64),\n",
       " (1, 3, 100000.0, 100, 200, 128),\n",
       " (1, 3, 100000.0, 150, 100, 64),\n",
       " (1, 3, 100000.0, 150, 100, 128),\n",
       " (1, 3, 100000.0, 150, 200, 64),\n",
       " (1, 3, 100000.0, 150, 200, 128),\n",
       " (1, 3, 100000.0, 200, 100, 64),\n",
       " (1, 3, 100000.0, 200, 100, 128),\n",
       " (1, 3, 100000.0, 200, 200, 64),\n",
       " (1, 3, 100000.0, 200, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 100, 128),\n",
       " (1, 3, 1000000.0, 100, 200, 64),\n",
       " (1, 3, 1000000.0, 100, 200, 128),\n",
       " (1, 3, 1000000.0, 150, 100, 64),\n",
       " (1, 3, 1000000.0, 150, 100, 128),\n",
       " (1, 3, 1000000.0, 150, 200, 64),\n",
       " (1, 3, 1000000.0, 150, 200, 128),\n",
       " (1, 3, 1000000.0, 200, 100, 64),\n",
       " (1, 3, 1000000.0, 200, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 200, 128),\n",
       " (1, 4, 1000.0, 100, 100, 64),\n",
       " (1, 4, 1000.0, 100, 100, 128),\n",
       " (1, 4, 1000.0, 100, 200, 64),\n",
       " (1, 4, 1000.0, 100, 200, 128),\n",
       " (1, 4, 1000.0, 150, 100, 64),\n",
       " (1, 4, 1000.0, 150, 100, 128),\n",
       " (1, 4, 1000.0, 150, 200, 64),\n",
       " (1, 4, 1000.0, 150, 200, 128),\n",
       " (1, 4, 1000.0, 200, 100, 64),\n",
       " (1, 4, 1000.0, 200, 100, 128),\n",
       " (1, 4, 1000.0, 200, 200, 64),\n",
       " (1, 4, 1000.0, 200, 200, 128),\n",
       " (1, 4, 10000.0, 100, 100, 64),\n",
       " (1, 4, 10000.0, 100, 100, 128),\n",
       " (1, 4, 10000.0, 100, 200, 64),\n",
       " (1, 4, 10000.0, 100, 200, 128),\n",
       " (1, 4, 10000.0, 150, 100, 64),\n",
       " (1, 4, 10000.0, 150, 100, 128),\n",
       " (1, 4, 10000.0, 150, 200, 64),\n",
       " (1, 4, 10000.0, 150, 200, 128),\n",
       " (1, 4, 10000.0, 200, 100, 64),\n",
       " (1, 4, 10000.0, 200, 100, 128),\n",
       " (1, 4, 10000.0, 200, 200, 64),\n",
       " (1, 4, 10000.0, 200, 200, 128),\n",
       " (1, 4, 100000.0, 100, 100, 64),\n",
       " (1, 4, 100000.0, 100, 100, 128),\n",
       " (1, 4, 100000.0, 100, 200, 64),\n",
       " (1, 4, 100000.0, 100, 200, 128),\n",
       " (1, 4, 100000.0, 150, 100, 64),\n",
       " (1, 4, 100000.0, 150, 100, 128),\n",
       " (1, 4, 100000.0, 150, 200, 64),\n",
       " (1, 4, 100000.0, 150, 200, 128),\n",
       " (1, 4, 100000.0, 200, 100, 64),\n",
       " (1, 4, 100000.0, 200, 100, 128),\n",
       " (1, 4, 100000.0, 200, 200, 64),\n",
       " (1, 4, 100000.0, 200, 200, 128),\n",
       " (1, 4, 1000000.0, 100, 100, 64),\n",
       " (1, 4, 1000000.0, 100, 100, 128),\n",
       " (1, 4, 1000000.0, 100, 200, 64),\n",
       " (1, 4, 1000000.0, 100, 200, 128),\n",
       " (1, 4, 1000000.0, 150, 100, 64),\n",
       " (1, 4, 1000000.0, 150, 100, 128),\n",
       " (1, 4, 1000000.0, 150, 200, 64),\n",
       " (1, 4, 1000000.0, 150, 200, 128),\n",
       " (1, 4, 1000000.0, 200, 100, 64),\n",
       " (1, 4, 1000000.0, 200, 100, 128),\n",
       " (1, 4, 1000000.0, 200, 200, 64),\n",
       " (1, 4, 1000000.0, 200, 200, 128),\n",
       " (10, 1, 1000.0, 100, 100, 64),\n",
       " (10, 1, 1000.0, 100, 100, 128),\n",
       " (10, 1, 1000.0, 100, 200, 64),\n",
       " (10, 1, 1000.0, 100, 200, 128),\n",
       " (10, 1, 1000.0, 150, 100, 64),\n",
       " (10, 1, 1000.0, 150, 100, 128),\n",
       " (10, 1, 1000.0, 150, 200, 64),\n",
       " (10, 1, 1000.0, 150, 200, 128),\n",
       " (10, 1, 1000.0, 200, 100, 64),\n",
       " (10, 1, 1000.0, 200, 100, 128),\n",
       " (10, 1, 1000.0, 200, 200, 64),\n",
       " (10, 1, 1000.0, 200, 200, 128),\n",
       " (10, 1, 10000.0, 100, 100, 64),\n",
       " (10, 1, 10000.0, 100, 100, 128),\n",
       " (10, 1, 10000.0, 100, 200, 64),\n",
       " (10, 1, 10000.0, 100, 200, 128),\n",
       " (10, 1, 10000.0, 150, 100, 64),\n",
       " (10, 1, 10000.0, 150, 100, 128),\n",
       " (10, 1, 10000.0, 150, 200, 64),\n",
       " (10, 1, 10000.0, 150, 200, 128),\n",
       " (10, 1, 10000.0, 200, 100, 64),\n",
       " (10, 1, 10000.0, 200, 100, 128),\n",
       " (10, 1, 10000.0, 200, 200, 64),\n",
       " (10, 1, 10000.0, 200, 200, 128),\n",
       " (10, 1, 100000.0, 100, 100, 64),\n",
       " (10, 1, 100000.0, 100, 100, 128),\n",
       " (10, 1, 100000.0, 100, 200, 64),\n",
       " (10, 1, 100000.0, 100, 200, 128),\n",
       " (10, 1, 100000.0, 150, 100, 64),\n",
       " (10, 1, 100000.0, 150, 100, 128),\n",
       " (10, 1, 100000.0, 150, 200, 64),\n",
       " (10, 1, 100000.0, 150, 200, 128),\n",
       " (10, 1, 100000.0, 200, 100, 64),\n",
       " (10, 1, 100000.0, 200, 100, 128),\n",
       " (10, 1, 100000.0, 200, 200, 64),\n",
       " (10, 1, 100000.0, 200, 200, 128),\n",
       " (10, 1, 1000000.0, 100, 100, 64),\n",
       " (10, 1, 1000000.0, 100, 100, 128),\n",
       " (10, 1, 1000000.0, 100, 200, 64),\n",
       " (10, 1, 1000000.0, 100, 200, 128),\n",
       " ...]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = [torch.optim.Adam(model.parameters(), \n",
    "                               lr=learning_rate),             \n",
    "              torch.optim.SGD(model.parameters(), \n",
    "                              lr=learning_rate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[14269 16316  4649 ...  8312 14965 23410]\n",
      "20000\n",
      "20000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)\n",
    "\n",
    "shuffled_index[:training_size]\n",
    "\n",
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))\n",
    "\n",
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all ngram tokens for easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams = params[1]\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "grams = \n",
    "\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram=grams)\n",
    "\n",
    "# Tokenize Validation\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                      n_gram=grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['maddy',\n",
       "  'debbie',\n",
       "  'rochon',\n",
       "  'is',\n",
       "  'a',\n",
       "  'mentally',\n",
       "  'unstable',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'with',\n",
       "  'a',\n",
       "  'troubled',\n",
       "  'past',\n",
       "  'who',\n",
       "  'gets',\n",
       "  'more',\n",
       "  'than',\n",
       "  'she',\n",
       "  'bargained',\n",
       "  'for',\n",
       "  'when',\n",
       "  'she',\n",
       "  'goes',\n",
       "  'to',\n",
       "  'a',\n",
       "  'pool',\n",
       "  'party',\n",
       "  'with',\n",
       "  'a',\n",
       "  'handsome',\n",
       "  'coworker',\n",
       "  'when',\n",
       "  'her',\n",
       "  'date',\n",
       "  'and',\n",
       "  'his',\n",
       "  'friends',\n",
       "  'jokingly',\n",
       "  'say',\n",
       "  'they',\n",
       "  'belong',\n",
       "  'to',\n",
       "  'a',\n",
       "  'murder',\n",
       "  'club',\n",
       "  'maddy',\n",
       "  'takes',\n",
       "  'it',\n",
       "  'seriously',\n",
       "  'and',\n",
       "  'moves',\n",
       "  'straight',\n",
       "  'up',\n",
       "  'to',\n",
       "  'level',\n",
       "  '3',\n",
       "  'by',\n",
       "  'bashing',\n",
       "  'in',\n",
       "  'the',\n",
       "  'brains',\n",
       "  'of',\n",
       "  'a',\n",
       "  'woman',\n",
       "  'in',\n",
       "  'a',\n",
       "  'parking',\n",
       "  'garage',\n",
       "  'for',\n",
       "  'denting',\n",
       "  'her',\n",
       "  'car',\n",
       "  'but',\n",
       "  'is',\n",
       "  'maddy',\n",
       "  'also',\n",
       "  'the',\n",
       "  'one',\n",
       "  'donning',\n",
       "  'a',\n",
       "  'plastic',\n",
       "  'mask',\n",
       "  'and',\n",
       "  'killing',\n",
       "  'off',\n",
       "  'other',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'group',\n",
       "  'or',\n",
       "  'has',\n",
       "  'someone',\n",
       "  'else',\n",
       "  'lost',\n",
       "  'it?the',\n",
       "  'plot',\n",
       "  'of',\n",
       "  'this',\n",
       "  'film',\n",
       "  'originally',\n",
       "  'titled',\n",
       "  'make',\n",
       "  'em',\n",
       "  'bleed',\n",
       "  'is',\n",
       "  'very',\n",
       "  'poorly',\n",
       "  'conceived',\n",
       "  'full',\n",
       "  'of',\n",
       "  'holes',\n",
       "  'and',\n",
       "  'spirals',\n",
       "  'completely',\n",
       "  'out',\n",
       "  'of',\n",
       "  'control',\n",
       "  'before',\n",
       "  'a',\n",
       "  'ludicrous',\n",
       "  'out',\n",
       "  'of',\n",
       "  'left',\n",
       "  'field',\n",
       "  'twist',\n",
       "  'ending',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'dialogue',\n",
       "  'is',\n",
       "  'downright',\n",
       "  'laughable',\n",
       "  'i',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'have',\n",
       "  'a',\n",
       "  'problem',\n",
       "  'with',\n",
       "  'rochon',\n",
       "  \"'s\",\n",
       "  'performance',\n",
       "  'but',\n",
       "  'the',\n",
       "  'supporting',\n",
       "  'cast',\n",
       "  'was',\n",
       "  'atrocious',\n",
       "  'however',\n",
       "  'i',\n",
       "  'managed',\n",
       "  'to',\n",
       "  'sit',\n",
       "  'through',\n",
       "  'this',\n",
       "  'full',\n",
       "  'moon',\n",
       "  'release',\n",
       "  'thoroughly',\n",
       "  'entertained',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'plenty',\n",
       "  'of',\n",
       "  'skin',\n",
       "  'and',\n",
       "  'blood',\n",
       "  'and',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'type',\n",
       "  'of',\n",
       "  'flick',\n",
       "  'to',\n",
       "  'sit',\n",
       "  'around',\n",
       "  'with',\n",
       "  'a',\n",
       "  'group',\n",
       "  'of',\n",
       "  'your',\n",
       "  'buddies',\n",
       "  'and',\n",
       "  'pick',\n",
       "  'apart',\n",
       "  'horror',\n",
       "  'fans',\n",
       "  'may',\n",
       "  'also',\n",
       "  'enjoy',\n",
       "  'the',\n",
       "  'cameos',\n",
       "  'from',\n",
       "  'brinke',\n",
       "  'stevens',\n",
       "  'and',\n",
       "  'lloyd',\n",
       "  'kaufman',\n",
       "  'as',\n",
       "  'debbie',\n",
       "  \"'s\",\n",
       "  'parents',\n",
       "  'and',\n",
       "  'julie',\n",
       "  'strain',\n",
       "  'an',\n",
       "  'early',\n",
       "  'victim).score',\n",
       "  '4',\n",
       "  'out',\n",
       "  'of',\n",
       "  '10'],\n",
       " ['following',\n",
       "  'the',\n",
       "  'release',\n",
       "  'of',\n",
       "  'cube',\n",
       "  '2',\n",
       "  'hypercube',\n",
       "  '2003',\n",
       "  'and',\n",
       "  'playing',\n",
       "  'off',\n",
       "  'the',\n",
       "  'alleged',\n",
       "  'success',\n",
       "  'of',\n",
       "  'the',\n",
       "  'original',\n",
       "  'cube',\n",
       "  '1998',\n",
       "  'director',\n",
       "  'ernie',\n",
       "  'barbarash',\n",
       "  'takes',\n",
       "  'the',\n",
       "  'liberty',\n",
       "  'of',\n",
       "  'bringing',\n",
       "  'us',\n",
       "  'the',\n",
       "  'third',\n",
       "  'installment',\n",
       "  'in',\n",
       "  'the',\n",
       "  'trilogy',\n",
       "  'the',\n",
       "  'prequel',\n",
       "  'cube',\n",
       "  'zero',\n",
       "  'deep',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bowels',\n",
       "  'of',\n",
       "  'a',\n",
       "  'giant',\n",
       "  'and',\n",
       "  'faceless',\n",
       "  'institution',\n",
       "  'time',\n",
       "  'and',\n",
       "  'place',\n",
       "  'unknown',\n",
       "  'two',\n",
       "  'low',\n",
       "  'ranking',\n",
       "  'operators',\n",
       "  'wynn',\n",
       "  'zachary',\n",
       "  'bennett',\n",
       "  'and',\n",
       "  'dodd',\n",
       "  'david',\n",
       "  'huband',\n",
       "  'sit',\n",
       "  'and',\n",
       "  'observe',\n",
       "  'on',\n",
       "  'monitors',\n",
       "  'the',\n",
       "  'behavior',\n",
       "  'of',\n",
       "  'people',\n",
       "  'that',\n",
       "  'have',\n",
       "  'been',\n",
       "  'placed',\n",
       "  'in',\n",
       "  'a',\n",
       "  'giant',\n",
       "  'network',\n",
       "  'of',\n",
       "  'cubic',\n",
       "  'chambers',\n",
       "  'some',\n",
       "  'of',\n",
       "  'which',\n",
       "  'are',\n",
       "  'rigged',\n",
       "  'with',\n",
       "  'death',\n",
       "  'traps',\n",
       "  'told',\n",
       "  'that',\n",
       "  'the',\n",
       "  'people',\n",
       "  'they',\n",
       "  'are',\n",
       "  'observing',\n",
       "  'are',\n",
       "  'convicted',\n",
       "  'felons',\n",
       "  'who',\n",
       "  'chose',\n",
       "  'this',\n",
       "  'horrific',\n",
       "  'and',\n",
       "  'deadly',\n",
       "  'ordeal',\n",
       "  'over',\n",
       "  'a',\n",
       "  'lethal',\n",
       "  'injection',\n",
       "  'these',\n",
       "  'observers',\n",
       "  'have',\n",
       "  'had',\n",
       "  'no',\n",
       "  'problem',\n",
       "  'with',\n",
       "  'their',\n",
       "  'jobs',\n",
       "  'until',\n",
       "  'wynn',\n",
       "  'a',\n",
       "  'mathematical',\n",
       "  'genius',\n",
       "  'discovers',\n",
       "  'that',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'prisoners',\n",
       "  'a',\n",
       "  'woman',\n",
       "  'named',\n",
       "  'cassandra',\n",
       "  'stephanie',\n",
       "  'moore',\n",
       "  'never',\n",
       "  'agreed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'put',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'cube',\n",
       "  'suddenly',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'realized',\n",
       "  'that',\n",
       "  'perhaps',\n",
       "  'their',\n",
       "  'jobs',\n",
       "  'are',\n",
       "  'not',\n",
       "  'what',\n",
       "  'they',\n",
       "  'seem',\n",
       "  'and',\n",
       "  'that',\n",
       "  'they',\n",
       "  'may',\n",
       "  'be',\n",
       "  'part',\n",
       "  'of',\n",
       "  'something',\n",
       "  'deeply',\n",
       "  'sick',\n",
       "  'and',\n",
       "  'twisted',\n",
       "  '...',\n",
       "  'for',\n",
       "  'people',\n",
       "  'that',\n",
       "  'have',\n",
       "  'seen',\n",
       "  'and',\n",
       "  'enjoyed',\n",
       "  'the',\n",
       "  'original',\n",
       "  'cube',\n",
       "  'this',\n",
       "  'prequel',\n",
       "  'will',\n",
       "  'probably',\n",
       "  'not',\n",
       "  'be',\n",
       "  'to',\n",
       "  'your',\n",
       "  'liking',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'not',\n",
       "  'that',\n",
       "  'the',\n",
       "  'story',\n",
       "  'does',\n",
       "  'not',\n",
       "  'have',\n",
       "  'potential',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'simply',\n",
       "  'that',\n",
       "  'the',\n",
       "  'first',\n",
       "  'cube',\n",
       "  'film',\n",
       "  'never',\n",
       "  'needed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'expanded',\n",
       "  'on',\n",
       "  'standing',\n",
       "  'alone',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'neat',\n",
       "  'little',\n",
       "  'psychological',\n",
       "  'thriller',\n",
       "  'with',\n",
       "  'very',\n",
       "  'interesting',\n",
       "  'concepts',\n",
       "  'and',\n",
       "  'a',\n",
       "  'certainty',\n",
       "  'about',\n",
       "  'its',\n",
       "  'own',\n",
       "  'message',\n",
       "  'it',\n",
       "  'was',\n",
       "  'also',\n",
       "  'nicely',\n",
       "  'self',\n",
       "  'contained',\n",
       "  'the',\n",
       "  'problem',\n",
       "  'with',\n",
       "  'cube',\n",
       "  'zero',\n",
       "  'is',\n",
       "  'that',\n",
       "  'it',\n",
       "  'destroys',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'mystique',\n",
       "  'of',\n",
       "  'the',\n",
       "  'original',\n",
       "  'attempting',\n",
       "  'to',\n",
       "  'answer',\n",
       "  'questions',\n",
       "  'with',\n",
       "  'more',\n",
       "  'questions',\n",
       "  'but',\n",
       "  'only',\n",
       "  'really',\n",
       "  'resulting',\n",
       "  'in',\n",
       "  'making',\n",
       "  'a',\n",
       "  'mess',\n",
       "  'of',\n",
       "  'what',\n",
       "  'never',\n",
       "  'needed',\n",
       "  'fixing',\n",
       "  'what',\n",
       "  'this',\n",
       "  'new',\n",
       "  'film',\n",
       "  'has',\n",
       "  'to',\n",
       "  'offer',\n",
       "  'which',\n",
       "  'is',\n",
       "  'questions',\n",
       "  'about',\n",
       "  'the',\n",
       "  'psychological',\n",
       "  'nature',\n",
       "  'of',\n",
       "  'authoritarianism',\n",
       "  'and',\n",
       "  'the',\n",
       "  'banality',\n",
       "  'of',\n",
       "  'evil',\n",
       "  'certainly',\n",
       "  'are',\n",
       "  'good',\n",
       "  'questions',\n",
       "  'to',\n",
       "  'be',\n",
       "  'raised',\n",
       "  'but',\n",
       "  'probably',\n",
       "  'should',\n",
       "  'have',\n",
       "  'been',\n",
       "  'done',\n",
       "  'so',\n",
       "  'on',\n",
       "  'their',\n",
       "  'own',\n",
       "  'merits',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'as',\n",
       "  'a',\n",
       "  'continuation',\n",
       "  'of',\n",
       "  'a',\n",
       "  'film',\n",
       "  'that',\n",
       "  'had',\n",
       "  'no',\n",
       "  'such',\n",
       "  'aspirations',\n",
       "  'having',\n",
       "  'said',\n",
       "  'this',\n",
       "  'the',\n",
       "  'other',\n",
       "  'traits',\n",
       "  'of',\n",
       "  'the',\n",
       "  'film',\n",
       "  'such',\n",
       "  'as',\n",
       "  'acting',\n",
       "  'and',\n",
       "  'direction',\n",
       "  'and',\n",
       "  'writing',\n",
       "  'are',\n",
       "  'not',\n",
       "  'awful',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'bleak',\n",
       "  'dark',\n",
       "  'look',\n",
       "  'to',\n",
       "  'the',\n",
       "  'film',\n",
       "  'akin',\n",
       "  'to',\n",
       "  'such',\n",
       "  'film',\n",
       "  'noir',\n",
       "  'as',\n",
       "  'the',\n",
       "  'matrix',\n",
       "  'and',\n",
       "  'dark',\n",
       "  'city',\n",
       "  'and',\n",
       "  'they',\n",
       "  'have',\n",
       "  'certainly',\n",
       "  'managed',\n",
       "  'to',\n",
       "  'recapture',\n",
       "  'the',\n",
       "  'claustrophobic',\n",
       "  'feeling',\n",
       "  'of',\n",
       "  'the',\n",
       "  'first',\n",
       "  'cube',\n",
       "  'unfortunately',\n",
       "  'for',\n",
       "  'barbarash',\n",
       "  'these',\n",
       "  'are',\n",
       "  'not',\n",
       "  'enough',\n",
       "  'positive',\n",
       "  'qualities',\n",
       "  'to',\n",
       "  'save',\n",
       "  'it'],\n",
       " ['nick',\n",
       "  'nolte',\n",
       "  'gives',\n",
       "  'an',\n",
       "  'excellent',\n",
       "  'performance',\n",
       "  'in',\n",
       "  'kurt',\n",
       "  'vonnegut',\n",
       "  \"'s\",\n",
       "  'dark',\n",
       "  'tale',\n",
       "  'notle',\n",
       "  'plays',\n",
       "  'howard',\n",
       "  'w.',\n",
       "  'campbell',\n",
       "  'who',\n",
       "  'was',\n",
       "  'a',\n",
       "  'double',\n",
       "  'agent',\n",
       "  'working',\n",
       "  'in',\n",
       "  'propaganda',\n",
       "  'during',\n",
       "  'world',\n",
       "  'war',\n",
       "  'ii',\n",
       "  'after',\n",
       "  'the',\n",
       "  'war',\n",
       "  'he',\n",
       "  'lives',\n",
       "  'anonymously',\n",
       "  'until',\n",
       "  'competing',\n",
       "  'factions',\n",
       "  'wish',\n",
       "  'to',\n",
       "  'dig',\n",
       "  'up',\n",
       "  'his',\n",
       "  'past',\n",
       "  'as',\n",
       "  'with',\n",
       "  'much',\n",
       "  'of',\n",
       "  'vonnegut',\n",
       "  \"'s\",\n",
       "  'work',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'meditation',\n",
       "  'on',\n",
       "  'the',\n",
       "  'absurdity',\n",
       "  'of',\n",
       "  'war',\n",
       "  'and',\n",
       "  'those',\n",
       "  'who',\n",
       "  'use',\n",
       "  'propaganda',\n",
       "  'for',\n",
       "  'their',\n",
       "  'own',\n",
       "  'aims',\n",
       "  'nolte',\n",
       "  'is',\n",
       "  'fantastic',\n",
       "  'self',\n",
       "  'assured',\n",
       "  'and',\n",
       "  'confident',\n",
       "  'as',\n",
       "  'the',\n",
       "  'younger',\n",
       "  'campbell',\n",
       "  'and',\n",
       "  'then',\n",
       "  'broken',\n",
       "  'and',\n",
       "  'haunted',\n",
       "  'as',\n",
       "  'the',\n",
       "  'older',\n",
       "  'man',\n",
       "  'who',\n",
       "  'is',\n",
       "  'forced',\n",
       "  'to',\n",
       "  'atone',\n",
       "  'for',\n",
       "  'the',\n",
       "  'sins',\n",
       "  'of',\n",
       "  'his',\n",
       "  'past',\n",
       "  '7',\n",
       "  'out',\n",
       "  'of',\n",
       "  '10'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'surprised',\n",
       "  'than',\n",
       "  'many',\n",
       "  'viewers',\n",
       "  'hold',\n",
       "  'more',\n",
       "  'respect',\n",
       "  'for',\n",
       "  'the',\n",
       "  'sequel',\n",
       "  'to',\n",
       "  'this',\n",
       "  'brilliant',\n",
       "  'movie',\n",
       "  '...',\n",
       "  'i',\n",
       "  'have',\n",
       "  'seen',\n",
       "  'all',\n",
       "  'the',\n",
       "  'guinea',\n",
       "  'pigs',\n",
       "  'and',\n",
       "  'this',\n",
       "  'one',\n",
       "  'is',\n",
       "  'easily',\n",
       "  'the',\n",
       "  'best',\n",
       "  'even',\n",
       "  'though',\n",
       "  'i',\n",
       "  've',\n",
       "  'seen',\n",
       "  'the',\n",
       "  'making',\n",
       "  'of',\n",
       "  'i',\n",
       "  'still',\n",
       "  'have',\n",
       "  'doubts',\n",
       "  'when',\n",
       "  'watching',\n",
       "  'those',\n",
       "  '35mins',\n",
       "  'of',\n",
       "  'pure',\n",
       "  'torture',\n",
       "  'its',\n",
       "  'that',\n",
       "  'powerful',\n",
       "  'a',\n",
       "  '10',\n",
       "  'out',\n",
       "  'of',\n",
       "  '10',\n",
       "  'because',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'achieved',\n",
       "  'perfectly',\n",
       "  'what',\n",
       "  'it',\n",
       "  'set',\n",
       "  'out',\n",
       "  'to',\n",
       "  'do',\n",
       "  'be',\n",
       "  'the',\n",
       "  'best',\n",
       "  'fake',\n",
       "  'snuff',\n",
       "  'film',\n",
       "  'ever',\n",
       "  'made'],\n",
       " ['i',\n",
       "  \"'ll\",\n",
       "  'tell',\n",
       "  'you',\n",
       "  'a',\n",
       "  'tale',\n",
       "  'of',\n",
       "  'the',\n",
       "  'summer',\n",
       "  'of',\n",
       "  '1994',\n",
       "  'a',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'i',\n",
       "  'attended',\n",
       "  'a',\n",
       "  'canada',\n",
       "  'day',\n",
       "  'concert',\n",
       "  'in',\n",
       "  'barrie',\n",
       "  'and',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'who',\n",
       "  \"'s\",\n",
       "  'who',\n",
       "  'of',\n",
       "  'the',\n",
       "  'top',\n",
       "  'canadian',\n",
       "  'bands',\n",
       "  'of',\n",
       "  'the',\n",
       "  'age',\n",
       "  'we',\n",
       "  'got',\n",
       "  'there',\n",
       "  'about',\n",
       "  '4',\n",
       "  'am',\n",
       "  'waited',\n",
       "  'in',\n",
       "  'line',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'and',\n",
       "  'when',\n",
       "  'the',\n",
       "  'doors',\n",
       "  'opened',\n",
       "  'at',\n",
       "  '9',\n",
       "  'am',\n",
       "  'we',\n",
       "  'were',\n",
       "  'among',\n",
       "  'the',\n",
       "  'first',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'gates',\n",
       "  'we',\n",
       "  'then',\n",
       "  'waited',\n",
       "  'and',\n",
       "  'waited',\n",
       "  'in',\n",
       "  'the',\n",
       "  'hot',\n",
       "  'sun',\n",
       "  'slowly',\n",
       "  'broiling',\n",
       "  'but',\n",
       "  'we',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'care',\n",
       "  'because',\n",
       "  'the',\n",
       "  'headliners',\n",
       "  'were',\n",
       "  'among',\n",
       "  'our',\n",
       "  'favourites',\n",
       "  'at',\n",
       "  'one',\n",
       "  'point',\n",
       "  'early',\n",
       "  'in',\n",
       "  'the',\n",
       "  'afternoon',\n",
       "  'i',\n",
       "  'sat',\n",
       "  'down',\n",
       "  'and',\n",
       "  'dozed',\n",
       "  'off',\n",
       "  'with',\n",
       "  'my',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'barrier',\n",
       "  'i',\n",
       "  'was',\n",
       "  'awakened',\n",
       "  'to',\n",
       "  'my',\n",
       "  'shock',\n",
       "  'and',\n",
       "  'dismay',\n",
       "  'by',\n",
       "  'a',\n",
       "  'shrieking',\n",
       "  'girl',\n",
       "  'wearing',\n",
       "  'a',\n",
       "  'rheostatics',\n",
       "  't',\n",
       "  'shirt',\n",
       "  'this',\n",
       "  'is',\n",
       "  'the',\n",
       "  'reason',\n",
       "  'i',\n",
       "  'have',\n",
       "  'hated',\n",
       "  'the',\n",
       "  'rheostatics',\n",
       "  'to',\n",
       "  'this',\n",
       "  'day',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'nothing',\n",
       "  'reasonable',\n",
       "  'nor',\n",
       "  'taste',\n",
       "  'determined',\n",
       "  'nor',\n",
       "  'really',\n",
       "  'anything',\n",
       "  'except',\n",
       "  'their',\n",
       "  'fandom',\n",
       "  'snotty',\n",
       "  'of',\n",
       "  'me',\n",
       "  'is',\n",
       "  \"n't\",\n",
       "  'it',\n",
       "  'so',\n",
       "  'i',\n",
       "  'in',\n",
       "  'my',\n",
       "  'hatred',\n",
       "  'of',\n",
       "  'the',\n",
       "  'band',\n",
       "  'have',\n",
       "  'denied',\n",
       "  'myself',\n",
       "  'the',\n",
       "  'delight',\n",
       "  'that',\n",
       "  'is',\n",
       "  'whale',\n",
       "  'music',\n",
       "  'desmond',\n",
       "  'howl',\n",
       "  'had',\n",
       "  'it',\n",
       "  'all',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'hard',\n",
       "  'to',\n",
       "  'say',\n",
       "  'what',\n",
       "  'he',\n",
       "  \"'s\",\n",
       "  'lost',\n",
       "  'since',\n",
       "  'he',\n",
       "  'lives',\n",
       "  'in',\n",
       "  'a',\n",
       "  'fantastic',\n",
       "  'mansion',\n",
       "  'wedged',\n",
       "  'between',\n",
       "  'the',\n",
       "  'ocean',\n",
       "  'and',\n",
       "  'the',\n",
       "  'mountains',\n",
       "  'the',\n",
       "  'bc',\n",
       "  'region',\n",
       "  'where',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'shot',\n",
       "  'is',\n",
       "  'breathtaking',\n",
       "  'the',\n",
       "  'life',\n",
       "  'most',\n",
       "  'of',\n",
       "  'us',\n",
       "  'dream',\n",
       "  'of',\n",
       "  'is',\n",
       "  'dismantled',\n",
       "  'by',\n",
       "  'dreams',\n",
       "  'phantoms',\n",
       "  'and',\n",
       "  'his',\n",
       "  'own',\n",
       "  'past',\n",
       "  'until',\n",
       "  'the',\n",
       "  'day',\n",
       "  'a',\n",
       "  'teenaged',\n",
       "  'criminal',\n",
       "  'breaks',\n",
       "  'in',\n",
       "  '...',\n",
       "  'and',\n",
       "  'trite',\n",
       "  'as',\n",
       "  'it',\n",
       "  'sounds',\n",
       "  'breaks',\n",
       "  'him',\n",
       "  'out',\n",
       "  'canadian',\n",
       "  'cinema',\n",
       "  'suffers',\n",
       "  'from',\n",
       "  'several',\n",
       "  'problems',\n",
       "  'generally',\n",
       "  'a',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'money',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'an',\n",
       "  'insufferable',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'asking',\n",
       "  'for',\n",
       "  'help',\n",
       "  'as',\n",
       "  'if',\n",
       "  'somehow',\n",
       "  'the',\n",
       "  'feature',\n",
       "  'would',\n",
       "  'cease',\n",
       "  'to',\n",
       "  'be',\n",
       "  'canadian',\n",
       "  'leads',\n",
       "  'to',\n",
       "  'lower',\n",
       "  'production',\n",
       "  'values',\n",
       "  'than',\n",
       "  'american',\n",
       "  'or',\n",
       "  'british',\n",
       "  'films',\n",
       "  'and',\n",
       "  'most',\n",
       "  'people',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'like',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'anything',\n",
       "  'that',\n",
       "  'sounds',\n",
       "  'or',\n",
       "  'looks',\n",
       "  'like',\n",
       "  'well',\n",
       "  'not',\n",
       "  'like',\n",
       "  'an',\n",
       "  'american',\n",
       "  'film',\n",
       "  'next',\n",
       "  'canadian',\n",
       "  'screenwriters',\n",
       "  'often',\n",
       "  'seem',\n",
       "  'so',\n",
       "  'caught',\n",
       "  'up',\n",
       "  'in',\n",
       "  'being',\n",
       "  'weird',\n",
       "  'that',\n",
       "  'they',\n",
       "  'lose',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'how',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'a',\n",
       "  'good',\n",
       "  'story',\n",
       "  'and',\n",
       "  'tell',\n",
       "  'it',\n",
       "  'well',\n",
       "  'third',\n",
       "  'they',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'think',\n",
       "  'that',\n",
       "  'gratuitous',\n",
       "  'nudity',\n",
       "  'often',\n",
       "  'full',\n",
       "  'frontal',\n",
       "  'makes',\n",
       "  'something',\n",
       "  'artistic',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'sure',\n",
       "  'anyone',\n",
       "  'who',\n",
       "  'watches',\n",
       "  'enough',\n",
       "  'canadian',\n",
       "  'movies',\n",
       "  'especially',\n",
       "  'late',\n",
       "  'at',\n",
       "  'night',\n",
       "  'on',\n",
       "  'the',\n",
       "  'cbc',\n",
       "  'knows',\n",
       "  'exactly',\n",
       "  'what',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'talking',\n",
       "  'about',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'almost',\n",
       "  'like',\n",
       "  'a',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'do',\n",
       "  'this',\n",
       "  'handbook',\n",
       "  'exists',\n",
       "  'out',\n",
       "  'there',\n",
       "  'somewhere',\n",
       "  'and',\n",
       "  'canadian',\n",
       "  'film',\n",
       "  'makers',\n",
       "  'threw',\n",
       "  'it',\n",
       "  'out',\n",
       "  'a',\n",
       "  'long',\n",
       "  'time',\n",
       "  'ago',\n",
       "  'in',\n",
       "  'the',\n",
       "  '90s',\n",
       "  'and',\n",
       "  '00s',\n",
       "  'however',\n",
       "  'some',\n",
       "  'films',\n",
       "  'such',\n",
       "  'as',\n",
       "  'bruce',\n",
       "  'mcdonald',\n",
       "  \"'s\",\n",
       "  'work',\n",
       "  'and',\n",
       "  'the',\n",
       "  'brilliant',\n",
       "  'c.r.a.z.y.',\n",
       "  'have',\n",
       "  'broken',\n",
       "  'this',\n",
       "  'mold',\n",
       "  'and',\n",
       "  'managed',\n",
       "  'to',\n",
       "  'maintain',\n",
       "  'what',\n",
       "  'makes',\n",
       "  'them',\n",
       "  'canadian',\n",
       "  'while',\n",
       "  'holding',\n",
       "  'onto',\n",
       "  'watchable',\n",
       "  'production',\n",
       "  'values',\n",
       "  'and',\n",
       "  'great',\n",
       "  'stories',\n",
       "  'whale',\n",
       "  'music',\n",
       "  'is',\n",
       "  'such',\n",
       "  'a',\n",
       "  'film',\n",
       "  'on',\n",
       "  'the',\n",
       "  'surface',\n",
       "  'deeper',\n",
       "  'than',\n",
       "  'just',\n",
       "  'its',\n",
       "  'canadian',\n",
       "  'isms',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'a',\n",
       "  'deeply',\n",
       "  'moving',\n",
       "  'story',\n",
       "  'of',\n",
       "  'a',\n",
       "  'man',\n",
       "  'who',\n",
       "  \"'s\",\n",
       "  'lost',\n",
       "  'his',\n",
       "  'grip',\n",
       "  'through',\n",
       "  'grief',\n",
       "  'and',\n",
       "  'excess',\n",
       "  'who',\n",
       "  'is',\n",
       "  'redeemed',\n",
       "  'by',\n",
       "  'music',\n",
       "  'then',\n",
       "  'by',\n",
       "  'love',\n",
       "  'and',\n",
       "  'that',\n",
       "  'redeems',\n",
       "  'even',\n",
       "  'the',\n",
       "  'rheostatics',\n",
       "  ':)']]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maddy', 'debbie']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watch', 'it', 'again']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tokens[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['story',\n",
       "  'starts',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'funny',\n",
       "  'happens',\n",
       "  'for',\n",
       "  'a',\n",
       "  'while',\n",
       "  'all',\n",
       "  'the',\n",
       "  'action',\n",
       "  'is',\n",
       "  'in',\n",
       "  'the',\n",
       "  'end',\n",
       "  'but',\n",
       "  'you',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'have',\n",
       "  'to',\n",
       "  'laugh',\n",
       "  'because',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'funny',\n",
       "  'but',\n",
       "  'because',\n",
       "  'the',\n",
       "  'story',\n",
       "  'is',\n",
       "  'pathetic',\n",
       "  'the',\n",
       "  'funniest',\n",
       "  'part',\n",
       "  'is',\n",
       "  'when',\n",
       "  'harvey',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'not',\n",
       "  'paranoia',\n",
       "  'keitel',\n",
       "  'really',\n",
       "  'loses',\n",
       "  'it',\n",
       "  'and',\n",
       "  'the',\n",
       "  'judge',\n",
       "  'starts',\n",
       "  'a',\n",
       "  'massacre',\n",
       "  'oscars',\n",
       "  'for',\n",
       "  'this',\n",
       "  'guy'],\n",
       " ['one',\n",
       "  'night',\n",
       "  'i',\n",
       "  'was',\n",
       "  'waiting',\n",
       "  'for',\n",
       "  'my',\n",
       "  'friends',\n",
       "  'to',\n",
       "  'come',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'apt',\n",
       "  'and',\n",
       "  'gymkata',\n",
       "  'happened',\n",
       "  'to',\n",
       "  'be',\n",
       "  'on',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'way',\n",
       "  'too',\n",
       "  'much',\n",
       "  'of',\n",
       "  'it',\n",
       "  'it',\n",
       "  'is',\n",
       "  'indeed',\n",
       "  'hilarious',\n",
       "  'and',\n",
       "  'horrifying',\n",
       "  'really',\n",
       "  'think',\n",
       "  'about',\n",
       "  'it',\n",
       "  'this',\n",
       "  'way',\n",
       "  '--',\n",
       "  'if',\n",
       "  'in',\n",
       "  'your',\n",
       "  'job',\n",
       "  'you',\n",
       "  'had',\n",
       "  'an',\n",
       "  'idea',\n",
       "  'for',\n",
       "  'something',\n",
       "  'this',\n",
       "  'bad',\n",
       "  'and',\n",
       "  'went',\n",
       "  'on',\n",
       "  'to',\n",
       "  'execute',\n",
       "  'it',\n",
       "  'in',\n",
       "  'as',\n",
       "  'terrible',\n",
       "  'a',\n",
       "  'fashion',\n",
       "  'as',\n",
       "  'this',\n",
       "  'how',\n",
       "  'long',\n",
       "  'exactly',\n",
       "  'would',\n",
       "  'you',\n",
       "  'last',\n",
       "  'not',\n",
       "  'as',\n",
       "  'long',\n",
       "  'as',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'a',\n",
       "  'must',\n",
       "  'see',\n",
       "  'obviously']]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    count = 0\n",
    "    previous_ngram = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(str(count+1) + \" / \" + str(len(param_space)))\n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "        \n",
    "        NUM_EPOCHS = 5\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = param_comb[2]          # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        max_sentence_length = param_comb[4] # max sentence length of data loader\n",
    "        batch_size = param_comb[5]\n",
    "\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "\n",
    "        # Tokenization\n",
    "        if previous_ngram != grams:\n",
    "            print(\"Tokenizing...\")\n",
    "            # Tokenize Training\n",
    "            train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                                   n_gram=grams)\n",
    "            \n",
    "            # Tokenize Validation\n",
    "            val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                                  n_gram=grams)\n",
    "\n",
    "            # Building Vocab\n",
    "            print(\"Building Vocabulary...\")\n",
    "            token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "            train_data_indices = token2index_dataset(train_data_tokens)\n",
    "            val_data_indices = token2index_dataset(val_data_tokens)\n",
    "        \n",
    "        # saving n-grams to not do tokenization in every epoch\n",
    "        previous_ngram = grams\n",
    "        \n",
    "        \n",
    "        #as assign max sentence length and batch size from \n",
    "        ## parameter space\n",
    "        MAX_SENTENCE_LENGTH = max_sentence_length\n",
    "        BATCH_SIZE = batch_size\n",
    "\n",
    "        ## load train and val data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "\n",
    "        ## assign embedding dimension\n",
    "        ## from parameter space\n",
    "        embed_dim = embed_dimension    \n",
    "\n",
    "        ## model\n",
    "        model = BagOfNgrams(len(id2token), emb_dim)\n",
    "        \n",
    "        optimizers = [torch.optim.Adam(model.parameters(), lr=lr_rate),\n",
    "                      torch.optim.SGD(model.parameters(), lr=lr_rate)]\n",
    "        \n",
    "        for optimizer in optimizers:\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                    model.train()\n",
    "                    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(data_batch, length_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # validate every 100 iterations\n",
    "                    if i > 0 and i % 100 == 0:\n",
    "                        # validate\n",
    "                        train_acc = test_model(train_loader, model)\n",
    "                        val_acc = test_model(val_loader, model)\n",
    "                        val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                        print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {},Validation Acc: {}'.format( \n",
    "                                   epoch+1, num_epochs, \n",
    "                                    i+1, len(train_loader), \n",
    "                                    train_acc, val_acc))\n",
    "                        \n",
    "        count = count + 1\n",
    "                        \n",
    "    return param_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1344\n",
      "Tokenizing...\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-85ac02c444a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparam_val_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-196-6631ae7a61fe>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(hyperparameter_space)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Tokenize Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n\u001b[0;32m---> 37\u001b[0;31m                                                                    n_gram=grams)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Tokenize Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-d8da3129343d>\u001b[0m in \u001b[0;36mtokenize_dataset\u001b[0;34m(dataset, n_gram)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# n-gram version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtoken_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-5eec85ebea73>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sent, n_gram)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# unigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    149\u001b[0m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[1;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36m_add_padding\u001b[0;34m(self, Yf)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mYf_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mYf_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_val_losses = hyperparameter_search(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save et tokenlari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
