{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses bag-of-ngrams approach to sentiment classification using the IMDB review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"data/imdb_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "    file_list = os.listdir(folder_path)\n",
    "    # for debugging, printing out the folder path and some files in it\n",
    "    print(folder_path)\n",
    "    print(file_list[:10])\n",
    "    \n",
    "    all_reviews = []\n",
    "    for file_path in file_list:\n",
    "        f = open(folder_path + file_path,\"r\")\n",
    "        all_reviews.append(f.readline())\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_reviews/train/pos/\n",
      "['4715_9.txt', '12390_8.txt', '8329_7.txt', '9063_8.txt', '3092_10.txt', '9865_8.txt', '6639_10.txt', '10460_10.txt', '10331_10.txt', '11606_10.txt']\n",
      "12500\n",
      "data/imdb_reviews/train/neg/\n",
      "['1821_4.txt', '10402_1.txt', '1062_4.txt', '9056_1.txt', '5392_3.txt', '2682_3.txt', '3351_4.txt', '399_2.txt', '10447_1.txt', '10096_1.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/pos/\n",
      "['4715_9.txt', '1930_9.txt', '3205_9.txt', '10186_10.txt', '147_10.txt', '7511_7.txt', '616_10.txt', '10460_10.txt', '3240_9.txt', '1975_9.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/neg/\n",
      "['1821_4.txt', '9487_1.txt', '4604_4.txt', '2828_2.txt', '10890_1.txt', '3351_4.txt', '8070_2.txt', '1027_4.txt', '8248_3.txt', '4290_4.txt']\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "print(len(train_pos))\n",
    "train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "print(len(train_neg))\n",
    "test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "print(len(test_pos))\n",
    "test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)\n",
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive examples = 12500\n",
      "Train Negative examples = 12500\n",
      "Test Positive examples = 12500\n",
      "Test Negative examples = 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "train_pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the labels of the test set for Test Error Measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "print(len(test_data_labels))\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "test_neg_clean = [cleanhtml(x) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing dots & question marks & paranthesis with space\n",
    "\n",
    "It seems that punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"asdasdasds.asdasda\".replace(\".\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_dqmp(review):\n",
    "    \n",
    "#     review = review.replace(\".\",\" \")\n",
    "#     review = review.replace(\"?\",\" \")\n",
    "#     review = review.replace(\")\",\" \")\n",
    "#     review = review.replace(\"(\",\" \")\n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove_dqmp(train_pos_clean[random_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_clean = [remove_dqmp(x) for x in train_pos_clean]\n",
    "# train_neg_clean = [remove_dqmp(x) for x in train_neg_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# This is word tokenizer\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# Modified for n-grams\n",
    "def tokenize(sent, n_gram = 0):\n",
    "    \n",
    "    tokens = tokenizer(sent)\n",
    "    \n",
    "    # unigrams\n",
    "    #unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    unigrams = [token.lemma_.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    output = []\n",
    "    output.extend(unigrams)\n",
    "    \n",
    "    n = 2\n",
    "    while n <= n_gram:\n",
    "        ngram_tokens = [\" \".join(unigrams[x:x+n]) \\\n",
    "                            for x in range(len(unigrams)-n+1)]\n",
    "        output.extend(ngram_tokens)\n",
    "        n = n + 1\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While it certainly wasn't the best movie I've ever seen, it was certainly worth the $8 (which can't be said for many movies these days.)This was a pleasant account of a true story, although many of the details of the real story were twisted for the movie, (ie, Billy Sunday's character was three or four people in the real story combined together.) Robert DeNiro was of course good, and Cuba Gooding, Jr., was also impressive.\""
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', '-pron-', 'certainly', 'be', 'not', 'the', 'good', 'movie', '-pron-', 'have', 'ever', 'see', '-pron-', 'be', 'certainly', 'worth', 'the', '8', 'which', 'can', 'not', 'be', 'say', 'for', 'many', 'movie', 'these', 'days.)this', 'be', 'a', 'pleasant', 'account', 'of', 'a', 'true', 'story', 'although', 'many', 'of', 'the', 'detail', 'of', 'the', 'real', 'story', 'be', 'twist', 'for', 'the', 'movie', 'ie', 'billy', 'sunday', \"'s\", 'character', 'be', 'three', 'or', 'four', 'people', 'in', 'the', 'real', 'story', 'combine', 'together', 'robert', 'deniro', 'be', 'of', 'course', 'good', 'and', 'cuba', 'gooding', 'jr.', 'be', 'also', 'impressive', 'while -pron-', '-pron- certainly', 'certainly be', 'be not', 'not the', 'the good', 'good movie', 'movie -pron-', '-pron- have', 'have ever', 'ever see', 'see -pron-', '-pron- be', 'be certainly', 'certainly worth', 'worth the', 'the 8', '8 which', 'which can', 'can not', 'not be', 'be say', 'say for', 'for many', 'many movie', 'movie these', 'these days.)this', 'days.)this be', 'be a', 'a pleasant', 'pleasant account', 'account of', 'of a', 'a true', 'true story', 'story although', 'although many', 'many of', 'of the', 'the detail', 'detail of', 'of the', 'the real', 'real story', 'story be', 'be twist', 'twist for', 'for the', 'the movie', 'movie ie', 'ie billy', 'billy sunday', \"sunday 's\", \"'s character\", 'character be', 'be three', 'three or', 'or four', 'four people', 'people in', 'in the', 'the real', 'real story', 'story combine', 'combine together', 'together robert', 'robert deniro', 'deniro be', 'be of', 'of course', 'course good', 'good and', 'and cuba', 'cuba gooding', 'gooding jr.', 'jr. be', 'be also', 'also impressive', 'while -pron- certainly', '-pron- certainly be', 'certainly be not', 'be not the', 'not the good', 'the good movie', 'good movie -pron-', 'movie -pron- have', '-pron- have ever', 'have ever see', 'ever see -pron-', 'see -pron- be', '-pron- be certainly', 'be certainly worth', 'certainly worth the', 'worth the 8', 'the 8 which', '8 which can', 'which can not', 'can not be', 'not be say', 'be say for', 'say for many', 'for many movie', 'many movie these', 'movie these days.)this', 'these days.)this be', 'days.)this be a', 'be a pleasant', 'a pleasant account', 'pleasant account of', 'account of a', 'of a true', 'a true story', 'true story although', 'story although many', 'although many of', 'many of the', 'of the detail', 'the detail of', 'detail of the', 'of the real', 'the real story', 'real story be', 'story be twist', 'be twist for', 'twist for the', 'for the movie', 'the movie ie', 'movie ie billy', 'ie billy sunday', \"billy sunday 's\", \"sunday 's character\", \"'s character be\", 'character be three', 'be three or', 'three or four', 'or four people', 'four people in', 'people in the', 'in the real', 'the real story', 'real story combine', 'story combine together', 'combine together robert', 'together robert deniro', 'robert deniro be', 'deniro be of', 'be of course', 'of course good', 'course good and', 'good and cuba', 'and cuba gooding', 'cuba gooding jr.', 'gooding jr. be', 'jr. be also', 'be also impressive', 'while -pron- certainly be', '-pron- certainly be not', 'certainly be not the', 'be not the good', 'not the good movie', 'the good movie -pron-', 'good movie -pron- have', 'movie -pron- have ever', '-pron- have ever see', 'have ever see -pron-', 'ever see -pron- be', 'see -pron- be certainly', '-pron- be certainly worth', 'be certainly worth the', 'certainly worth the 8', 'worth the 8 which', 'the 8 which can', '8 which can not', 'which can not be', 'can not be say', 'not be say for', 'be say for many', 'say for many movie', 'for many movie these', 'many movie these days.)this', 'movie these days.)this be', 'these days.)this be a', 'days.)this be a pleasant', 'be a pleasant account', 'a pleasant account of', 'pleasant account of a', 'account of a true', 'of a true story', 'a true story although', 'true story although many', 'story although many of', 'although many of the', 'many of the detail', 'of the detail of', 'the detail of the', 'detail of the real', 'of the real story', 'the real story be', 'real story be twist', 'story be twist for', 'be twist for the', 'twist for the movie', 'for the movie ie', 'the movie ie billy', 'movie ie billy sunday', \"ie billy sunday 's\", \"billy sunday 's character\", \"sunday 's character be\", \"'s character be three\", 'character be three or', 'be three or four', 'three or four people', 'or four people in', 'four people in the', 'people in the real', 'in the real story', 'the real story combine', 'real story combine together', 'story combine together robert', 'combine together robert deniro', 'together robert deniro be', 'robert deniro be of', 'deniro be of course', 'be of course good', 'of course good and', 'course good and cuba', 'good and cuba gooding', 'and cuba gooding jr.', 'cuba gooding jr. be', 'gooding jr. be also', 'jr. be also impressive']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tokens = tokenize(train_pos_clean[random_text], n_gram = 4)\n",
    "#tokens = tokenize(train_pos_clean[random_text])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_clean = train_pos_clean + train_neg_clean\n",
    "len(train_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_clean = test_pos_clean + test_neg_clean\n",
    "len(test_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training -> Training + Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be smaller than 25000\n",
    "training_size = 20000\n",
    "\n",
    "assert training_size < 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[15821 15685  4147 ... 18888 20316 23805]\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15821, 15685,  4147, ..., 17207, 22378, 14852])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_index[:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, n_gram):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], \n",
    "#                                                batch_size=512, \n",
    "#                                                n_threads=4)):\n",
    "\n",
    "    itr = 0\n",
    "    for sample in dataset:\n",
    "        \n",
    "        if itr % 50 == 0:\n",
    "            print(str(itr) + \" / \" + str(len(dataset)))\n",
    "        # unigram version\n",
    "        #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        # n-gram version\n",
    "        tokens = tokenize(sample,n_gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "        itr = itr + 1\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n"
     ]
    }
   ],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram = 2)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                     n_gram = 2)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n",
      "0 / 25000\n",
      "50 / 25000\n",
      "100 / 25000\n",
      "150 / 25000\n",
      "200 / 25000\n",
      "250 / 25000\n",
      "300 / 25000\n",
      "350 / 25000\n",
      "400 / 25000\n",
      "450 / 25000\n",
      "500 / 25000\n",
      "550 / 25000\n",
      "600 / 25000\n",
      "650 / 25000\n",
      "700 / 25000\n",
      "750 / 25000\n",
      "800 / 25000\n",
      "850 / 25000\n",
      "900 / 25000\n",
      "950 / 25000\n",
      "1000 / 25000\n",
      "1050 / 25000\n",
      "1100 / 25000\n",
      "1150 / 25000\n",
      "1200 / 25000\n",
      "1250 / 25000\n",
      "1300 / 25000\n",
      "1350 / 25000\n",
      "1400 / 25000\n",
      "1450 / 25000\n",
      "1500 / 25000\n",
      "1550 / 25000\n",
      "1600 / 25000\n",
      "1650 / 25000\n",
      "1700 / 25000\n",
      "1750 / 25000\n",
      "1800 / 25000\n",
      "1850 / 25000\n",
      "1900 / 25000\n",
      "1950 / 25000\n",
      "2000 / 25000\n",
      "2050 / 25000\n",
      "2100 / 25000\n",
      "2150 / 25000\n",
      "2200 / 25000\n",
      "2250 / 25000\n",
      "2300 / 25000\n",
      "2350 / 25000\n",
      "2400 / 25000\n",
      "2450 / 25000\n",
      "2500 / 25000\n",
      "2550 / 25000\n",
      "2600 / 25000\n",
      "2650 / 25000\n",
      "2700 / 25000\n",
      "2750 / 25000\n",
      "2800 / 25000\n",
      "2850 / 25000\n",
      "2900 / 25000\n",
      "2950 / 25000\n",
      "3000 / 25000\n",
      "3050 / 25000\n",
      "3100 / 25000\n",
      "3150 / 25000\n",
      "3200 / 25000\n",
      "3250 / 25000\n",
      "3300 / 25000\n",
      "3350 / 25000\n",
      "3400 / 25000\n",
      "3450 / 25000\n",
      "3500 / 25000\n",
      "3550 / 25000\n",
      "3600 / 25000\n",
      "3650 / 25000\n",
      "3700 / 25000\n",
      "3750 / 25000\n",
      "3800 / 25000\n",
      "3850 / 25000\n",
      "3900 / 25000\n",
      "3950 / 25000\n",
      "4000 / 25000\n",
      "4050 / 25000\n",
      "4100 / 25000\n",
      "4150 / 25000\n",
      "4200 / 25000\n",
      "4250 / 25000\n",
      "4300 / 25000\n",
      "4350 / 25000\n",
      "4400 / 25000\n",
      "4450 / 25000\n",
      "4500 / 25000\n",
      "4550 / 25000\n",
      "4600 / 25000\n",
      "4650 / 25000\n",
      "4700 / 25000\n",
      "4750 / 25000\n",
      "4800 / 25000\n",
      "4850 / 25000\n",
      "4900 / 25000\n",
      "4950 / 25000\n",
      "5000 / 25000\n",
      "5050 / 25000\n",
      "5100 / 25000\n",
      "5150 / 25000\n",
      "5200 / 25000\n",
      "5250 / 25000\n",
      "5300 / 25000\n",
      "5350 / 25000\n",
      "5400 / 25000\n",
      "5450 / 25000\n",
      "5500 / 25000\n",
      "5550 / 25000\n",
      "5600 / 25000\n",
      "5650 / 25000\n",
      "5700 / 25000\n",
      "5750 / 25000\n",
      "5800 / 25000\n",
      "5850 / 25000\n",
      "5900 / 25000\n",
      "5950 / 25000\n",
      "6000 / 25000\n",
      "6050 / 25000\n",
      "6100 / 25000\n",
      "6150 / 25000\n",
      "6200 / 25000\n",
      "6250 / 25000\n",
      "6300 / 25000\n",
      "6350 / 25000\n",
      "6400 / 25000\n",
      "6450 / 25000\n",
      "6500 / 25000\n",
      "6550 / 25000\n",
      "6600 / 25000\n",
      "6650 / 25000\n",
      "6700 / 25000\n",
      "6750 / 25000\n",
      "6800 / 25000\n",
      "6850 / 25000\n",
      "6900 / 25000\n",
      "6950 / 25000\n",
      "7000 / 25000\n",
      "7050 / 25000\n",
      "7100 / 25000\n",
      "7150 / 25000\n",
      "7200 / 25000\n",
      "7250 / 25000\n",
      "7300 / 25000\n",
      "7350 / 25000\n",
      "7400 / 25000\n",
      "7450 / 25000\n",
      "7500 / 25000\n",
      "7550 / 25000\n",
      "7600 / 25000\n",
      "7650 / 25000\n",
      "7700 / 25000\n",
      "7750 / 25000\n",
      "7800 / 25000\n",
      "7850 / 25000\n",
      "7900 / 25000\n",
      "7950 / 25000\n",
      "8000 / 25000\n",
      "8050 / 25000\n",
      "8100 / 25000\n",
      "8150 / 25000\n",
      "8200 / 25000\n",
      "8250 / 25000\n",
      "8300 / 25000\n",
      "8350 / 25000\n",
      "8400 / 25000\n",
      "8450 / 25000\n",
      "8500 / 25000\n",
      "8550 / 25000\n",
      "8600 / 25000\n",
      "8650 / 25000\n",
      "8700 / 25000\n",
      "8750 / 25000\n",
      "8800 / 25000\n",
      "8850 / 25000\n",
      "8900 / 25000\n",
      "8950 / 25000\n",
      "9000 / 25000\n",
      "9050 / 25000\n",
      "9100 / 25000\n",
      "9150 / 25000\n",
      "9200 / 25000\n",
      "9250 / 25000\n",
      "9300 / 25000\n",
      "9350 / 25000\n",
      "9400 / 25000\n",
      "9450 / 25000\n",
      "9500 / 25000\n",
      "9550 / 25000\n",
      "9600 / 25000\n",
      "9650 / 25000\n",
      "9700 / 25000\n",
      "9750 / 25000\n",
      "9800 / 25000\n",
      "9850 / 25000\n",
      "9900 / 25000\n",
      "9950 / 25000\n",
      "10000 / 25000\n",
      "10050 / 25000\n",
      "10100 / 25000\n",
      "10150 / 25000\n",
      "10200 / 25000\n",
      "10250 / 25000\n",
      "10300 / 25000\n",
      "10350 / 25000\n",
      "10400 / 25000\n",
      "10450 / 25000\n",
      "10500 / 25000\n",
      "10550 / 25000\n",
      "10600 / 25000\n",
      "10650 / 25000\n",
      "10700 / 25000\n",
      "10750 / 25000\n",
      "10800 / 25000\n",
      "10850 / 25000\n",
      "10900 / 25000\n",
      "10950 / 25000\n",
      "11000 / 25000\n",
      "11050 / 25000\n",
      "11100 / 25000\n",
      "11150 / 25000\n",
      "11200 / 25000\n",
      "11250 / 25000\n",
      "11300 / 25000\n",
      "11350 / 25000\n",
      "11400 / 25000\n",
      "11450 / 25000\n",
      "11500 / 25000\n",
      "11550 / 25000\n",
      "11600 / 25000\n",
      "11650 / 25000\n",
      "11700 / 25000\n",
      "11750 / 25000\n",
      "11800 / 25000\n",
      "11850 / 25000\n",
      "11900 / 25000\n",
      "11950 / 25000\n",
      "12000 / 25000\n",
      "12050 / 25000\n",
      "12100 / 25000\n",
      "12150 / 25000\n",
      "12200 / 25000\n",
      "12250 / 25000\n",
      "12300 / 25000\n",
      "12350 / 25000\n",
      "12400 / 25000\n",
      "12450 / 25000\n",
      "12500 / 25000\n",
      "12550 / 25000\n",
      "12600 / 25000\n",
      "12650 / 25000\n",
      "12700 / 25000\n",
      "12750 / 25000\n",
      "12800 / 25000\n",
      "12850 / 25000\n",
      "12900 / 25000\n",
      "12950 / 25000\n",
      "13000 / 25000\n",
      "13050 / 25000\n",
      "13100 / 25000\n",
      "13150 / 25000\n",
      "13200 / 25000\n",
      "13250 / 25000\n",
      "13300 / 25000\n",
      "13350 / 25000\n",
      "13400 / 25000\n",
      "13450 / 25000\n",
      "13500 / 25000\n",
      "13550 / 25000\n",
      "13600 / 25000\n",
      "13650 / 25000\n",
      "13700 / 25000\n",
      "13750 / 25000\n",
      "13800 / 25000\n",
      "13850 / 25000\n",
      "13900 / 25000\n",
      "13950 / 25000\n",
      "14000 / 25000\n",
      "14050 / 25000\n",
      "14100 / 25000\n",
      "14150 / 25000\n",
      "14200 / 25000\n",
      "14250 / 25000\n",
      "14300 / 25000\n",
      "14350 / 25000\n",
      "14400 / 25000\n",
      "14450 / 25000\n",
      "14500 / 25000\n",
      "14550 / 25000\n",
      "14600 / 25000\n",
      "14650 / 25000\n",
      "14700 / 25000\n",
      "14750 / 25000\n",
      "14800 / 25000\n",
      "14850 / 25000\n",
      "14900 / 25000\n",
      "14950 / 25000\n",
      "15000 / 25000\n",
      "15050 / 25000\n",
      "15100 / 25000\n",
      "15150 / 25000\n",
      "15200 / 25000\n",
      "15250 / 25000\n",
      "15300 / 25000\n",
      "15350 / 25000\n",
      "15400 / 25000\n",
      "15450 / 25000\n",
      "15500 / 25000\n",
      "15550 / 25000\n",
      "15600 / 25000\n",
      "15650 / 25000\n",
      "15700 / 25000\n",
      "15750 / 25000\n",
      "15800 / 25000\n",
      "15850 / 25000\n",
      "15900 / 25000\n",
      "15950 / 25000\n",
      "16000 / 25000\n",
      "16050 / 25000\n",
      "16100 / 25000\n",
      "16150 / 25000\n",
      "16200 / 25000\n",
      "16250 / 25000\n",
      "16300 / 25000\n",
      "16350 / 25000\n",
      "16400 / 25000\n",
      "16450 / 25000\n",
      "16500 / 25000\n",
      "16550 / 25000\n",
      "16600 / 25000\n",
      "16650 / 25000\n",
      "16700 / 25000\n",
      "16750 / 25000\n",
      "16800 / 25000\n",
      "16850 / 25000\n",
      "16900 / 25000\n",
      "16950 / 25000\n",
      "17000 / 25000\n",
      "17050 / 25000\n",
      "17100 / 25000\n",
      "17150 / 25000\n",
      "17200 / 25000\n",
      "17250 / 25000\n",
      "17300 / 25000\n",
      "17350 / 25000\n",
      "17400 / 25000\n",
      "17450 / 25000\n",
      "17500 / 25000\n",
      "17550 / 25000\n",
      "17600 / 25000\n",
      "17650 / 25000\n",
      "17700 / 25000\n",
      "17750 / 25000\n",
      "17800 / 25000\n",
      "17850 / 25000\n",
      "17900 / 25000\n",
      "17950 / 25000\n",
      "18000 / 25000\n",
      "18050 / 25000\n",
      "18100 / 25000\n",
      "18150 / 25000\n",
      "18200 / 25000\n",
      "18250 / 25000\n",
      "18300 / 25000\n",
      "18350 / 25000\n",
      "18400 / 25000\n",
      "18450 / 25000\n",
      "18500 / 25000\n",
      "18550 / 25000\n",
      "18600 / 25000\n",
      "18650 / 25000\n",
      "18700 / 25000\n",
      "18750 / 25000\n",
      "18800 / 25000\n",
      "18850 / 25000\n",
      "18900 / 25000\n",
      "18950 / 25000\n",
      "19000 / 25000\n",
      "19050 / 25000\n",
      "19100 / 25000\n",
      "19150 / 25000\n",
      "19200 / 25000\n",
      "19250 / 25000\n",
      "19300 / 25000\n",
      "19350 / 25000\n",
      "19400 / 25000\n",
      "19450 / 25000\n",
      "19500 / 25000\n",
      "19550 / 25000\n",
      "19600 / 25000\n",
      "19650 / 25000\n",
      "19700 / 25000\n",
      "19750 / 25000\n",
      "19800 / 25000\n",
      "19850 / 25000\n",
      "19900 / 25000\n",
      "19950 / 25000\n",
      "20000 / 25000\n",
      "20050 / 25000\n",
      "20100 / 25000\n",
      "20150 / 25000\n",
      "20200 / 25000\n",
      "20250 / 25000\n",
      "20300 / 25000\n",
      "20350 / 25000\n",
      "20400 / 25000\n",
      "20450 / 25000\n",
      "20500 / 25000\n",
      "20550 / 25000\n",
      "20600 / 25000\n",
      "20650 / 25000\n",
      "20700 / 25000\n",
      "20750 / 25000\n",
      "20800 / 25000\n",
      "20850 / 25000\n",
      "20900 / 25000\n",
      "20950 / 25000\n",
      "21000 / 25000\n",
      "21050 / 25000\n",
      "21100 / 25000\n",
      "21150 / 25000\n",
      "21200 / 25000\n",
      "21250 / 25000\n",
      "21300 / 25000\n",
      "21350 / 25000\n",
      "21400 / 25000\n",
      "21450 / 25000\n",
      "21500 / 25000\n",
      "21550 / 25000\n",
      "21600 / 25000\n",
      "21650 / 25000\n",
      "21700 / 25000\n",
      "21750 / 25000\n",
      "21800 / 25000\n",
      "21850 / 25000\n",
      "21900 / 25000\n",
      "21950 / 25000\n",
      "22000 / 25000\n",
      "22050 / 25000\n",
      "22100 / 25000\n",
      "22150 / 25000\n",
      "22200 / 25000\n",
      "22250 / 25000\n",
      "22300 / 25000\n",
      "22350 / 25000\n",
      "22400 / 25000\n",
      "22450 / 25000\n",
      "22500 / 25000\n",
      "22550 / 25000\n",
      "22600 / 25000\n",
      "22650 / 25000\n",
      "22700 / 25000\n",
      "22750 / 25000\n",
      "22800 / 25000\n",
      "22850 / 25000\n",
      "22900 / 25000\n",
      "22950 / 25000\n",
      "23000 / 25000\n",
      "23050 / 25000\n",
      "23100 / 25000\n",
      "23150 / 25000\n",
      "23200 / 25000\n",
      "23250 / 25000\n",
      "23300 / 25000\n",
      "23350 / 25000\n",
      "23400 / 25000\n",
      "23450 / 25000\n",
      "23500 / 25000\n",
      "23550 / 25000\n",
      "23600 / 25000\n",
      "23650 / 25000\n",
      "23700 / 25000\n",
      "23750 / 25000\n",
      "23800 / 25000\n",
      "23850 / 25000\n",
      "23900 / 25000\n",
      "23950 / 25000\n",
      "24000 / 25000\n",
      "24050 / 25000\n",
      "24100 / 25000\n",
      "24150 / 25000\n",
      "24200 / 25000\n",
      "24250 / 25000\n",
      "24300 / 25000\n",
      "24350 / 25000\n",
      "24400 / 25000\n",
      "24450 / 25000\n",
      "24500 / 25000\n",
      "24550 / 25000\n",
      "24600 / 25000\n",
      "24650 / 25000\n",
      "24700 / 25000\n",
      "24750 / 25000\n",
      "24800 / 25000\n",
      "24850 / 25000\n",
      "24900 / 25000\n",
      "24950 / 25000\n"
     ]
    }
   ],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_all_clean,\n",
    "                                      n_gram = 2)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'surely', 'one', 'of', 'the', 'worst', 'films', 'ever', 'made', 'and', 'released', 'by', 'a', 'major', 'hollywood', 'studio', 'the', 'plot', 'is', 'simply', 'stupid', 'the', 'dialog', 'is', 'written', 'in', 'clichs', 'you', 'can', 'complete', 'a', 'great', 'many', 'sentences', 'in', 'the', 'script', 'because', 'of', 'this', 'the', 'acting', 'is', 'ridiculously', 'bad', 'especially', 'that', 'of', 'rod', 'cameron', 'the', 'choreography', 'is', 'silly', 'and', 'wholly', 'unerotic', 'one', 'can', 'only', 'pity', 'the', 'reviewer', 'who', 'saw', '23-year', 'old', 'yvonne', \"'s\", 'dance', 'as', 'sexual', 'it', \"'s\", 'merely', 'very', 'bad', 'choreography', 'the', 'ballet', 'scene', 'in', 'the', 'film', \"'s\", 'beginning', 'is', 'especially', 'ludicrous', 'if', 'you', 'are', 'into', 'bad', 'movies', 'and', 'enjoy', 'laughing', 'at', 'some', 'of', 'hollywood', \"'s\", 'turkeys', 'this', 'is', 'for', 'you', 'i', 'bought', 'the', 'colorized', 'version', 'on', 'vhs', 'making', 'the', 'movie', 'even', 'worse', 'yvonne', \"'s\", 'heavy', 'makeup', 'when', 'colored', 'has', 'her', 'looking', 'like', 'a', 'clown', 'all', 'the', 'time', 'and', 'she', \"'s\", 'the', 'best', 'part', 'of', 'this', 'film', 'what', 'a', 'way', 'to', 'launch', 'a', 'career', 'this is', 'is surely', 'surely one', 'one of', 'of the', 'the worst', 'worst films', 'films ever', 'ever made', 'made and', 'and released', 'released by', 'by a', 'a major', 'major hollywood', 'hollywood studio', 'studio the', 'the plot', 'plot is', 'is simply', 'simply stupid', 'stupid the', 'the dialog', 'dialog is', 'is written', 'written in', 'in clichs', 'clichs you', 'you can', 'can complete', 'complete a', 'a great', 'great many', 'many sentences', 'sentences in', 'in the', 'the script', 'script because', 'because of', 'of this', 'this the', 'the acting', 'acting is', 'is ridiculously', 'ridiculously bad', 'bad especially', 'especially that', 'that of', 'of rod', 'rod cameron', 'cameron the', 'the choreography', 'choreography is', 'is silly', 'silly and', 'and wholly', 'wholly unerotic', 'unerotic one', 'one can', 'can only', 'only pity', 'pity the', 'the reviewer', 'reviewer who', 'who saw', 'saw 23-year', '23-year old', 'old yvonne', \"yvonne 's\", \"'s dance\", 'dance as', 'as sexual', 'sexual it', \"it 's\", \"'s merely\", 'merely very', 'very bad', 'bad choreography', 'choreography the', 'the ballet', 'ballet scene', 'scene in', 'in the', 'the film', \"film 's\", \"'s beginning\", 'beginning is', 'is especially', 'especially ludicrous', 'ludicrous if', 'if you', 'you are', 'are into', 'into bad', 'bad movies', 'movies and', 'and enjoy', 'enjoy laughing', 'laughing at', 'at some', 'some of', 'of hollywood', \"hollywood 's\", \"'s turkeys\", 'turkeys this', 'this is', 'is for', 'for you', 'you i', 'i bought', 'bought the', 'the colorized', 'colorized version', 'version on', 'on vhs', 'vhs making', 'making the', 'the movie', 'movie even', 'even worse', 'worse yvonne', \"yvonne 's\", \"'s heavy\", 'heavy makeup', 'makeup when', 'when colored', 'colored has', 'has her', 'her looking', 'looking like', 'like a', 'a clown', 'clown all', 'all the', 'the time', 'time and', 'and she', \"she 's\", \"'s the\", 'the best', 'best part', 'part of', 'of this', 'this film', 'film what', 'what a', 'a way', 'way to', 'to launch', 'launch a', 'a career'], ['mabel', 'at', 'the', 'wheel', 'is', 'one', 'of', 'those', 'movies', 'with', 'a', 'behind', 'the', 'scenes', 'story', 'that', \"'s\", 'more', 'interesting', 'than', 'the', 'movie', 'itself', 'this', 'was', 'chaplin', \"'s\", 'tenth', 'comedy', 'for', 'keystone', 'during', 'his', 'year', 'of', 'apprenticeship', 'and', 'his', 'first', 'two', 'reeler', 'here', 'he', 'played', 'one', 'of', 'his', 'last', 'out', 'and', 'out', 'villain', 'roles', 'although', 'the', 'feature', 'length', 'tillie', \"'s\", 'punctured', 'romance', 'was', 'yet', 'to', 'come', 'and', 'it', 'also', 'marked', 'one', 'of', 'the', 'last', 'times', 'he', 'would', 'work', 'for', 'a', 'director', 'other', 'than', 'himself', 'in', 'fact', 'chaplin', \"'s\", 'conflicts', 'with', 'director', 'and', 'co', 'star', 'mabel', 'normand', 'almost', 'got', 'him', 'fired', 'from', 'the', 'studio', 'chaplin', 'had', \"n't\", 'gotten', 'along', 'with', 'his', 'earlier', 'directors', 'henry', 'lehrman', 'and', 'george', 'nichols', 'but', 'according', 'to', 'his', 'autobiography', 'having', 'to', 'take', 'direction', 'from', 'a', 'mere', 'girl', 'was', 'the', 'last', 'straw', 'charlie', 'and', 'mabel', 'argued', 'bitterly', 'during', 'the', 'making', 'of', 'this', 'film', 'chaplin', 'was', 'still', 'a', 'newcomer', 'at', 'keystone', 'and', 'his', 'colleagues', 'did', \"n't\", 'know', 'what', 'to', 'make', 'of', 'him', 'but', 'everyone', 'loved', 'mabel', 'producer', 'mack', 'sennett', 'was', 'on', 'the', 'verge', 'of', 'firing', 'chaplin', 'when', 'he', 'learned', 'that', 'the', 'newcomer', \"'s\", 'films', 'were', 'catching', 'on', 'and', 'exhibitors', 'wanted', 'more', 'of', 'them', 'a.s.a.p.', 'so', 'chaplin', 'was', 'promised', 'the', 'chance', 'to', 'direct', 'himself', 'in', 'return', 'for', 'finishing', 'this', 'movie', 'the', 'way', 'mabel', 'wanted', 'it', 'unfortunately', 'none', 'of', 'that', 'drama', 'is', 'visible', 'on', 'screen', 'in', 'mabel', 'at', 'the', 'wheel', 'which', 'looks', 'like', 'typical', 'keystone', 'chaos', 'the', 'story', 'concerns', 'an', 'auto', 'race', 'in', 'which', 'mabel', \"'s\", 'beau', 'harry', 'mccoy', 'is', 'scheduled', 'to', 'compete', 'but', 'wicked', 'charlie', 'and', 'his', 'henchmen', 'abduct', 'the', 'lad', 'and', 'mabel', 'must', 'take', 'the', 'wheel', 'in', 'his', 'place', 'for', 'all', 'the', 'racing', 'around', 'brick', 'hurling', 'and', 'finger', 'biting', 'the', 'film', 'is', 'frankly', 'short', 'on', 'laughs', 'but', 'there', 'are', 'a', 'few', 'points', 'of', 'interest', 'there', \"'s\", 'some', 'good', 'cinematography', 'and', 'editing', 'in', 'the', 'race', 'sequence', 'though', 'there', 'are', \"n't\", 'really', 'any', 'gags', 'just', 'lots', 'of', 'frantic', 'activity', 'chaplin', 'himself', 'looks', 'odd', 'sporting', 'a', 'goat', 'like', 'beard', 'on', 'his', 'chin', 'and', 'wearing', 'the', 'top', 'hat', 'and', 'frock', 'coat', 'he', 'wore', 'in', 'his', 'very', 'first', 'film', 'appearance', 'making', 'a', 'living', 'but', 'the', 'outfit', 'suits', 'the', 'old', 'fashioned', 'villainy', 'he', 'displays', 'throughout', 'at', 'least', 'it', \"'s\", 'novel', 'to', 'watch', 'him', 'play', 'such', 'an', 'uncharacteristic', 'role', 'visible', 'in', 'the', 'stands', 'at', 'the', 'race', 'track', 'are', 'such', 'keystone', 'stalwarts', 'as', 'chester', 'conklin', 'edgar', 'kennedy', 'in', 'a', 'strangely', 'dandified', 'get', 'up', 'and', 'a', 'more', 'characteristic', 'mack', 'sennett', 'spitting', 'tobacco', 'and', 'doing', 'his', 'usual', 'mindless', 'rube', 'routine', 'as', 'a', 'performer', 'sennett', 'was', 'about', 'as', 'subtle', 'as', 'the', 'movies', 'he', 'produced', 'but', 'you', 'have', 'to', 'give', 'the', 'guy', 'credit', 'he', 'knew', 'what', 'people', 'liked', 'these', 'films', 'were', 'hugely', 'popular', 'in', 'their', 'day', 'mack', \"'s\", 'performance', 'does', \"n't\", 'add', 'much', 'to', 'mabel', 'at', 'the', 'wheel', 'but', 'he', 'probably', 'had', 'to', 'be', 'on', 'hand', 'for', 'the', 'filming', 'of', 'this', 'one', 'to', 'make', 'sure', 'his', 'stars', 'did', \"n't\", 'kill', 'each', 'other', 'mabel at', 'at the', 'the wheel', 'wheel is', 'is one', 'one of', 'of those', 'those movies', 'movies with', 'with a', 'a behind', 'behind the', 'the scenes', 'scenes story', 'story that', \"that 's\", \"'s more\", 'more interesting', 'interesting than', 'than the', 'the movie', 'movie itself', 'itself this', 'this was', 'was chaplin', \"chaplin 's\", \"'s tenth\", 'tenth comedy', 'comedy for', 'for keystone', 'keystone during', 'during his', 'his year', 'year of', 'of apprenticeship', 'apprenticeship and', 'and his', 'his first', 'first two', 'two reeler', 'reeler here', 'here he', 'he played', 'played one', 'one of', 'of his', 'his last', 'last out', 'out and', 'and out', 'out villain', 'villain roles', 'roles although', 'although the', 'the feature', 'feature length', 'length tillie', \"tillie 's\", \"'s punctured\", 'punctured romance', 'romance was', 'was yet', 'yet to', 'to come', 'come and', 'and it', 'it also', 'also marked', 'marked one', 'one of', 'of the', 'the last', 'last times', 'times he', 'he would', 'would work', 'work for', 'for a', 'a director', 'director other', 'other than', 'than himself', 'himself in', 'in fact', 'fact chaplin', \"chaplin 's\", \"'s conflicts\", 'conflicts with', 'with director', 'director and', 'and co', 'co star', 'star mabel', 'mabel normand', 'normand almost', 'almost got', 'got him', 'him fired', 'fired from', 'from the', 'the studio', 'studio chaplin', 'chaplin had', \"had n't\", \"n't gotten\", 'gotten along', 'along with', 'with his', 'his earlier', 'earlier directors', 'directors henry', 'henry lehrman', 'lehrman and', 'and george', 'george nichols', 'nichols but', 'but according', 'according to', 'to his', 'his autobiography', 'autobiography having', 'having to', 'to take', 'take direction', 'direction from', 'from a', 'a mere', 'mere girl', 'girl was', 'was the', 'the last', 'last straw', 'straw charlie', 'charlie and', 'and mabel', 'mabel argued', 'argued bitterly', 'bitterly during', 'during the', 'the making', 'making of', 'of this', 'this film', 'film chaplin', 'chaplin was', 'was still', 'still a', 'a newcomer', 'newcomer at', 'at keystone', 'keystone and', 'and his', 'his colleagues', 'colleagues did', \"did n't\", \"n't know\", 'know what', 'what to', 'to make', 'make of', 'of him', 'him but', 'but everyone', 'everyone loved', 'loved mabel', 'mabel producer', 'producer mack', 'mack sennett', 'sennett was', 'was on', 'on the', 'the verge', 'verge of', 'of firing', 'firing chaplin', 'chaplin when', 'when he', 'he learned', 'learned that', 'that the', 'the newcomer', \"newcomer 's\", \"'s films\", 'films were', 'were catching', 'catching on', 'on and', 'and exhibitors', 'exhibitors wanted', 'wanted more', 'more of', 'of them', 'them a.s.a.p.', 'a.s.a.p. so', 'so chaplin', 'chaplin was', 'was promised', 'promised the', 'the chance', 'chance to', 'to direct', 'direct himself', 'himself in', 'in return', 'return for', 'for finishing', 'finishing this', 'this movie', 'movie the', 'the way', 'way mabel', 'mabel wanted', 'wanted it', 'it unfortunately', 'unfortunately none', 'none of', 'of that', 'that drama', 'drama is', 'is visible', 'visible on', 'on screen', 'screen in', 'in mabel', 'mabel at', 'at the', 'the wheel', 'wheel which', 'which looks', 'looks like', 'like typical', 'typical keystone', 'keystone chaos', 'chaos the', 'the story', 'story concerns', 'concerns an', 'an auto', 'auto race', 'race in', 'in which', 'which mabel', \"mabel 's\", \"'s beau\", 'beau harry', 'harry mccoy', 'mccoy is', 'is scheduled', 'scheduled to', 'to compete', 'compete but', 'but wicked', 'wicked charlie', 'charlie and', 'and his', 'his henchmen', 'henchmen abduct', 'abduct the', 'the lad', 'lad and', 'and mabel', 'mabel must', 'must take', 'take the', 'the wheel', 'wheel in', 'in his', 'his place', 'place for', 'for all', 'all the', 'the racing', 'racing around', 'around brick', 'brick hurling', 'hurling and', 'and finger', 'finger biting', 'biting the', 'the film', 'film is', 'is frankly', 'frankly short', 'short on', 'on laughs', 'laughs but', 'but there', 'there are', 'are a', 'a few', 'few points', 'points of', 'of interest', 'interest there', \"there 's\", \"'s some\", 'some good', 'good cinematography', 'cinematography and', 'and editing', 'editing in', 'in the', 'the race', 'race sequence', 'sequence though', 'though there', 'there are', \"are n't\", \"n't really\", 'really any', 'any gags', 'gags just', 'just lots', 'lots of', 'of frantic', 'frantic activity', 'activity chaplin', 'chaplin himself', 'himself looks', 'looks odd', 'odd sporting', 'sporting a', 'a goat', 'goat like', 'like beard', 'beard on', 'on his', 'his chin', 'chin and', 'and wearing', 'wearing the', 'the top', 'top hat', 'hat and', 'and frock', 'frock coat', 'coat he', 'he wore', 'wore in', 'in his', 'his very', 'very first', 'first film', 'film appearance', 'appearance making', 'making a', 'a living', 'living but', 'but the', 'the outfit', 'outfit suits', 'suits the', 'the old', 'old fashioned', 'fashioned villainy', 'villainy he', 'he displays', 'displays throughout', 'throughout at', 'at least', 'least it', \"it 's\", \"'s novel\", 'novel to', 'to watch', 'watch him', 'him play', 'play such', 'such an', 'an uncharacteristic', 'uncharacteristic role', 'role visible', 'visible in', 'in the', 'the stands', 'stands at', 'at the', 'the race', 'race track', 'track are', 'are such', 'such keystone', 'keystone stalwarts', 'stalwarts as', 'as chester', 'chester conklin', 'conklin edgar', 'edgar kennedy', 'kennedy in', 'in a', 'a strangely', 'strangely dandified', 'dandified get', 'get up', 'up and', 'and a', 'a more', 'more characteristic', 'characteristic mack', 'mack sennett', 'sennett spitting', 'spitting tobacco', 'tobacco and', 'and doing', 'doing his', 'his usual', 'usual mindless', 'mindless rube', 'rube routine', 'routine as', 'as a', 'a performer', 'performer sennett', 'sennett was', 'was about', 'about as', 'as subtle', 'subtle as', 'as the', 'the movies', 'movies he', 'he produced', 'produced but', 'but you', 'you have', 'have to', 'to give', 'give the', 'the guy', 'guy credit', 'credit he', 'he knew', 'knew what', 'what people', 'people liked', 'liked these', 'these films', 'films were', 'were hugely', 'hugely popular', 'popular in', 'in their', 'their day', 'day mack', \"mack 's\", \"'s performance\", 'performance does', \"does n't\", \"n't add\", 'add much', 'much to', 'to mabel', 'mabel at', 'at the', 'the wheel', 'wheel but', 'but he', 'he probably', 'probably had', 'had to', 'to be', 'be on', 'on hand', 'hand for', 'for the', 'the filming', 'filming of', 'of this', 'this one', 'one to', 'to make', 'make sure', 'sure his', 'his stars', 'stars did', \"did n't\", \"n't kill\", 'kill each', 'each other']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'surely', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(all_train_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove blank space tokens\n",
    "\n",
    "In the above tokenization, some blankspace strings were observed, thus this section adresses that by deleting them from the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blankspaces = [\" \",\"  \",\"   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_blankspaces(review):\n",
    "    \n",
    "#     review = [x for x in review if x not in blankspaces] \n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(remove_blankspaces(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data_tokens_clean = [remove_blankspaces(token) for token in train_data_tokens]\n",
    "# len(train_data_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens_clean = remove_blankspaces(all_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9538806"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289344"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,vocab_size=max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8255 ; token my money\n",
      "Token my money; token id 8255\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDBDataset(train_data_indices, training_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, validation_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [51/625], Training Acc: 51.965,Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [101/625], Training Acc: 69.47,Validation Acc: 67.8\n",
      "Epoch: [1/10], Step: [151/625], Training Acc: 81.755,Validation Acc: 80.14\n",
      "Epoch: [1/10], Step: [201/625], Training Acc: 84.655,Validation Acc: 82.76\n",
      "Epoch: [1/10], Step: [251/625], Training Acc: 86.205,Validation Acc: 84.12\n",
      "Epoch: [1/10], Step: [301/625], Training Acc: 87.52,Validation Acc: 84.44\n",
      "Epoch: [1/10], Step: [351/625], Training Acc: 88.47,Validation Acc: 85.52\n",
      "Epoch: [1/10], Step: [401/625], Training Acc: 88.695,Validation Acc: 85.42\n",
      "Epoch: [1/10], Step: [451/625], Training Acc: 88.65,Validation Acc: 85.02\n",
      "Epoch: [1/10], Step: [501/625], Training Acc: 90.575,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [551/625], Training Acc: 90.955,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [601/625], Training Acc: 91.545,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [51/625], Training Acc: 91.66,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [101/625], Training Acc: 91.36,Validation Acc: 86.16\n",
      "Epoch: [2/10], Step: [151/625], Training Acc: 91.815,Validation Acc: 86.7\n",
      "Epoch: [2/10], Step: [201/625], Training Acc: 92.165,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [251/625], Training Acc: 92.52,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [301/625], Training Acc: 91.02,Validation Acc: 84.94\n",
      "Epoch: [2/10], Step: [351/625], Training Acc: 90.405,Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [401/625], Training Acc: 92.965,Validation Acc: 86.8\n",
      "Epoch: [2/10], Step: [451/625], Training Acc: 93.18,Validation Acc: 86.64\n",
      "Epoch: [2/10], Step: [501/625], Training Acc: 92.6,Validation Acc: 85.84\n",
      "Epoch: [2/10], Step: [551/625], Training Acc: 93.67,Validation Acc: 86.9\n",
      "Epoch: [2/10], Step: [601/625], Training Acc: 94.16,Validation Acc: 86.9\n",
      "Epoch: [3/10], Step: [51/625], Training Acc: 94.08,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [101/625], Training Acc: 94.445,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [151/625], Training Acc: 94.335,Validation Acc: 86.64\n",
      "Epoch: [3/10], Step: [201/625], Training Acc: 93.075,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [251/625], Training Acc: 94.62,Validation Acc: 86.42\n",
      "Epoch: [3/10], Step: [301/625], Training Acc: 94.36,Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [351/625], Training Acc: 94.85,Validation Acc: 86.4\n",
      "Epoch: [3/10], Step: [401/625], Training Acc: 94.705,Validation Acc: 85.98\n",
      "Epoch: [3/10], Step: [451/625], Training Acc: 95.19,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [501/625], Training Acc: 95.215,Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [551/625], Training Acc: 94.475,Validation Acc: 85.5\n",
      "Epoch: [3/10], Step: [601/625], Training Acc: 95.13,Validation Acc: 85.92\n",
      "Epoch: [4/10], Step: [51/625], Training Acc: 94.39,Validation Acc: 85.26\n",
      "Epoch: [4/10], Step: [101/625], Training Acc: 95.16,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [151/625], Training Acc: 95.31,Validation Acc: 85.86\n",
      "Epoch: [4/10], Step: [201/625], Training Acc: 94.805,Validation Acc: 85.22\n",
      "Epoch: [4/10], Step: [251/625], Training Acc: 95.23,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [301/625], Training Acc: 95.89,Validation Acc: 85.3\n",
      "Epoch: [4/10], Step: [351/625], Training Acc: 95.895,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [401/625], Training Acc: 95.84,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [451/625], Training Acc: 96.21,Validation Acc: 85.18\n",
      "Epoch: [4/10], Step: [501/625], Training Acc: 96.0,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [551/625], Training Acc: 95.8,Validation Acc: 85.42\n",
      "Epoch: [4/10], Step: [601/625], Training Acc: 96.345,Validation Acc: 85.62\n",
      "Epoch: [5/10], Step: [51/625], Training Acc: 96.41,Validation Acc: 85.5\n",
      "Epoch: [5/10], Step: [101/625], Training Acc: 96.415,Validation Acc: 85.32\n",
      "Epoch: [5/10], Step: [151/625], Training Acc: 96.14,Validation Acc: 84.98\n",
      "Epoch: [5/10], Step: [201/625], Training Acc: 95.265,Validation Acc: 84.26\n",
      "Epoch: [5/10], Step: [251/625], Training Acc: 95.785,Validation Acc: 84.56\n",
      "Epoch: [5/10], Step: [301/625], Training Acc: 96.415,Validation Acc: 85.36\n",
      "Epoch: [5/10], Step: [351/625], Training Acc: 96.61,Validation Acc: 85.48\n",
      "Epoch: [5/10], Step: [401/625], Training Acc: 95.835,Validation Acc: 84.32\n",
      "Epoch: [5/10], Step: [451/625], Training Acc: 96.675,Validation Acc: 85.12\n",
      "Epoch: [5/10], Step: [501/625], Training Acc: 96.98,Validation Acc: 85.3\n",
      "Epoch: [5/10], Step: [551/625], Training Acc: 96.965,Validation Acc: 85.08\n",
      "Epoch: [5/10], Step: [601/625], Training Acc: 96.755,Validation Acc: 84.64\n",
      "Epoch: [6/10], Step: [51/625], Training Acc: 97.115,Validation Acc: 85.04\n",
      "Epoch: [6/10], Step: [101/625], Training Acc: 97.185,Validation Acc: 84.78\n",
      "Epoch: [6/10], Step: [151/625], Training Acc: 96.855,Validation Acc: 84.54\n",
      "Epoch: [6/10], Step: [201/625], Training Acc: 96.615,Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [251/625], Training Acc: 95.92,Validation Acc: 83.54\n",
      "Epoch: [6/10], Step: [301/625], Training Acc: 96.895,Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [351/625], Training Acc: 96.995,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [401/625], Training Acc: 97.005,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [451/625], Training Acc: 96.785,Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [501/625], Training Acc: 97.03,Validation Acc: 84.74\n",
      "Epoch: [6/10], Step: [551/625], Training Acc: 95.895,Validation Acc: 83.78\n",
      "Epoch: [6/10], Step: [601/625], Training Acc: 96.885,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [51/625], Training Acc: 96.99,Validation Acc: 84.42\n",
      "Epoch: [7/10], Step: [101/625], Training Acc: 97.6,Validation Acc: 84.66\n",
      "Epoch: [7/10], Step: [151/625], Training Acc: 97.25,Validation Acc: 84.4\n",
      "Epoch: [7/10], Step: [201/625], Training Acc: 97.48,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [251/625], Training Acc: 97.555,Validation Acc: 84.32\n",
      "Epoch: [7/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 84.54\n",
      "Epoch: [7/10], Step: [351/625], Training Acc: 96.79,Validation Acc: 83.8\n",
      "Epoch: [7/10], Step: [401/625], Training Acc: 97.3,Validation Acc: 83.76\n",
      "Epoch: [7/10], Step: [451/625], Training Acc: 97.43,Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [501/625], Training Acc: 97.525,Validation Acc: 84.22\n",
      "Epoch: [7/10], Step: [551/625], Training Acc: 97.67,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [601/625], Training Acc: 97.295,Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [51/625], Training Acc: 97.83,Validation Acc: 84.56\n",
      "Epoch: [8/10], Step: [101/625], Training Acc: 97.805,Validation Acc: 84.52\n",
      "Epoch: [8/10], Step: [151/625], Training Acc: 97.855,Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [201/625], Training Acc: 97.81,Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [251/625], Training Acc: 97.655,Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [301/625], Training Acc: 97.075,Validation Acc: 84.06\n",
      "Epoch: [8/10], Step: [351/625], Training Acc: 97.505,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [401/625], Training Acc: 97.59,Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [451/625], Training Acc: 97.46,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [501/625], Training Acc: 97.655,Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [551/625], Training Acc: 96.37,Validation Acc: 82.9\n",
      "Epoch: [8/10], Step: [601/625], Training Acc: 96.815,Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [51/625], Training Acc: 97.91,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [101/625], Training Acc: 97.89,Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [151/625], Training Acc: 97.805,Validation Acc: 83.56\n",
      "Epoch: [9/10], Step: [201/625], Training Acc: 97.41,Validation Acc: 83.74\n",
      "Epoch: [9/10], Step: [251/625], Training Acc: 97.95,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 83.68\n",
      "Epoch: [9/10], Step: [351/625], Training Acc: 97.85,Validation Acc: 83.92\n",
      "Epoch: [9/10], Step: [401/625], Training Acc: 98.08,Validation Acc: 83.64\n",
      "Epoch: [9/10], Step: [451/625], Training Acc: 98.22,Validation Acc: 83.48\n",
      "Epoch: [9/10], Step: [501/625], Training Acc: 97.505,Validation Acc: 83.3\n",
      "Epoch: [9/10], Step: [551/625], Training Acc: 98.14,Validation Acc: 83.62\n",
      "Epoch: [9/10], Step: [601/625], Training Acc: 97.92,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [51/625], Training Acc: 98.1,Validation Acc: 83.78\n",
      "Epoch: [10/10], Step: [101/625], Training Acc: 96.255,Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [151/625], Training Acc: 97.645,Validation Acc: 83.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [201/625], Training Acc: 98.11,Validation Acc: 83.42\n",
      "Epoch: [10/10], Step: [251/625], Training Acc: 96.115,Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [301/625], Training Acc: 97.575,Validation Acc: 83.18\n",
      "Epoch: [10/10], Step: [351/625], Training Acc: 98.245,Validation Acc: 83.76\n",
      "Epoch: [10/10], Step: [401/625], Training Acc: 98.13,Validation Acc: 83.66\n",
      "Epoch: [10/10], Step: [451/625], Training Acc: 98.185,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [501/625], Training Acc: 98.445,Validation Acc: 83.36\n",
      "Epoch: [10/10], Step: [551/625], Training Acc: 97.95,Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [601/625], Training Acc: 97.66,Validation Acc: 83.54\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # check training score every 100 iterations\n",
    "        ## validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {},Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, \n",
    "                len(train_loader), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we are going to try to optimize are the following:\n",
    "\n",
    "* n-gram max length\n",
    "* optimizer choice\n",
    "* embedding size\n",
    "* vocab size\n",
    "* learning rate of the optimizer\n",
    "\n",
    "And maybe increase the batch size to speed up the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = [torch.optim.Adam(model.parameters(), \n",
    "                               lr=learning_rate),             \n",
    "              torch.optim.SGD(model.parameters(), \n",
    "                              lr=learning_rate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[14269 16316  4649 ...  8312 14965 23410]\n",
      "20000\n",
      "20000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)\n",
    "\n",
    "shuffled_index[:training_size]\n",
    "\n",
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))\n",
    "\n",
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all ngram tokens for easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams = params[1]\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "grams = 4\n",
    "\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram=grams)\n",
    "\n",
    "# Tokenize Validation\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                      n_gram=grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_4\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "grams = \"lemma_4\"\n",
    "print(grams)\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_\"+str(grams)+\".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_tokens[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_tokens[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(val_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params,\n",
    "                          epochs=5,\n",
    "                          optimizer_name = \"Adam\",\n",
    "                          lemmatize = False):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    # counter for progress\n",
    "    count = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Parameter Combination = \" + str(count+1) + \" / \" + str(len(param_space)))\n",
    "        count = count + 1      \n",
    "        \n",
    "        NUM_EPOCHS = epochs\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = int(param_comb[2]) # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        MAX_SENTENCE_LENGTH = param_comb[4] # max sentence length of data loader\n",
    "        BATCH_SIZE = param_comb[5]\n",
    "        \n",
    "        print(\"Learning Rate = \" + str(lr_rate))\n",
    "        print(\"Ngram = \" + str(grams))\n",
    "        print(\"Vocab Size = \" + str(max_vocab_size))\n",
    "        print(\"Embedding Dimension = \" + str(embed_dimension))\n",
    "        print(\"Max Sentence Length = \" + str(MAX_SENTENCE_LENGTH))\n",
    "        print(\"Batch Size = \" + str(BATCH_SIZE))\n",
    "\n",
    "        # Tokenization\n",
    "        # All tokens are created before the hyperparameter search loop\n",
    "        # Load the tokens here\n",
    "        \n",
    "        \n",
    "        train_data_tokens = pkl.load(open(\"train_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        all_train_tokens = pkl.load(open(\"all_train_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "\n",
    "        val_data_tokens = pkl.load(open(\"val_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        \n",
    "        print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "        print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "        \n",
    "        # Building Vocabulary\n",
    "        # implicitly gets the max_vocab_size parameter\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                         max_vocab_size=max_vocab_size)\n",
    "        \n",
    "        # Lets check the dictionary by loading random token from it\n",
    "        random_token_id = random.randint(0, len(id2token)-1)\n",
    "        random_token = id2token[random_token_id]\n",
    "        print (\"Token id {} -> token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "        print (\"Token {} -> token id {}\".format(random_token, token2id[random_token]))\n",
    "        \n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "        # double checking\n",
    "        print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Load training and validation data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)  \n",
    "\n",
    "        # Initialize the N-gram Model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "        \n",
    "        # Both Adam and SGD will be tried\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "        else:\n",
    "            print(\"this optimizer is not implemented yet\")\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        # Caution: Two different optimizers\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "    #for optimizer in optimizers:\n",
    "        print(\"Optimization Start\")\n",
    "        print(optimizer)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Validate every 100 iterations\n",
    "                # Adjust it to accustom changing batch sizes\n",
    "                if i > 0 and i % (50 * (64 / BATCH_SIZE)) == 0:\n",
    "\n",
    "                    # Accuracy Calculations\n",
    "                    train_acc = test_model(train_loader, model)\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                    # Logging\n",
    "                    print('Epoch:[{}/{}],Step:[{}/{}],Training Acc:{},Validation Acc:{}'.format( \n",
    "                               epoch+1, NUM_EPOCHS, \n",
    "                                i+1, len(train_loader), \n",
    "                                train_acc, val_acc))\n",
    "                      \n",
    "    return val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.01, 1, 100000.0, 100, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 100, 128),\n",
       " (0.01, 1, 100000.0, 100, 200, 64),\n",
       " (0.01, 1, 100000.0, 100, 200, 128),\n",
       " (0.01, 1, 100000.0, 150, 100, 64),\n",
       " (0.01, 1, 100000.0, 150, 100, 128),\n",
       " (0.01, 1, 100000.0, 150, 200, 64),\n",
       " (0.01, 1, 100000.0, 150, 200, 128),\n",
       " (0.01, 1, 100000.0, 200, 100, 64),\n",
       " (0.01, 1, 100000.0, 200, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 100, 128),\n",
       " (0.01, 1, 1000000.0, 100, 200, 64),\n",
       " (0.01, 1, 1000000.0, 100, 200, 128),\n",
       " (0.01, 1, 1000000.0, 150, 100, 64),\n",
       " (0.01, 1, 1000000.0, 150, 100, 128),\n",
       " (0.01, 1, 1000000.0, 150, 200, 64),\n",
       " (0.01, 1, 1000000.0, 150, 200, 128),\n",
       " (0.01, 1, 1000000.0, 200, 100, 64),\n",
       " (0.01, 1, 1000000.0, 200, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 100, 128),\n",
       " (0.01, 2, 100000.0, 100, 200, 64),\n",
       " (0.01, 2, 100000.0, 100, 200, 128),\n",
       " (0.01, 2, 100000.0, 150, 100, 64),\n",
       " (0.01, 2, 100000.0, 150, 100, 128),\n",
       " (0.01, 2, 100000.0, 150, 200, 64),\n",
       " (0.01, 2, 100000.0, 150, 200, 128),\n",
       " (0.01, 2, 100000.0, 200, 100, 64),\n",
       " (0.01, 2, 100000.0, 200, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 100, 128),\n",
       " (0.01, 2, 1000000.0, 100, 200, 64),\n",
       " (0.01, 2, 1000000.0, 100, 200, 128),\n",
       " (0.01, 2, 1000000.0, 150, 100, 64),\n",
       " (0.01, 2, 1000000.0, 150, 100, 128),\n",
       " (0.01, 2, 1000000.0, 150, 200, 64),\n",
       " (0.01, 2, 1000000.0, 150, 200, 128),\n",
       " (0.01, 2, 1000000.0, 200, 100, 64),\n",
       " (0.01, 2, 1000000.0, 200, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 100, 128),\n",
       " (0.01, 3, 100000.0, 100, 200, 64),\n",
       " (0.01, 3, 100000.0, 100, 200, 128),\n",
       " (0.01, 3, 100000.0, 150, 100, 64),\n",
       " (0.01, 3, 100000.0, 150, 100, 128),\n",
       " (0.01, 3, 100000.0, 150, 200, 64),\n",
       " (0.01, 3, 100000.0, 150, 200, 128),\n",
       " (0.01, 3, 100000.0, 200, 100, 64),\n",
       " (0.01, 3, 100000.0, 200, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 100, 128),\n",
       " (0.01, 3, 1000000.0, 100, 200, 64),\n",
       " (0.01, 3, 1000000.0, 100, 200, 128),\n",
       " (0.01, 3, 1000000.0, 150, 100, 64),\n",
       " (0.01, 3, 1000000.0, 150, 100, 128),\n",
       " (0.01, 3, 1000000.0, 150, 200, 64),\n",
       " (0.01, 3, 1000000.0, 150, 200, 128),\n",
       " (0.01, 3, 1000000.0, 200, 100, 64),\n",
       " (0.01, 3, 1000000.0, 200, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 100, 128),\n",
       " (0.1, 1, 100000.0, 100, 200, 64),\n",
       " (0.1, 1, 100000.0, 100, 200, 128),\n",
       " (0.1, 1, 100000.0, 150, 100, 64),\n",
       " (0.1, 1, 100000.0, 150, 100, 128),\n",
       " (0.1, 1, 100000.0, 150, 200, 64),\n",
       " (0.1, 1, 100000.0, 150, 200, 128),\n",
       " (0.1, 1, 100000.0, 200, 100, 64),\n",
       " (0.1, 1, 100000.0, 200, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 100, 128),\n",
       " (0.1, 1, 1000000.0, 100, 200, 64),\n",
       " (0.1, 1, 1000000.0, 100, 200, 128),\n",
       " (0.1, 1, 1000000.0, 150, 100, 64),\n",
       " (0.1, 1, 1000000.0, 150, 100, 128),\n",
       " (0.1, 1, 1000000.0, 150, 200, 64),\n",
       " (0.1, 1, 1000000.0, 150, 200, 128),\n",
       " (0.1, 1, 1000000.0, 200, 100, 64),\n",
       " (0.1, 1, 1000000.0, 200, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 100, 128),\n",
       " (0.1, 2, 100000.0, 100, 200, 64),\n",
       " (0.1, 2, 100000.0, 100, 200, 128),\n",
       " (0.1, 2, 100000.0, 150, 100, 64),\n",
       " (0.1, 2, 100000.0, 150, 100, 128),\n",
       " (0.1, 2, 100000.0, 150, 200, 64),\n",
       " (0.1, 2, 100000.0, 150, 200, 128),\n",
       " (0.1, 2, 100000.0, 200, 100, 64),\n",
       " (0.1, 2, 100000.0, 200, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 100, 128),\n",
       " (0.1, 2, 1000000.0, 100, 200, 64),\n",
       " (0.1, 2, 1000000.0, 100, 200, 128),\n",
       " (0.1, 2, 1000000.0, 150, 100, 64),\n",
       " (0.1, 2, 1000000.0, 150, 100, 128),\n",
       " (0.1, 2, 1000000.0, 150, 200, 64),\n",
       " (0.1, 2, 1000000.0, 150, 200, 128),\n",
       " (0.1, 2, 1000000.0, 200, 100, 64),\n",
       " (0.1, 2, 1000000.0, 200, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 100, 128),\n",
       " (0.1, 3, 100000.0, 100, 200, 64),\n",
       " (0.1, 3, 100000.0, 100, 200, 128),\n",
       " (0.1, 3, 100000.0, 150, 100, 64),\n",
       " (0.1, 3, 100000.0, 150, 100, 128),\n",
       " (0.1, 3, 100000.0, 150, 200, 64),\n",
       " (0.1, 3, 100000.0, 150, 200, 128),\n",
       " (0.1, 3, 100000.0, 200, 100, 64),\n",
       " (0.1, 3, 100000.0, 200, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 100, 128),\n",
       " (0.1, 3, 1000000.0, 100, 200, 64),\n",
       " (0.1, 3, 1000000.0, 100, 200, 128),\n",
       " (0.1, 3, 1000000.0, 150, 100, 64),\n",
       " (0.1, 3, 1000000.0, 150, 100, 128),\n",
       " (0.1, 3, 1000000.0, 150, 200, 64),\n",
       " (0.1, 3, 1000000.0, 150, 200, 128),\n",
       " (0.1, 3, 1000000.0, 200, 100, 64),\n",
       " (0.1, 3, 1000000.0, 200, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 200, 128),\n",
       " (1, 1, 100000.0, 100, 100, 64),\n",
       " (1, 1, 100000.0, 100, 100, 128),\n",
       " (1, 1, 100000.0, 100, 200, 64),\n",
       " (1, 1, 100000.0, 100, 200, 128),\n",
       " (1, 1, 100000.0, 150, 100, 64),\n",
       " (1, 1, 100000.0, 150, 100, 128),\n",
       " (1, 1, 100000.0, 150, 200, 64),\n",
       " (1, 1, 100000.0, 150, 200, 128),\n",
       " (1, 1, 100000.0, 200, 100, 64),\n",
       " (1, 1, 100000.0, 200, 100, 128),\n",
       " (1, 1, 100000.0, 200, 200, 64),\n",
       " (1, 1, 100000.0, 200, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 100, 128),\n",
       " (1, 1, 1000000.0, 100, 200, 64),\n",
       " (1, 1, 1000000.0, 100, 200, 128),\n",
       " (1, 1, 1000000.0, 150, 100, 64),\n",
       " (1, 1, 1000000.0, 150, 100, 128),\n",
       " (1, 1, 1000000.0, 150, 200, 64),\n",
       " (1, 1, 1000000.0, 150, 200, 128),\n",
       " (1, 1, 1000000.0, 200, 100, 64),\n",
       " (1, 1, 1000000.0, 200, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 200, 128),\n",
       " (1, 2, 100000.0, 100, 100, 64),\n",
       " (1, 2, 100000.0, 100, 100, 128),\n",
       " (1, 2, 100000.0, 100, 200, 64),\n",
       " (1, 2, 100000.0, 100, 200, 128),\n",
       " (1, 2, 100000.0, 150, 100, 64),\n",
       " (1, 2, 100000.0, 150, 100, 128),\n",
       " (1, 2, 100000.0, 150, 200, 64),\n",
       " (1, 2, 100000.0, 150, 200, 128),\n",
       " (1, 2, 100000.0, 200, 100, 64),\n",
       " (1, 2, 100000.0, 200, 100, 128),\n",
       " (1, 2, 100000.0, 200, 200, 64),\n",
       " (1, 2, 100000.0, 200, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 100, 128),\n",
       " (1, 2, 1000000.0, 100, 200, 64),\n",
       " (1, 2, 1000000.0, 100, 200, 128),\n",
       " (1, 2, 1000000.0, 150, 100, 64),\n",
       " (1, 2, 1000000.0, 150, 100, 128),\n",
       " (1, 2, 1000000.0, 150, 200, 64),\n",
       " (1, 2, 1000000.0, 150, 200, 128),\n",
       " (1, 2, 1000000.0, 200, 100, 64),\n",
       " (1, 2, 1000000.0, 200, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 200, 128),\n",
       " (1, 3, 100000.0, 100, 100, 64),\n",
       " (1, 3, 100000.0, 100, 100, 128),\n",
       " (1, 3, 100000.0, 100, 200, 64),\n",
       " (1, 3, 100000.0, 100, 200, 128),\n",
       " (1, 3, 100000.0, 150, 100, 64),\n",
       " (1, 3, 100000.0, 150, 100, 128),\n",
       " (1, 3, 100000.0, 150, 200, 64),\n",
       " (1, 3, 100000.0, 150, 200, 128),\n",
       " (1, 3, 100000.0, 200, 100, 64),\n",
       " (1, 3, 100000.0, 200, 100, 128),\n",
       " (1, 3, 100000.0, 200, 200, 64),\n",
       " (1, 3, 100000.0, 200, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 100, 128),\n",
       " (1, 3, 1000000.0, 100, 200, 64),\n",
       " (1, 3, 1000000.0, 100, 200, 128),\n",
       " (1, 3, 1000000.0, 150, 100, 64),\n",
       " (1, 3, 1000000.0, 150, 100, 128),\n",
       " (1, 3, 1000000.0, 150, 200, 64),\n",
       " (1, 3, 1000000.0, 150, 200, 128),\n",
       " (1, 3, 1000000.0, 200, 100, 64),\n",
       " (1, 3, 1000000.0, 200, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 200, 128),\n",
       " (2, 1, 100000.0, 100, 100, 64),\n",
       " (2, 1, 100000.0, 100, 100, 128),\n",
       " (2, 1, 100000.0, 100, 200, 64),\n",
       " (2, 1, 100000.0, 100, 200, 128),\n",
       " (2, 1, 100000.0, 150, 100, 64),\n",
       " (2, 1, 100000.0, 150, 100, 128),\n",
       " (2, 1, 100000.0, 150, 200, 64),\n",
       " (2, 1, 100000.0, 150, 200, 128),\n",
       " (2, 1, 100000.0, 200, 100, 64),\n",
       " (2, 1, 100000.0, 200, 100, 128),\n",
       " (2, 1, 100000.0, 200, 200, 64),\n",
       " (2, 1, 100000.0, 200, 200, 128),\n",
       " (2, 1, 1000000.0, 100, 100, 64),\n",
       " (2, 1, 1000000.0, 100, 100, 128),\n",
       " (2, 1, 1000000.0, 100, 200, 64),\n",
       " (2, 1, 1000000.0, 100, 200, 128),\n",
       " (2, 1, 1000000.0, 150, 100, 64),\n",
       " (2, 1, 1000000.0, 150, 100, 128),\n",
       " (2, 1, 1000000.0, 150, 200, 64),\n",
       " (2, 1, 1000000.0, 150, 200, 128),\n",
       " (2, 1, 1000000.0, 200, 100, 64),\n",
       " (2, 1, 1000000.0, 200, 100, 128),\n",
       " (2, 1, 1000000.0, 200, 200, 64),\n",
       " (2, 1, 1000000.0, 200, 200, 128),\n",
       " (2, 2, 100000.0, 100, 100, 64),\n",
       " (2, 2, 100000.0, 100, 100, 128),\n",
       " (2, 2, 100000.0, 100, 200, 64),\n",
       " (2, 2, 100000.0, 100, 200, 128),\n",
       " (2, 2, 100000.0, 150, 100, 64),\n",
       " (2, 2, 100000.0, 150, 100, 128),\n",
       " (2, 2, 100000.0, 150, 200, 64),\n",
       " (2, 2, 100000.0, 150, 200, 128),\n",
       " (2, 2, 100000.0, 200, 100, 64),\n",
       " (2, 2, 100000.0, 200, 100, 128),\n",
       " (2, 2, 100000.0, 200, 200, 64),\n",
       " (2, 2, 100000.0, 200, 200, 128),\n",
       " (2, 2, 1000000.0, 100, 100, 64),\n",
       " (2, 2, 1000000.0, 100, 100, 128),\n",
       " (2, 2, 1000000.0, 100, 200, 64),\n",
       " (2, 2, 1000000.0, 100, 200, 128),\n",
       " (2, 2, 1000000.0, 150, 100, 64),\n",
       " (2, 2, 1000000.0, 150, 100, 128),\n",
       " (2, 2, 1000000.0, 150, 200, 64),\n",
       " (2, 2, 1000000.0, 150, 200, 128),\n",
       " (2, 2, 1000000.0, 200, 100, 64),\n",
       " (2, 2, 1000000.0, 200, 100, 128),\n",
       " (2, 2, 1000000.0, 200, 200, 64),\n",
       " (2, 2, 1000000.0, 200, 200, 128),\n",
       " (2, 3, 100000.0, 100, 100, 64),\n",
       " (2, 3, 100000.0, 100, 100, 128),\n",
       " (2, 3, 100000.0, 100, 200, 64),\n",
       " (2, 3, 100000.0, 100, 200, 128),\n",
       " (2, 3, 100000.0, 150, 100, 64),\n",
       " (2, 3, 100000.0, 150, 100, 128),\n",
       " (2, 3, 100000.0, 150, 200, 64),\n",
       " (2, 3, 100000.0, 150, 200, 128),\n",
       " (2, 3, 100000.0, 200, 100, 64),\n",
       " (2, 3, 100000.0, 200, 100, 128),\n",
       " (2, 3, 100000.0, 200, 200, 64),\n",
       " (2, 3, 100000.0, 200, 200, 128),\n",
       " (2, 3, 1000000.0, 100, 100, 64),\n",
       " (2, 3, 1000000.0, 100, 100, 128),\n",
       " (2, 3, 1000000.0, 100, 200, 64),\n",
       " (2, 3, 1000000.0, 100, 200, 128),\n",
       " (2, 3, 1000000.0, 150, 100, 64),\n",
       " (2, 3, 1000000.0, 150, 100, 128),\n",
       " (2, 3, 1000000.0, 150, 200, 64),\n",
       " (2, 3, 1000000.0, 150, 200, 128),\n",
       " (2, 3, 1000000.0, 200, 100, 64),\n",
       " (2, 3, 1000000.0, 200, 100, 128),\n",
       " (2, 3, 1000000.0, 200, 200, 64),\n",
       " (2, 3, 1000000.0, 200, 200, 128)]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [[1e-2,1e-1,1,2], ## learning rates\n",
    "          list(range(1,4)), ## ngrams\n",
    "          [1e5,1e6], ## vocab size\n",
    "          [100,150,200], ## embedding size\n",
    "          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "# params = [[1e-1,1,2,5], ## learning rates\n",
    "#           list(range(1,2)), ## ngrams\n",
    "#           [1e5], ## vocab size\n",
    "#           [100], ## embedding size\n",
    "#           [100], ## max sentence length\n",
    "#           [64] ## batch size\n",
    "#          ]\n",
    "\n",
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 1948 -> token fate\n",
      "Token fate -> token id 1948\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.62,Validation Acc:69.36\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.99,Validation Acc:80.88\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.16,Validation Acc:83.86\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.715,Validation Acc:84.4\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.31,Validation Acc:85.72\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.01,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:88.905,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.795,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.58,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.205,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.685,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.53,Validation Acc:86.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 2 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 43413 -> token robyn\n",
      "Token robyn -> token id 43413\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.13,Validation Acc:64.2\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:72.11,Validation Acc:71.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:78.99,Validation Acc:77.54\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.815,Validation Acc:82.24\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.525,Validation Acc:83.58\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.535,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.24,Validation Acc:84.6\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.17,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.01,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.205,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.01,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:89.715,Validation Acc:85.62\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 3 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 25784 -> token nitro\n",
      "Token nitro -> token id 25784\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:64.59,Validation Acc:63.76\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:79.045,Validation Acc:77.62\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.31,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.59,Validation Acc:84.26\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:86.955,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.24,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.82,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.93,Validation Acc:84.82\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.275,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.905,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.95,Validation Acc:85.86\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.95,Validation Acc:85.68\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 4 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 81015 -> token devlin\n",
      "Token devlin -> token id 81015\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.84,Validation Acc:60.42\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:70.765,Validation Acc:69.9\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:77.41,Validation Acc:76.56\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.4,Validation Acc:81.9\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.85,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.25,Validation Acc:85.42\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.995,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:87.995,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:88.985,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.625,Validation Acc:86.42\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.285,Validation Acc:86.58\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.325,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 5 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 54872 -> token ramen\n",
      "Token ramen -> token id 54872\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.61,Validation Acc:72.24\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.7,Validation Acc:81.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.03,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.745,Validation Acc:85.22\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.395,Validation Acc:85.3\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.45,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.935,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.505,Validation Acc:84.1\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.96,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.9,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.135,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.85,Validation Acc:86.08\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 6 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 52972 -> token this.*frodo\n",
      "Token this.*frodo -> token id 52972\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.825,Validation Acc:68.4\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.51,Validation Acc:73.1\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.8,Validation Acc:80.48\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.92,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.56,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.715,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.745,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.1,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.0,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.275,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.575,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.11,Validation Acc:86.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 7 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 68072 -> token astaire).it\n",
      "Token astaire).it -> token id 68072\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.67,Validation Acc:73.32\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.455,Validation Acc:81.96\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.85,Validation Acc:82.68\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.645,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.325,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.78,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.265,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.255,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.0,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.36,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:88.17,Validation Acc:83.76\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.5,Validation Acc:85.76\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 8 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 62287 -> token doodlebop\n",
      "Token doodlebop -> token id 62287\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:69.385,Validation Acc:68.32\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.915,Validation Acc:74.94\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.655,Validation Acc:81.62\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.415,Validation Acc:82.76\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.495,Validation Acc:84.48\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.775,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.79,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.25,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.915,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.02,Validation Acc:85.04\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.605,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.125,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 9 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 1045 -> token cold\n",
      "Token cold -> token id 1045\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.245,Validation Acc:67.52\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.82,Validation Acc:82.48\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.995,Validation Acc:84.18\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.2,Validation Acc:85.78\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.095,Validation Acc:85.7\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.41,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.25,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.11,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.075,Validation Acc:84.92\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.785,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.455,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.545,Validation Acc:86.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 10 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 28071 -> token deadliest\n",
      "Token deadliest -> token id 28071\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:69.825,Validation Acc:69.06\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:78.95,Validation Acc:77.62\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:84.325,Validation Acc:82.08\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.78,Validation Acc:83.68\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.455,Validation Acc:84.78\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.18,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.375,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.135,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.365,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.62,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.43,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.095,Validation Acc:85.38\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 11 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 54090 -> token again!the\n",
      "Token again!the -> token id 54090\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.52,Validation Acc:69.04\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.975,Validation Acc:82.44\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.54,Validation Acc:84.26\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.0,Validation Acc:85.48\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.12,Validation Acc:85.46\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.915,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:88.8,Validation Acc:84.76\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.745,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.04,Validation Acc:84.88\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.305,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.62,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.755,Validation Acc:86.0\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 12 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 53014 -> token 1955,1956,1957\n",
      "Token 1955,1956,1957 -> token id 53014\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:63.28,Validation Acc:62.42\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.87,Validation Acc:74.98\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:83.14,Validation Acc:81.68\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.07,Validation Acc:82.26\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.08,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.675,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.485,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.515,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.94,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.41,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.685,Validation Acc:85.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[151/157],Training Acc:90.765,Validation Acc:86.12\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 13 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 54056 -> token grads\n",
      "Token grads -> token id 54056\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:71.445,Validation Acc:70.32\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.745,Validation Acc:81.74\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.32,Validation Acc:83.6\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.63,Validation Acc:84.02\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.4,Validation Acc:85.1\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.955,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.715,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.095,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.735,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.445,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.245,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.28,Validation Acc:84.68\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 14 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 17427 -> token vida\n",
      "Token vida -> token id 17427\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:65.89,Validation Acc:65.26\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:73.495,Validation Acc:72.06\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.445,Validation Acc:79.28\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.565,Validation Acc:82.2\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.965,Validation Acc:83.32\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.92,Validation Acc:84.62\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.955,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.06,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.56,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.82,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:89.96,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.645,Validation Acc:86.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 15 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 8786 -> token regime\n",
      "Token regime -> token id 8786\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:62.525,Validation Acc:61.9\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.01,Validation Acc:81.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.06,Validation Acc:82.96\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.885,Validation Acc:84.8\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.255,Validation Acc:85.04\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:87.025,Validation Acc:83.42\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.54,Validation Acc:85.52\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.97,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.65,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.51,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.285,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.56,Validation Acc:85.64\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 16 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 48813 -> token lotion\n",
      "Token lotion -> token id 48813\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.52,Validation Acc:61.36\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.39,Validation Acc:73.42\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.61,Validation Acc:79.7\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.485,Validation Acc:81.38\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.285,Validation Acc:83.08\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.26,Validation Acc:84.86\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.305,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.635,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.12,Validation Acc:85.52\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.045,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:89.965,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.63,Validation Acc:86.56\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 17 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 38576 -> token 82-minute\n",
      "Token 82-minute -> token id 38576\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:72.29,Validation Acc:71.26\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.005,Validation Acc:80.46\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.795,Validation Acc:84.0\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:85.79,Validation Acc:82.82\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.82,Validation Acc:85.74\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:87.78,Validation Acc:83.82\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.31,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.07,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.58,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.675,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.595,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.305,Validation Acc:85.78\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 18 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 47559 -> token tomeihere\n",
      "Token tomeihere -> token id 47559\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:57.27,Validation Acc:57.52\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.705,Validation Acc:75.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.265,Validation Acc:78.72\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.255,Validation Acc:83.6\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.685,Validation Acc:84.18\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.92,Validation Acc:85.34\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.785,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.305,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.05,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.245,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.36,Validation Acc:86.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[151/157],Training Acc:90.985,Validation Acc:86.82\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 19 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 82229 -> token yasnaya\n",
      "Token yasnaya -> token id 82229\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.29,Validation Acc:73.72\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.375,Validation Acc:81.4\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.43,Validation Acc:83.44\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.52,Validation Acc:84.98\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.67,Validation Acc:85.64\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:87.65,Validation Acc:84.16\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.605,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.52,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.215,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:89.89,Validation Acc:84.76\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:89.15,Validation Acc:83.74\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.435,Validation Acc:85.78\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 20 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 72314 -> token timesif\n",
      "Token timesif -> token id 72314\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.015,Validation Acc:65.5\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:71.115,Validation Acc:70.32\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:79.3,Validation Acc:78.48\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.98,Validation Acc:82.98\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.82,Validation Acc:84.72\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.99,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.755,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.855,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.995,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.08,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.635,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.055,Validation Acc:86.72\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 21 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 34795 -> token metschurat\n",
      "Token metschurat -> token id 34795\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:76.07,Validation Acc:73.86\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.67,Validation Acc:79.7\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:87.055,Validation Acc:84.9\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:84.275,Validation Acc:81.26\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.205,Validation Acc:86.08\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.57,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.425,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.8,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.925,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.175,Validation Acc:85.92\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.905,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.3,Validation Acc:84.6\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 22 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 36038 -> token assassinating\n",
      "Token assassinating -> token id 36038\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:60.58,Validation Acc:59.98\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:77.34,Validation Acc:76.08\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:83.72,Validation Acc:81.74\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.63,Validation Acc:83.44\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.445,Validation Acc:84.72\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.465,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.675,Validation Acc:85.38\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.685,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.175,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.67,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.66,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.005,Validation Acc:86.22\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 23 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 42411 -> token hump\n",
      "Token hump -> token id 42411\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.925,Validation Acc:75.06\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.985,Validation Acc:82.78\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.315,Validation Acc:82.2\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.91,Validation Acc:85.62\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.98,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.265,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.22,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.635,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.535,Validation Acc:85.4\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.345,Validation Acc:85.4\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.44,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.125,Validation Acc:85.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 24 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 9180 -> token administration\n",
      "Token administration -> token id 9180\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.99,Validation Acc:66.62\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:77.505,Validation Acc:76.32\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:83.045,Validation Acc:81.66\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.085,Validation Acc:84.12\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.385,Validation Acc:85.04\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.345,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.675,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.46,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.05,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.115,Validation Acc:85.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[126/157],Training Acc:90.375,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.175,Validation Acc:86.18\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 25 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 29904 -> token are playing\n",
      "Token are playing -> token id 29904\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:71.575,Validation Acc:71.06\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.48,Validation Acc:79.68\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.11,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.34,Validation Acc:84.64\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.62,Validation Acc:85.26\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.83,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.655,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.785,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.58,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.875,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.905,Validation Acc:86.6\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.4,Validation Acc:86.48\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 26 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 52008 -> token a routine\n",
      "Token a routine -> token id 52008\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:65.64,Validation Acc:65.5\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:71.42,Validation Acc:69.42\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:70.18,Validation Acc:68.36\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.32,Validation Acc:81.7\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.445,Validation Acc:81.48\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.365,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.7,Validation Acc:84.94\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.36,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.545,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.92,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.455,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.77,Validation Acc:86.38\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 27 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 70124 -> token to baby\n",
      "Token to baby -> token id 70124\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:67.29,Validation Acc:65.52\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.58,Validation Acc:80.78\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.855,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:85.965,Validation Acc:83.12\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.25,Validation Acc:86.04\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.785,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.975,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.22,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.935,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.555,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.84,Validation Acc:85.38\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.37,Validation Acc:86.28\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 28 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 158 -> token here\n",
      "Token here -> token id 158\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.46,Validation Acc:65.34\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.605,Validation Acc:73.2\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:78.585,Validation Acc:77.04\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.8,Validation Acc:81.78\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.005,Validation Acc:83.86\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.515,Validation Acc:84.44\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.65,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.27,Validation Acc:84.04\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.185,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.495,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.29,Validation Acc:86.62\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.86,Validation Acc:86.98\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 29 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 90461 -> token said yes\n",
      "Token said yes -> token id 90461\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.66,Validation Acc:72.3\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.36,Validation Acc:81.18\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.94,Validation Acc:83.68\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.11,Validation Acc:84.62\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.345,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.375,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.27,Validation Acc:86.82\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.08,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.18,Validation Acc:86.68\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.765,Validation Acc:86.78\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.165,Validation Acc:86.8\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.195,Validation Acc:85.56\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 30 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 62192 -> token owns a\n",
      "Token owns a -> token id 62192\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.86,Validation Acc:60.88\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.44,Validation Acc:75.72\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.365,Validation Acc:80.86\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.47,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.525,Validation Acc:85.24\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.11,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.525,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.165,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.55,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.175,Validation Acc:85.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[126/157],Training Acc:91.92,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.795,Validation Acc:85.88\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 31 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 37876 -> token foul mouthed\n",
      "Token foul mouthed -> token id 37876\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:72.375,Validation Acc:70.74\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.51,Validation Acc:80.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.045,Validation Acc:83.8\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.895,Validation Acc:84.86\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.765,Validation Acc:83.6\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.615,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.755,Validation Acc:84.28\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.83,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.835,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.38,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.095,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.49,Validation Acc:86.24\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 32 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 85042 -> token be pointless\n",
      "Token be pointless -> token id 85042\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:67.34,Validation Acc:66.76\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.54,Validation Acc:74.16\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.45,Validation Acc:79.56\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.425,Validation Acc:83.08\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.28,Validation Acc:84.4\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.75,Validation Acc:84.7\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.73,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.675,Validation Acc:84.28\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.04,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.205,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.0,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.07,Validation Acc:86.26\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 33 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 57380 -> token him another\n",
      "Token him another -> token id 57380\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.745,Validation Acc:74.42\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.685,Validation Acc:80.88\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.79,Validation Acc:84.0\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.005,Validation Acc:84.22\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.095,Validation Acc:85.54\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.39,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.66,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.205,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.41,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.125,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.735,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.245,Validation Acc:86.12\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 34 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 84891 -> token screen after\n",
      "Token screen after -> token id 84891\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.16,Validation Acc:67.7\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:78.06,Validation Acc:76.7\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.65,Validation Acc:80.7\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.47,Validation Acc:84.42\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:88.19,Validation Acc:85.3\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:89.275,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.54,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.895,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.815,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.925,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.355,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.755,Validation Acc:86.72\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 35 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 91825 -> token was fresh\n",
      "Token was fresh -> token id 91825\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:76.32,Validation Acc:74.46\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:84.23,Validation Acc:81.8\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.905,Validation Acc:83.18\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.945,Validation Acc:84.26\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.37,Validation Acc:85.26\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.24,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.945,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.88,Validation Acc:86.42\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.245,Validation Acc:86.44\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.945,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.735,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.775,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 36 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 98216 -> token much appreciated\n",
      "Token much appreciated -> token id 98216\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.685,Validation Acc:65.56\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:77.605,Validation Acc:76.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.575,Validation Acc:80.06\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.195,Validation Acc:84.44\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.615,Validation Acc:85.16\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.97,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.14,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.89,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.52,Validation Acc:86.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[101/157],Training Acc:91.795,Validation Acc:86.5\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.595,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.805,Validation Acc:86.3\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 37 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 789909 -> token old nothwest\n",
      "Token old nothwest -> token id 789909\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.775,Validation Acc:69.66\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.275,Validation Acc:80.12\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.98,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:85.785,Validation Acc:82.62\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.01,Validation Acc:84.76\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:86.89,Validation Acc:82.92\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.9,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.29,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.205,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.405,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.82,Validation Acc:86.74\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.165,Validation Acc:86.5\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 38 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 901223 -> token ourselves kids\n",
      "Token ourselves kids -> token id 901223\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:67.805,Validation Acc:67.62\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:71.19,Validation Acc:71.1\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:77.65,Validation Acc:77.04\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.12,Validation Acc:82.18\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.0,Validation Acc:84.06\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.14,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.63,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.425,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.175,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.79,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.455,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.505,Validation Acc:85.98\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 39 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 424011 -> token us recruits\n",
      "Token us recruits -> token id 424011\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.57,Validation Acc:68.56\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:80.85,Validation Acc:79.88\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:83.125,Validation Acc:81.3\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.89,Validation Acc:83.84\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.975,Validation Acc:86.38\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.87,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.32,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.485,Validation Acc:86.64\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.915,Validation Acc:86.64\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.18,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.15,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.265,Validation Acc:86.42\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 40 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 122509 -> token weirdness is\n",
      "Token weirdness is -> token id 122509\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:50.725,Validation Acc:50.12\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.37,Validation Acc:73.4\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.135,Validation Acc:78.78\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.425,Validation Acc:81.22\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.815,Validation Acc:83.98\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.415,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.62,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.685,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.205,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.695,Validation Acc:86.64\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.1,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.69,Validation Acc:86.8\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 41 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 492297 -> token answer first\n",
      "Token answer first -> token id 492297\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:72.525,Validation Acc:70.86\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.37,Validation Acc:81.34\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.7,Validation Acc:84.24\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.95,Validation Acc:85.04\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.635,Validation Acc:85.86\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.825,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.39,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.73,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.965,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.975,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.965,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.46,Validation Acc:84.92\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 42 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 207689 -> token movie kicks\n",
      "Token movie kicks -> token id 207689\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:50.56,Validation Acc:50.04\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.14,Validation Acc:74.66\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.395,Validation Acc:80.4\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.955,Validation Acc:83.1\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.185,Validation Acc:84.96\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.175,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.445,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.84,Validation Acc:86.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[76/157],Training Acc:90.68,Validation Acc:86.62\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.85,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.78,Validation Acc:86.88\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.595,Validation Acc:86.7\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 43 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 762111 -> token there assumptions\n",
      "Token there assumptions -> token id 762111\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.17,Validation Acc:68.08\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.64,Validation Acc:80.96\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.08,Validation Acc:82.86\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.605,Validation Acc:83.16\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.37,Validation Acc:85.76\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.64,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.18,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.975,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.335,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.64,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.14,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.485,Validation Acc:86.08\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 44 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 593765 -> token figuratively in\n",
      "Token figuratively in -> token id 593765\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:63.235,Validation Acc:63.12\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.475,Validation Acc:75.5\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.775,Validation Acc:80.42\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.41,Validation Acc:83.12\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.66,Validation Acc:84.28\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.225,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.46,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.095,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.855,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.325,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.155,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.08,Validation Acc:86.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 45 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 870469 -> token rusty salted\n",
      "Token rusty salted -> token id 870469\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.285,Validation Acc:74.1\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.96,Validation Acc:81.82\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.64,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.62,Validation Acc:85.12\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.725,Validation Acc:85.28\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.98,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.61,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.58,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.38,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.95,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.71,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.59,Validation Acc:86.12\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 46 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 248685 -> token role now\n",
      "Token role now -> token id 248685\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.665,Validation Acc:61.06\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:78.86,Validation Acc:77.42\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.455,Validation Acc:80.36\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.05,Validation Acc:83.4\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.98,Validation Acc:83.98\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:89.155,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.22,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.615,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.11,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.605,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.635,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.465,Validation Acc:84.88\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 47 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 607524 -> token funny nope\n",
      "Token funny nope -> token id 607524\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.96,Validation Acc:73.86\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:84.15,Validation Acc:81.72\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.34,Validation Acc:83.42\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.845,Validation Acc:84.22\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.845,Validation Acc:85.8\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.76,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.35,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.69,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.805,Validation Acc:85.34\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.91,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.33,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.96,Validation Acc:85.84\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 48 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 634446 -> token one mademouiselle\n",
      "Token one mademouiselle -> token id 634446\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:71.205,Validation Acc:70.38\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:78.59,Validation Acc:77.52\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:84.025,Validation Acc:82.14\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.13,Validation Acc:83.54\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:88.135,Validation Acc:85.48\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.75,Validation Acc:85.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/2],Step:[26/157],Training Acc:90.24,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.37,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.185,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:92.08,Validation Acc:86.88\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.335,Validation Acc:86.84\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.735,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 49 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 74749 -> token place on a\n",
      "Token place on a -> token id 74749\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:67.38,Validation Acc:66.16\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:80.745,Validation Acc:79.64\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.96,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.27,Validation Acc:84.6\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.545,Validation Acc:84.7\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.915,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.545,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.175,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.705,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.31,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.79,Validation Acc:86.5\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.965,Validation Acc:86.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 50 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 82871 -> token the dead to\n",
      "Token the dead to -> token id 82871\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:54.185,Validation Acc:54.72\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:65.48,Validation Acc:64.92\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:78.71,Validation Acc:76.9\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:81.745,Validation Acc:80.52\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.615,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:82.17,Validation Acc:79.34\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.41,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.94,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.135,Validation Acc:84.86\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.64,Validation Acc:86.66\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.995,Validation Acc:86.44\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.63,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 51 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 69712 -> token day before\n",
      "Token day before -> token id 69712\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:57.255,Validation Acc:56.4\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:79.245,Validation Acc:77.48\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.385,Validation Acc:83.18\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.225,Validation Acc:84.68\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.725,Validation Acc:85.86\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.03,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.585,Validation Acc:86.6\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.105,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.455,Validation Acc:84.84\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.395,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.765,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.24,Validation Acc:86.38\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 52 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 13037 -> token parallel\n",
      "Token parallel -> token id 13037\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:50.595,Validation Acc:50.08\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:63.47,Validation Acc:62.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:73.415,Validation Acc:70.8\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:82.395,Validation Acc:80.8\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.435,Validation Acc:82.2\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.435,Validation Acc:83.7\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.535,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.98,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.34,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.48,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.945,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:89.57,Validation Acc:84.44\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 53 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 72481 -> token selfishness\n",
      "Token selfishness -> token id 72481\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.175,Validation Acc:73.02\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.59,Validation Acc:81.3\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.8,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.44,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.43,Validation Acc:85.74\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.44,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.225,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.82,Validation Acc:83.44\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.975,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.49,Validation Acc:85.92\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.0,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.975,Validation Acc:86.22\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 54 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 27390 -> token believability\n",
      "Token believability -> token id 27390\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.045,Validation Acc:67.7\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:73.04,Validation Acc:72.2\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.77,Validation Acc:78.84\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.315,Validation Acc:81.34\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.075,Validation Acc:84.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[151/157],Training Acc:88.315,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.365,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.845,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.34,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.245,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.93,Validation Acc:86.66\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.485,Validation Acc:86.3\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 55 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 39849 -> token believe me i\n",
      "Token believe me i -> token id 39849\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:66.65,Validation Acc:66.52\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:80.61,Validation Acc:79.02\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.395,Validation Acc:84.3\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.205,Validation Acc:85.5\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.315,Validation Acc:85.86\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.71,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.72,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.68,Validation Acc:86.62\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.93,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.99,Validation Acc:84.88\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.59,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.44,Validation Acc:86.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 56 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 99128 -> token murdering his\n",
      "Token murdering his -> token id 99128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:63.7,Validation Acc:63.2\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:67.705,Validation Acc:66.44\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.075,Validation Acc:78.7\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.04,Validation Acc:83.12\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.115,Validation Acc:84.16\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.185,Validation Acc:84.96\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.12,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.915,Validation Acc:85.52\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.385,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.595,Validation Acc:84.24\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.36,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.325,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 57 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 5684 -> token long as\n",
      "Token long as -> token id 5684\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.725,Validation Acc:69.3\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.32,Validation Acc:80.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.675,Validation Acc:83.72\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.355,Validation Acc:84.02\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.805,Validation Acc:86.1\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:91.04,Validation Acc:86.68\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.67,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.425,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.26,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.875,Validation Acc:86.58\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.21,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.55,Validation Acc:85.94\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 58 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 31286 -> token to horror\n",
      "Token to horror -> token id 31286\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:54.58,Validation Acc:54.22\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.275,Validation Acc:73.6\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.295,Validation Acc:79.64\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.265,Validation Acc:83.16\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.41,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.865,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.885,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.195,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.565,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.915,Validation Acc:86.62\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.295,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.55,Validation Acc:86.64\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 59 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 90415 -> token over 2\n",
      "Token over 2 -> token id 90415\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.745,Validation Acc:69.84\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.66,Validation Acc:81.82\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.23,Validation Acc:84.16\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.23,Validation Acc:83.74\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.45,Validation Acc:85.5\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.3,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.39,Validation Acc:86.42\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.855,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.515,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.165,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.42,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.54,Validation Acc:85.84\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 60 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 5791 -> token seen in a\n",
      "Token seen in a -> token id 5791\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.885,Validation Acc:68.16\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.995,Validation Acc:75.36\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.1,Validation Acc:78.72\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.975,Validation Acc:84.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[126/157],Training Acc:86.245,Validation Acc:83.76\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.17,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.9,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.06,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.895,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.03,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.9,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.805,Validation Acc:87.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 61 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 866962 -> token scream they set\n",
      "Token scream they set -> token id 866962\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:68.555,Validation Acc:67.44\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:80.86,Validation Acc:78.94\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.195,Validation Acc:82.4\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.64,Validation Acc:84.94\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.425,Validation Acc:85.06\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.85,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.645,Validation Acc:86.66\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.61,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.57,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.205,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.435,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.26,Validation Acc:85.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 62 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 860619 -> token was fun when\n",
      "Token was fun when -> token id 860619\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:53.36,Validation Acc:52.5\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:71.145,Validation Acc:70.34\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:73.12,Validation Acc:72.06\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:82.98,Validation Acc:81.36\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:83.4,Validation Acc:81.1\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.73,Validation Acc:83.86\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.745,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:87.355,Validation Acc:83.42\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.96,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.645,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.425,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.14,Validation Acc:85.98\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 63 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 155749 -> token music is by\n",
      "Token music is by -> token id 155749\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.385,Validation Acc:67.72\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:78.64,Validation Acc:76.8\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.285,Validation Acc:83.46\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.61,Validation Acc:84.88\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.48,Validation Acc:83.96\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.475,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.6,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.225,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.505,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.105,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.73,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.12,Validation Acc:86.34\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 64 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 326540 -> token darth vader in\n",
      "Token darth vader in -> token id 326540\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:52.805,Validation Acc:52.82\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:68.515,Validation Acc:67.82\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:72.85,Validation Acc:72.04\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:79.805,Validation Acc:77.64\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.12,Validation Acc:82.34\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.32,Validation Acc:83.74\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.26,Validation Acc:83.9\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:87.88,Validation Acc:84.26\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.075,Validation Acc:85.48\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.925,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.795,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.325,Validation Acc:86.52\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 65 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 132272 -> token of it like\n",
      "Token of it like -> token id 132272\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.0,Validation Acc:72.0\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.17,Validation Acc:80.54\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.625,Validation Acc:83.64\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.08,Validation Acc:84.86\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.205,Validation Acc:84.0\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.345,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.965,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.685,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.27,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.545,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.055,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.54,Validation Acc:85.54\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 66 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 623665 -> token 9/10 to\n",
      "Token 9/10 to -> token id 623665\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:62.63,Validation Acc:62.86\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:72.68,Validation Acc:72.16\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.575,Validation Acc:79.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[101/157],Training Acc:82.12,Validation Acc:80.28\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.465,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.065,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.46,Validation Acc:86.16\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.765,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.415,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.71,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.8,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.245,Validation Acc:86.7\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 67 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 573441 -> token morita to\n",
      "Token morita to -> token id 573441\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:60.845,Validation Acc:61.08\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.47,Validation Acc:79.6\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.745,Validation Acc:83.46\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.905,Validation Acc:84.8\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.525,Validation Acc:85.08\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.505,Validation Acc:84.28\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.135,Validation Acc:86.16\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.645,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.225,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.59,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.965,Validation Acc:86.44\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.275,Validation Acc:86.16\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 68 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 871949 -> token madness and michel\n",
      "Token madness and michel -> token id 871949\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:64.365,Validation Acc:64.5\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:72.4,Validation Acc:70.8\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:80.735,Validation Acc:79.0\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.135,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.045,Validation Acc:84.7\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.255,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.165,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.07,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.745,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.405,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.355,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.365,Validation Acc:86.62\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 69 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 923611 -> token also sends the\n",
      "Token also sends the -> token id 923611\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.635,Validation Acc:72.04\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.665,Validation Acc:80.7\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.185,Validation Acc:83.58\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.325,Validation Acc:84.88\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.275,Validation Acc:85.08\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.43,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.675,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.235,Validation Acc:85.48\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.025,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.48,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.985,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.315,Validation Acc:83.74\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 70 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 381325 -> token the last mimzy\n",
      "Token the last mimzy -> token id 381325\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:60.215,Validation Acc:59.52\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:77.215,Validation Acc:75.84\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.955,Validation Acc:81.12\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.05,Validation Acc:83.2\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.845,Validation Acc:84.96\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.89,Validation Acc:83.84\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.24,Validation Acc:83.84\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.77,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.04,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.89,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.21,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.82,Validation Acc:85.68\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 71 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 940992 -> token the costumes which\n",
      "Token the costumes which -> token id 940992\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.885,Validation Acc:72.94\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.865,Validation Acc:80.72\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.35,Validation Acc:84.42\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.15,Validation Acc:85.06\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.905,Validation Acc:86.14\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.525,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.64,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.18,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.53,Validation Acc:84.84\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.57,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.52,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.575,Validation Acc:84.9\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 72 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14231568\n",
      "Token id 139029 -> token the richard\n",
      "Token the richard -> token id 139029\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:69.395,Validation Acc:68.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[51/157],Training Acc:71.26,Validation Acc:70.66\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.635,Validation Acc:80.6\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.67,Validation Acc:81.7\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.605,Validation Acc:83.5\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.755,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.715,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.61,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.72,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.29,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.295,Validation Acc:86.5\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.755,Validation Acc:86.56\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 73 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 52317 -> token my friend and i\n",
      "Token my friend and i -> token id 52317\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:64.785,Validation Acc:64.4\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:77.17,Validation Acc:75.7\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.005,Validation Acc:82.92\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.585,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.25,Validation Acc:84.66\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:87.945,Validation Acc:83.26\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.295,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.22,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.415,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.305,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.22,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.925,Validation Acc:86.42\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 74 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 28193 -> token there are more\n",
      "Token there are more -> token id 28193\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.09,Validation Acc:61.4\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:71.55,Validation Acc:70.96\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:79.005,Validation Acc:77.94\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:82.86,Validation Acc:81.18\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.515,Validation Acc:83.5\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.78,Validation Acc:84.22\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.785,Validation Acc:84.2\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.24,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.81,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.22,Validation Acc:85.38\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.555,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.12,Validation Acc:85.64\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 75 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 97573 -> token attraction of\n",
      "Token attraction of -> token id 97573\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:61.235,Validation Acc:60.72\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:80.34,Validation Acc:78.8\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:81.875,Validation Acc:78.96\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.725,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.655,Validation Acc:85.02\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.615,Validation Acc:84.94\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.72,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.15,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.525,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.155,Validation Acc:85.92\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.28,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.175,Validation Acc:85.66\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 76 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 81520 -> token to inspire\n",
      "Token to inspire -> token id 81520\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:67.275,Validation Acc:65.1\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:68.235,Validation Acc:67.5\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:77.65,Validation Acc:76.02\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:79.745,Validation Acc:77.72\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.745,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.63,Validation Acc:83.5\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.235,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.155,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.82,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.535,Validation Acc:84.94\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:89.55,Validation Acc:84.42\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.675,Validation Acc:86.66\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 77 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 21061 -> token a cast of\n",
      "Token a cast of -> token id 21061\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.09,Validation Acc:72.26\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.375,Validation Acc:80.34\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.175,Validation Acc:83.68\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.39,Validation Acc:85.28\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.34,Validation Acc:85.7\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.35,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.13,Validation Acc:86.58\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.545,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.025,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.12,Validation Acc:85.86\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.39,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.715,Validation Acc:86.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 78 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 10570 -> token movie that is\n",
      "Token movie that is -> token id 10570\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[26/157],Training Acc:69.09,Validation Acc:67.52\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:70.345,Validation Acc:70.08\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.735,Validation Acc:79.98\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.97,Validation Acc:82.6\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.605,Validation Acc:83.58\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.965,Validation Acc:84.94\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.905,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.975,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.58,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.055,Validation Acc:86.4\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.5,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.705,Validation Acc:86.3\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 79 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 70628 -> token her children\n",
      "Token her children -> token id 70628\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:72.02,Validation Acc:71.22\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:78.26,Validation Acc:76.5\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.825,Validation Acc:82.76\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.255,Validation Acc:84.42\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.38,Validation Acc:85.32\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.305,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.215,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.57,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.46,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.49,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.785,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.515,Validation Acc:85.96\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 80 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 12831 -> token to people\n",
      "Token to people -> token id 12831\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:50.38,Validation Acc:50.58\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.075,Validation Acc:73.48\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.705,Validation Acc:79.84\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.21,Validation Acc:83.28\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.885,Validation Acc:84.2\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.81,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.445,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.89,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.37,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.825,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.72,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.44,Validation Acc:86.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 81 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 9203 -> token two of the\n",
      "Token two of the -> token id 9203\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.01,Validation Acc:71.12\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.855,Validation Acc:81.3\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.95,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.075,Validation Acc:83.96\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.875,Validation Acc:85.06\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.445,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.065,Validation Acc:84.92\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.68,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.355,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.77,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.815,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.65,Validation Acc:85.84\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 82 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 68111 -> token 50 's and\n",
      "Token 50 's and -> token id 68111\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:55.955,Validation Acc:55.76\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:72.975,Validation Acc:71.56\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:83.35,Validation Acc:81.54\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.38,Validation Acc:82.08\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.795,Validation Acc:83.58\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.5,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.755,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.54,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.09,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.72,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.98,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.535,Validation Acc:86.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 83 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 89625 -> token getting some\n",
      "Token getting some -> token id 89625\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.075,Validation Acc:71.54\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:84.05,Validation Acc:81.56\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.36,Validation Acc:83.44\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.24,Validation Acc:84.96\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.78,Validation Acc:83.62\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.585,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.86,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.775,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.595,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.565,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.905,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.45,Validation Acc:86.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 84 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 57557 -> token delivered a\n",
      "Token delivered a -> token id 57557\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/2],Step:[26/157],Training Acc:67.585,Validation Acc:66.48\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.315,Validation Acc:75.3\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.145,Validation Acc:80.5\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.24,Validation Acc:83.14\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.56,Validation Acc:84.76\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.53,Validation Acc:84.92\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.495,Validation Acc:84.6\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.75,Validation Acc:85.14\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.99,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.795,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.84,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.59,Validation Acc:86.56\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 85 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 512941 -> token i recommend it and\n",
      "Token i recommend it and -> token id 512941\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:57.965,Validation Acc:57.92\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:79.415,Validation Acc:77.96\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.415,Validation Acc:81.92\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:83.315,Validation Acc:80.7\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.695,Validation Acc:85.6\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.925,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.635,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.71,Validation Acc:85.98\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.115,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.375,Validation Acc:86.82\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.1,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.945,Validation Acc:86.28\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 86 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 512850 -> token kidnap victim is\n",
      "Token kidnap victim is -> token id 512850\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:56.94,Validation Acc:56.34\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:70.635,Validation Acc:69.56\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:77.765,Validation Acc:76.36\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:82.43,Validation Acc:80.42\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.55,Validation Acc:82.18\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.065,Validation Acc:84.7\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.305,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.84,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.915,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.555,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.705,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.84,Validation Acc:85.54\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 87 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 669864 -> token can understand why they\n",
      "Token can understand why they -> token id 669864\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:68.735,Validation Acc:67.74\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.275,Validation Acc:79.68\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.49,Validation Acc:83.3\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.4,Validation Acc:84.86\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.21,Validation Acc:85.22\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.25,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.775,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.08,Validation Acc:86.44\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.79,Validation Acc:86.56\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.895,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.465,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.155,Validation Acc:86.5\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 88 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 928124 -> token mask looks\n",
      "Token mask looks -> token id 928124\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:63.07,Validation Acc:62.28\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:69.98,Validation Acc:69.42\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:79.465,Validation Acc:78.2\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.455,Validation Acc:81.44\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.565,Validation Acc:83.14\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.935,Validation Acc:83.82\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.59,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.115,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.21,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.465,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.44,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.755,Validation Acc:86.16\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 89 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 531679 -> token when joe bob briggs\n",
      "Token when joe bob briggs -> token id 531679\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:64.845,Validation Acc:63.02\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.27,Validation Acc:80.3\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.89,Validation Acc:82.42\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.065,Validation Acc:84.98\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.335,Validation Acc:85.8\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.24,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.195,Validation Acc:86.48\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.545,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:88.685,Validation Acc:83.44\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.005,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.83,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.46,Validation Acc:86.14\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 90 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 552136 -> token toni collette 's\n",
      "Token toni collette 's -> token id 552136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:53.175,Validation Acc:53.32\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:76.08,Validation Acc:74.8\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.155,Validation Acc:79.88\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.79,Validation Acc:82.84\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.885,Validation Acc:84.56\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.605,Validation Acc:85.14\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.615,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.29,Validation Acc:86.04\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.395,Validation Acc:85.58\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.465,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.08,Validation Acc:86.16\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.365,Validation Acc:85.92\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 91 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 569476 -> token course for me\n",
      "Token course for me -> token id 569476\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.8,Validation Acc:69.0\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.9,Validation Acc:80.18\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.905,Validation Acc:83.48\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.83,Validation Acc:84.64\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.925,Validation Acc:84.8\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.245,Validation Acc:84.44\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.31,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.26,Validation Acc:85.92\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.115,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.67,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.68,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.685,Validation Acc:85.9\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 92 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 428015 -> token dry in\n",
      "Token dry in -> token id 428015\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:51.3,Validation Acc:51.52\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:73.92,Validation Acc:73.24\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:78.005,Validation Acc:76.0\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.545,Validation Acc:81.04\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.33,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.045,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.265,Validation Acc:85.52\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.03,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.68,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.125,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.925,Validation Acc:86.3\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.82,Validation Acc:85.82\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 93 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 478181 -> token out to prove\n",
      "Token out to prove -> token id 478181\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.735,Validation Acc:73.08\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:78.955,Validation Acc:76.98\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.31,Validation Acc:83.96\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.16,Validation Acc:85.14\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.405,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.07,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.875,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.745,Validation Acc:86.46\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.13,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:89.94,Validation Acc:83.7\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.3,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.505,Validation Acc:85.92\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 94 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 584004 -> token and roberts\n",
      "Token and roberts -> token id 584004\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:54.44,Validation Acc:54.32\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.34,Validation Acc:74.1\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:76.85,Validation Acc:75.34\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.07,Validation Acc:83.88\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.695,Validation Acc:85.02\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:89.2,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.925,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.44,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.925,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.315,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.075,Validation Acc:86.1\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.11,Validation Acc:85.32\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 95 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n",
      "Token id 974490 -> token particularly memorable is the\n",
      "Token particularly memorable is the -> token id 974490\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:72.54,Validation Acc:70.84\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.465,Validation Acc:81.38\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.36,Validation Acc:84.3\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.52,Validation Acc:84.86\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.735,Validation Acc:86.12\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.805,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.385,Validation Acc:86.52\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.82,Validation Acc:86.5\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.24,Validation Acc:84.1\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.645,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.165,Validation Acc:86.16\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.51,Validation Acc:85.22\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 96 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 4\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 18935424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 92172 -> token of miike\n",
      "Token of miike -> token id 92172\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.255,Validation Acc:67.04\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:79.53,Validation Acc:77.34\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.915,Validation Acc:81.12\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.775,Validation Acc:81.2\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.105,Validation Acc:83.98\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.27,Validation Acc:84.88\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.04,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.615,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.11,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.545,Validation Acc:86.36\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.19,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.19,Validation Acc:85.98\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 97 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 56098 -> token nastasja\n",
      "Token nastasja -> token id 56098\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:83.975,Validation Acc:82.2\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.11,Validation Acc:84.0\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.38,Validation Acc:84.72\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.23,Validation Acc:84.76\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.11,Validation Acc:85.32\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.745,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.49,Validation Acc:84.16\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.54,Validation Acc:84.42\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.875,Validation Acc:84.02\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:88.83,Validation Acc:83.44\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.575,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.56,Validation Acc:84.7\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 98 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 70641 -> token waimea\n",
      "Token waimea -> token id 70641\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:76.345,Validation Acc:75.0\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.325,Validation Acc:83.66\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.71,Validation Acc:85.56\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.71,Validation Acc:85.32\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:90.145,Validation Acc:85.44\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:91.17,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.785,Validation Acc:84.82\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.395,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.475,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.565,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.955,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.99,Validation Acc:85.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 99 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 71804 -> token smolder\n",
      "Token smolder -> token id 71804\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.47,Validation Acc:82.12\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.365,Validation Acc:84.22\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.465,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.425,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.755,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:87.085,Validation Acc:82.22\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.97,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.095,Validation Acc:84.14\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.905,Validation Acc:84.32\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.18,Validation Acc:84.18\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.345,Validation Acc:84.62\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.345,Validation Acc:84.62\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 100 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 62800 -> token mallik\n",
      "Token mallik -> token id 62800\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:80.885,Validation Acc:79.66\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:85.885,Validation Acc:83.22\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.86,Validation Acc:84.74\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:86.895,Validation Acc:82.64\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.515,Validation Acc:85.04\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.565,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.03,Validation Acc:85.48\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.31,Validation Acc:84.36\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.37,Validation Acc:84.78\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.225,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.765,Validation Acc:85.38\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.465,Validation Acc:84.54\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 101 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 52839 -> token ranyaldo\n",
      "Token ranyaldo -> token id 52839\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:82.585,Validation Acc:80.8\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.065,Validation Acc:84.08\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:78.54,Validation Acc:75.86\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.075,Validation Acc:85.1\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.815,Validation Acc:85.58\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:86.435,Validation Acc:81.7\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.235,Validation Acc:84.86\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.59,Validation Acc:84.72\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.185,Validation Acc:84.42\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:89.145,Validation Acc:84.26\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:88.515,Validation Acc:82.44\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.24,Validation Acc:84.04\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 102 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 58996 -> token molteni\n",
      "Token molteni -> token id 58996\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.775,Validation Acc:82.76\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.58,Validation Acc:82.72\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.655,Validation Acc:83.98\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.52,Validation Acc:85.32\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.115,Validation Acc:84.58\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.555,Validation Acc:85.48\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.99,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.81,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.19,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.235,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.175,Validation Acc:84.48\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.13,Validation Acc:84.24\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 103 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 47852 -> token zay\n",
      "Token zay -> token id 47852\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:83.715,Validation Acc:81.38\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.29,Validation Acc:82.68\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.13,Validation Acc:84.22\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.86,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.295,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.445,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.195,Validation Acc:84.86\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.2,Validation Acc:84.06\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.75,Validation Acc:84.14\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:86.93,Validation Acc:81.02\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:86.46,Validation Acc:81.5\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:88.7,Validation Acc:83.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 104 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 43073 -> token bodacious\n",
      "Token bodacious -> token id 43073\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.13,Validation Acc:82.34\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:81.835,Validation Acc:78.34\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:87.82,Validation Acc:83.92\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.56,Validation Acc:85.58\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.64,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.565,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.735,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.005,Validation Acc:84.96\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.96,Validation Acc:84.42\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.645,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.29,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.33,Validation Acc:85.06\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 105 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 73452 -> token interlocking\n",
      "Token interlocking -> token id 73452\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:82.76,Validation Acc:80.44\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.215,Validation Acc:83.5\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:87.93,Validation Acc:84.54\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.635,Validation Acc:82.84\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.175,Validation Acc:83.98\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.245,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:85.795,Validation Acc:80.68\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.5,Validation Acc:83.3\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:86.025,Validation Acc:81.98\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:87.945,Validation Acc:82.94\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:88.675,Validation Acc:82.66\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:86.09,Validation Acc:81.58\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 106 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 61317 -> token underscoring\n",
      "Token underscoring -> token id 61317\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:82.775,Validation Acc:80.24\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:84.705,Validation Acc:81.22\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.095,Validation Acc:83.88\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:88.815,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.845,Validation Acc:85.18\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.32,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.035,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.755,Validation Acc:82.54\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.775,Validation Acc:84.12\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:87.26,Validation Acc:82.2\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.11,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.715,Validation Acc:84.82\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 107 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 67701 -> token vilyenkov\n",
      "Token vilyenkov -> token id 67701\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.6,Validation Acc:72.44\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.775,Validation Acc:84.24\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:83.045,Validation Acc:79.24\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.51,Validation Acc:84.04\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.85,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.725,Validation Acc:85.14\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:86.66,Validation Acc:81.88\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:78.0,Validation Acc:75.06\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.715,Validation Acc:84.1\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.4,Validation Acc:84.48\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:88.555,Validation Acc:83.04\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.165,Validation Acc:84.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 108 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 78982 -> token outlooking\n",
      "Token outlooking -> token id 78982\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.955,Validation Acc:82.94\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.255,Validation Acc:83.84\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:87.8,Validation Acc:84.0\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:88.33,Validation Acc:84.12\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:90.3,Validation Acc:85.56\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.155,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.475,Validation Acc:83.62\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.565,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.515,Validation Acc:83.66\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.545,Validation Acc:83.96\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.945,Validation Acc:84.88\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.815,Validation Acc:84.94\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 109 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 53741 -> token p;ace\n",
      "Token p;ace -> token id 53741\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:79.57,Validation Acc:77.24\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.98,Validation Acc:84.08\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:87.735,Validation Acc:83.44\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.135,Validation Acc:84.7\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.09,Validation Acc:84.6\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.695,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.445,Validation Acc:83.96\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:81.415,Validation Acc:77.1\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.75,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.845,Validation Acc:84.5\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.345,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.085,Validation Acc:83.94\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 110 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 68921 -> token halloween\"s\n",
      "Token halloween\"s -> token id 68921\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:76.32,Validation Acc:74.66\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:84.11,Validation Acc:81.06\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.68,Validation Acc:85.1\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.495,Validation Acc:85.44\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:90.12,Validation Acc:84.94\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.41,Validation Acc:85.4\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.03,Validation Acc:85.42\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.025,Validation Acc:84.94\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.34,Validation Acc:83.6\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.1,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.0,Validation Acc:84.78\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.485,Validation Acc:85.38\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 111 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 9003 -> token diving\n",
      "Token diving -> token id 9003\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:76.615,Validation Acc:73.42\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.415,Validation Acc:83.62\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:87.41,Validation Acc:82.6\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.67,Validation Acc:84.08\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:87.11,Validation Acc:82.7\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.3,Validation Acc:83.54\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.385,Validation Acc:84.76\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.29,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:88.465,Validation Acc:82.96\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:89.61,Validation Acc:83.72\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.875,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.99,Validation Acc:84.58\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 112 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 71988 -> token p'z\n",
      "Token p'z -> token id 71988\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:82.135,Validation Acc:80.0\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:84.415,Validation Acc:80.88\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.1,Validation Acc:84.5\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.605,Validation Acc:85.18\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:88.645,Validation Acc:84.86\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.585,Validation Acc:85.76\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.075,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.21,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.14,Validation Acc:84.54\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.555,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.38,Validation Acc:83.56\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.145,Validation Acc:85.34\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 113 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 69337 -> token afterschool\n",
      "Token afterschool -> token id 69337\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:80.57,Validation Acc:78.56\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.3,Validation Acc:84.42\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.495,Validation Acc:84.78\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:85.32,Validation Acc:81.54\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:86.895,Validation Acc:82.78\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:85.675,Validation Acc:81.64\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.115,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.17,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.455,Validation Acc:84.78\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:86.04,Validation Acc:81.02\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.225,Validation Acc:84.04\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.71,Validation Acc:84.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 114 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 59348 -> token maize\n",
      "Token maize -> token id 59348\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:83.72,Validation Acc:81.26\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.08,Validation Acc:82.32\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.58,Validation Acc:84.16\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.035,Validation Acc:84.54\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:90.385,Validation Acc:86.44\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.625,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.705,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.39,Validation Acc:85.08\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:88.595,Validation Acc:83.26\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.315,Validation Acc:85.04\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.175,Validation Acc:83.76\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.735,Validation Acc:84.86\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 115 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 66165 -> token fw\n",
      "Token fw -> token id 66165\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.57,Validation Acc:82.68\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.425,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:80.43,Validation Acc:77.56\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.295,Validation Acc:85.18\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.5,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:88.915,Validation Acc:84.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.54,Validation Acc:85.38\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.355,Validation Acc:84.32\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.645,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:84.54,Validation Acc:80.02\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.225,Validation Acc:83.52\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:88.985,Validation Acc:82.48\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 116 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 12338 -> token camerawork\n",
      "Token camerawork -> token id 12338\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:83.805,Validation Acc:81.82\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.075,Validation Acc:84.4\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.38,Validation Acc:84.8\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.44,Validation Acc:85.86\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.355,Validation Acc:85.38\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.58,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.17,Validation Acc:84.18\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.235,Validation Acc:85.42\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.92,Validation Acc:84.82\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.145,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:89.645,Validation Acc:83.74\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.67,Validation Acc:85.04\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 117 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 67463 -> token people-\"psst\n",
      "Token people-\"psst -> token id 67463\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.8,Validation Acc:74.1\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.64,Validation Acc:83.26\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.095,Validation Acc:84.28\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.36,Validation Acc:85.08\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.875,Validation Acc:85.52\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.995,Validation Acc:84.78\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:88.645,Validation Acc:83.92\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.195,Validation Acc:83.6\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.005,Validation Acc:84.1\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:88.9,Validation Acc:83.8\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:86.635,Validation Acc:81.58\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:86.925,Validation Acc:80.94\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 118 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 27752 -> token murdstone\n",
      "Token murdstone -> token id 27752\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.105,Validation Acc:83.22\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.895,Validation Acc:85.22\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.755,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.54,Validation Acc:85.1\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.85,Validation Acc:85.52\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.63,Validation Acc:86.18\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.835,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.14,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.965,Validation Acc:84.02\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:91.38,Validation Acc:85.44\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:89.935,Validation Acc:83.58\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.86,Validation Acc:85.24\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 119 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 11781 -> token edinburgh\n",
      "Token edinburgh -> token id 11781\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:83.165,Validation Acc:81.12\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.26,Validation Acc:83.6\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.51,Validation Acc:80.66\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.255,Validation Acc:83.34\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.53,Validation Acc:84.52\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.105,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.49,Validation Acc:84.76\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.4,Validation Acc:83.06\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:88.095,Validation Acc:82.62\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:89.09,Validation Acc:82.86\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:89.015,Validation Acc:83.44\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:68.595,Validation Acc:65.54\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 120 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 637 -> token middle\n",
      "Token middle -> token id 637\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.3,Validation Acc:82.7\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:81.84,Validation Acc:80.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.685,Validation Acc:85.52\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.66,Validation Acc:85.52\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:90.5,Validation Acc:86.16\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.055,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:90.805,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:90.34,Validation Acc:85.28\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.12,Validation Acc:83.62\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.745,Validation Acc:84.3\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.265,Validation Acc:84.6\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:88.565,Validation Acc:83.18\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 121 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 39619 -> token and returns\n",
      "Token and returns -> token id 39619\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:82.255,Validation Acc:79.98\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.005,Validation Acc:82.28\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.465,Validation Acc:84.04\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.605,Validation Acc:84.12\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.08,Validation Acc:85.04\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:91.78,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.2,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.55,Validation Acc:83.22\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.345,Validation Acc:84.62\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:93.065,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.695,Validation Acc:84.26\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.87,Validation Acc:85.62\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 122 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 29259 -> token n't hurt\n",
      "Token n't hurt -> token id 29259\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:77.5,Validation Acc:75.16\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.095,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.765,Validation Acc:85.0\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.945,Validation Acc:85.62\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.405,Validation Acc:86.2\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:91.38,Validation Acc:85.18\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:93.075,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.495,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:92.585,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.33,Validation Acc:84.98\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:93.89,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.125,Validation Acc:85.06\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 123 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 77395 -> token skye\n",
      "Token skye -> token id 77395\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.755,Validation Acc:82.54\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.595,Validation Acc:83.7\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.995,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:90.21,Validation Acc:84.62\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.68,Validation Acc:85.64\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.88,Validation Acc:85.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.56,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:92.325,Validation Acc:84.76\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.11,Validation Acc:82.56\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:93.445,Validation Acc:84.96\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.04,Validation Acc:84.16\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.01,Validation Acc:84.52\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 124 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 19803 -> token there seems\n",
      "Token there seems -> token id 19803\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.18,Validation Acc:81.86\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:84.94,Validation Acc:81.3\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:84.945,Validation Acc:80.38\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.4,Validation Acc:85.84\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.575,Validation Acc:86.02\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:91.645,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.825,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.275,Validation Acc:84.5\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:93.77,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:92.17,Validation Acc:84.12\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.37,Validation Acc:83.76\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:92.765,Validation Acc:84.34\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 125 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 37337 -> token a cowboy\n",
      "Token a cowboy -> token id 37337\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.8,Validation Acc:82.1\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.99,Validation Acc:84.4\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:89.975,Validation Acc:84.9\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:91.135,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.935,Validation Acc:85.12\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:92.485,Validation Acc:85.9\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.08,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.315,Validation Acc:83.38\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.2,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.4,Validation Acc:83.84\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.71,Validation Acc:84.08\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.52,Validation Acc:82.8\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 126 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 83572 -> token dey young\n",
      "Token dey young -> token id 83572\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:79.815,Validation Acc:77.26\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.72,Validation Acc:83.74\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.295,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.61,Validation Acc:84.98\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.855,Validation Acc:85.7\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:92.3,Validation Acc:86.16\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.715,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.525,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:92.46,Validation Acc:84.5\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.15,Validation Acc:85.14\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.205,Validation Acc:84.02\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.25,Validation Acc:85.44\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 127 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 2426 -> token find a\n",
      "Token find a -> token id 2426\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:75.475,Validation Acc:73.0\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.925,Validation Acc:84.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.51,Validation Acc:83.78\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.61,Validation Acc:84.58\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.275,Validation Acc:85.62\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.98,Validation Acc:83.68\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.295,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:92.15,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.82,Validation Acc:83.52\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.66,Validation Acc:84.02\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.51,Validation Acc:83.56\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.605,Validation Acc:85.08\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 128 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 12234 -> token two things\n",
      "Token two things -> token id 12234\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:81.19,Validation Acc:78.74\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:85.07,Validation Acc:82.72\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:88.825,Validation Acc:84.5\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.745,Validation Acc:85.5\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.655,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:91.63,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:93.03,Validation Acc:85.88\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:93.105,Validation Acc:85.84\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.785,Validation Acc:83.1\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.02,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:93.89,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.52,Validation Acc:85.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 129 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 48743 -> token walking and\n",
      "Token walking and -> token id 48743\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:81.695,Validation Acc:79.6\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.235,Validation Acc:83.96\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:79.405,Validation Acc:75.48\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.69,Validation Acc:84.64\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.17,Validation Acc:85.76\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:91.775,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.3,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.81,Validation Acc:84.62\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.18,Validation Acc:82.32\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.08,Validation Acc:83.54\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.05,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.895,Validation Acc:83.08\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 130 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 16197 -> token and mr.\n",
      "Token and mr. -> token id 16197\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:83.84,Validation Acc:81.5\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:86.595,Validation Acc:83.08\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.435,Validation Acc:85.1\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.24,Validation Acc:84.26\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.365,Validation Acc:85.56\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.28,Validation Acc:84.18\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.68,Validation Acc:84.48\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:91.96,Validation Acc:84.92\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:93.05,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:92.655,Validation Acc:84.66\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:88.5,Validation Acc:81.38\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:93.64,Validation Acc:84.74\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 131 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 71369 -> token different for\n",
      "Token different for -> token id 71369\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.01,Validation Acc:81.66\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.785,Validation Acc:84.08\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.625,Validation Acc:83.84\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:90.545,Validation Acc:85.9\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.725,Validation Acc:85.34\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.865,Validation Acc:84.14\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.195,Validation Acc:84.38\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.53,Validation Acc:84.16\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:88.1,Validation Acc:81.66\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:92.34,Validation Acc:84.6\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.07,Validation Acc:85.06\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:92.33,Validation Acc:84.52\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 132 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 35516 -> token about getting\n",
      "Token about getting -> token id 35516\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:79.375,Validation Acc:77.64\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.49,Validation Acc:84.94\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.405,Validation Acc:85.92\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.68,Validation Acc:85.22\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.41,Validation Acc:85.36\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:92.315,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.675,Validation Acc:85.48\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.0,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:93.105,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.505,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:93.77,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.315,Validation Acc:84.94\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 133 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 507125 -> token exuded a\n",
      "Token exuded a -> token id 507125\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:83.115,Validation Acc:80.72\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.51,Validation Acc:83.64\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.86,Validation Acc:84.24\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:90.68,Validation Acc:85.3\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.24,Validation Acc:85.82\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:92.285,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.745,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.86,Validation Acc:84.56\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.9,Validation Acc:85.08\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:93.17,Validation Acc:84.96\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.915,Validation Acc:83.7\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:94.01,Validation Acc:85.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 134 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 336807 -> token 27-part\n",
      "Token 27-part -> token id 336807\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:82.82,Validation Acc:80.4\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.315,Validation Acc:83.68\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:87.77,Validation Acc:83.76\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.04,Validation Acc:83.92\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.19,Validation Acc:84.9\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:91.9,Validation Acc:86.02\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:91.93,Validation Acc:84.78\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:93.225,Validation Acc:85.62\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:91.64,Validation Acc:83.92\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.855,Validation Acc:85.26\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:92.705,Validation Acc:83.68\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:89.09,Validation Acc:81.48\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 135 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 863967 -> token ducks nails\n",
      "Token ducks nails -> token id 863967\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.79,Validation Acc:82.54\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.55,Validation Acc:83.82\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.74,Validation Acc:81.22\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.685,Validation Acc:84.22\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:91.32,Validation Acc:85.68\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:91.12,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.8,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:91.84,Validation Acc:84.64\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:93.055,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:93.175,Validation Acc:84.68\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.365,Validation Acc:84.58\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:88.26,Validation Acc:80.88\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 136 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 480792 -> token ballads is\n",
      "Token ballads is -> token id 480792\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:83.885,Validation Acc:82.76\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:85.04,Validation Acc:81.72\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:84.005,Validation Acc:79.7\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.24,Validation Acc:83.86\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:89.265,Validation Acc:83.64\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:92.45,Validation Acc:86.14\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.77,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.545,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:92.62,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.535,Validation Acc:85.26\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:94.285,Validation Acc:85.22\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.685,Validation Acc:85.5\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 137 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 8842 -> token pieces of\n",
      "Token pieces of -> token id 8842\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:84.75,Validation Acc:82.16\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:87.215,Validation Acc:83.18\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:87.07,Validation Acc:82.42\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.07,Validation Acc:84.24\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.61,Validation Acc:84.16\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.5,Validation Acc:85.02\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.88,Validation Acc:84.8\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:87.74,Validation Acc:81.26\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.925,Validation Acc:85.1\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:93.385,Validation Acc:84.96\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:92.415,Validation Acc:84.36\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:93.04,Validation Acc:85.3\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 138 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 271868 -> token dead folks\n",
      "Token dead folks -> token id 271868\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:82.4,Validation Acc:80.52\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.02,Validation Acc:83.3\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.08,Validation Acc:84.08\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:90.41,Validation Acc:85.14\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.68,Validation Acc:85.96\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:90.68,Validation Acc:84.72\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.31,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.63,Validation Acc:84.9\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:92.47,Validation Acc:84.42\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.74,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:93.295,Validation Acc:84.5\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.165,Validation Acc:85.48\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 139 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 189991 -> token the empress\n",
      "Token the empress -> token id 189991\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:81.545,Validation Acc:79.5\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.5,Validation Acc:83.02\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:88.985,Validation Acc:84.44\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:89.68,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.6,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:91.31,Validation Acc:85.08\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:92.53,Validation Acc:85.7\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.92,Validation Acc:83.42\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:92.565,Validation Acc:84.5\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:86.795,Validation Acc:79.74\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:93.135,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.135,Validation Acc:82.48\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 140 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 973341 -> token lab work\n",
      "Token lab work -> token id 973341\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:84.485,Validation Acc:83.14\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:87.71,Validation Acc:84.56\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:89.735,Validation Acc:84.94\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:89.315,Validation Acc:84.02\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:91.38,Validation Acc:85.68\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:92.525,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:92.295,Validation Acc:85.04\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:92.645,Validation Acc:85.04\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:92.775,Validation Acc:84.74\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:93.105,Validation Acc:84.62\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:91.67,Validation Acc:83.42\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:94.62,Validation Acc:84.98\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 141 / 480\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9507712\n",
      "Token id 73191 -> token and fire\n",
      "Token and fire -> token id 73191\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:85.215,Validation Acc:82.42\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:86.685,Validation Acc:83.0\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:89.04,Validation Acc:84.52\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:90.325,Validation Acc:85.18\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:90.85,Validation Acc:85.52\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:90.7,Validation Acc:84.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:91.26,Validation Acc:84.22\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:92.19,Validation Acc:84.84\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:86.475,Validation Acc:79.78\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.635,Validation Acc:84.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-57e3d58b96bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m param_val_losses_adam = hyperparameter_search(hyperparameter_space = params,\n\u001b[1;32m      2\u001b[0m                                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                          optimizer_name = \"Adam\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-319-d48f874943c2>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(hyperparameter_space, epochs, optimizer_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;31m# Adjust it to accustom changing batch sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_val_losses_adam = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"Adam\",\n",
    "                                          lemmatize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_val_losses_sgd = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 10,\n",
    "                                         optimizer_name = \"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 1, 100000.0, 100, 100, 64) [82.26, 83.48, 84.34, 85.2, 83.78, 85.28, 84.88, 82.8, 84.84, 84.4, 83.16, 84.8]\n",
      "(0.1, 1, 100000.0, 100, 100, 128) [83.24, 78.62, 84.92, 85.38, 85.7, 85.4, 85.28, 84.44, 85.2, 84.86, 85.4, 85.96]\n",
      "(1, 1, 100000.0, 100, 100, 64) [78.52, 79.34, 64.98, 82.32, 79.98, 79.36, 75.4, 79.26, 80.8, 75.32, 80.42, 81.56]\n",
      "(1, 1, 100000.0, 100, 100, 128) [51.9, 52.48, 75.2, 82.9, 75.08, 73.48, 84.02, 84.46, 80.56, 83.08, 83.2, 80.08]\n",
      "(2, 1, 100000.0, 100, 100, 64) [78.58, 52.7, 79.3, 77.6, 69.44, 78.26, 81.92, 81.8, 73.88, 74.9, 79.58, 79.26]\n",
      "(2, 1, 100000.0, 100, 100, 128) [68.06, 67.5, 80.56, 68.62, 82.84, 83.54, 80.02, 82.04, 82.1, 81.46, 83.42, 83.66]\n",
      "(5, 1, 100000.0, 100, 100, 64) [81.5, 76.28, 70.52, 56.0, 74.78, 72.3, 71.08, 79.52, 82.86, 81.08, 74.76, 77.86]\n",
      "(5, 1, 100000.0, 100, 100, 128) [77.3, 71.52, 82.48, 81.4, 75.44, 82.62, 78.06, 79.76, 81.08, 83.5, 74.34, 81.64]\n"
     ]
    }
   ],
   "source": [
    "for key, value in param_val_losses_adam.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in param_val_losses_sgd.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
