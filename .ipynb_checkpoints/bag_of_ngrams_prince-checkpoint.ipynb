{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import spacy\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"data/imdb_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "    file_list = os.listdir(folder_path)\n",
    "    # for debugging, printing out the folder path and some files in it\n",
    "    print(folder_path)\n",
    "    print(file_list[:10])\n",
    "    \n",
    "    all_reviews = []\n",
    "    for file_path in file_list:\n",
    "        f = open(folder_path + file_path,\"r\")\n",
    "        all_reviews.append(f.readline())\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_reviews/train/pos/\n",
      "['4715_9.txt', '12390_8.txt', '8329_7.txt', '9063_8.txt', '3092_10.txt', '9865_8.txt', '6639_10.txt', '10460_10.txt', '10331_10.txt', '11606_10.txt']\n",
      "12500\n",
      "data/imdb_reviews/train/neg/\n",
      "['1821_4.txt', '10402_1.txt', '1062_4.txt', '9056_1.txt', '5392_3.txt', '2682_3.txt', '3351_4.txt', '399_2.txt', '10447_1.txt', '10096_1.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/pos/\n",
      "['4715_9.txt', '1930_9.txt', '3205_9.txt', '10186_10.txt', '147_10.txt', '7511_7.txt', '616_10.txt', '10460_10.txt', '3240_9.txt', '1975_9.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/neg/\n",
      "['1821_4.txt', '9487_1.txt', '4604_4.txt', '2828_2.txt', '10890_1.txt', '3351_4.txt', '8070_2.txt', '1027_4.txt', '8248_3.txt', '4290_4.txt']\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "print(len(train_pos))\n",
    "train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "print(len(train_neg))\n",
    "test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "print(len(test_pos))\n",
    "test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive examples = 12500\n",
      "Train Negative examples = 12500\n",
      "Test Positive examples = 12500\n",
      "Test Negative examples = 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[1 1 1 ... 0 0 0]\n",
      "25000\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "train_pos_labels\n",
    "\n",
    "train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "train_neg_labels\n",
    "\n",
    "train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "print(len(train_data_labels))\n",
    "print(train_data_labels)\n",
    "\n",
    "test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "print(len(test_data_labels))\n",
    "print(test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "test_neg_clean = [cleanhtml(x) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_clean = train_pos_clean + train_neg_clean\n",
    "len(train_all_clean)\n",
    "\n",
    "test_all_clean = test_pos_clean + test_neg_clean\n",
    "len(test_all_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[16668  5151 19604 ...  5002 16380  2885]\n",
      "20000\n",
      "20000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "training_size = 20000\n",
    "\n",
    "assert training_size < 25000\n",
    "\n",
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)\n",
    "\n",
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))\n",
    "\n",
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# This is word tokenizer\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# Modified for n-grams\n",
    "def tokenize(sent, n_gram = 0, lemmatize = False):\n",
    "    \n",
    "    tokens = tokenizer(sent)\n",
    "    \n",
    "    # unigrams\n",
    "    if lemmatize == False:\n",
    "        unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        #LEMMATIZED\n",
    "        unigrams = [token.lemma_.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    \n",
    "    \n",
    "    output = []\n",
    "    output.extend(unigrams)\n",
    "    \n",
    "    n = 2\n",
    "    while n <= n_gram:\n",
    "        ngram_tokens = [\" \".join(unigrams[x:x+n]) for x in range(len(unigrams)-n+1)]\n",
    "        output.extend(ngram_tokens)\n",
    "        n = n + 1\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, n_gram, lemmatize = True):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], \n",
    "#                                                batch_size=512, \n",
    "#                                                n_threads=4)):\n",
    "\n",
    "    itr = 0\n",
    "    for sample in dataset:\n",
    "        \n",
    "        if itr % 50 == 0:\n",
    "            print(str(itr) + \" / \" + str(len(dataset)))\n",
    "        # unigram version\n",
    "        #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        # n-gram version\n",
    "        tokens = tokenize(sample,n_gram, lemmatize = lemmatize)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "        itr = itr + 1\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = [1,2,3]\n",
    "lemmatize_list = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "1_lemma\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "2\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "2_lemma\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "3\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "3_lemma\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "1\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "1\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "2\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "2\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n",
      "3\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n",
      "3\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "for lemmatize_arg in lemmatize_list:\n",
    "    for gram_no in grams:\n",
    "        print(str(gram_no))\n",
    "\n",
    "        train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                               n_gram=gram_no, \n",
    "                                                               lemmatize = lemmatize_arg)\n",
    "\n",
    "        # Tokenize Validation\n",
    "        val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                              n_gram=gram_no, \n",
    "                                              lemmatize = lemmatize_arg)\n",
    "\n",
    "        if lemmatize_arg == True:\n",
    "            gram_no = str(gram_no) + \"_lemma\"\n",
    "        else:\n",
    "            gram_no = str(gram_no)\n",
    "        print(gram_no)\n",
    "\n",
    "        # val set tokens\n",
    "        print (\"Tokenizing val data\")\n",
    "        pkl.dump(val_data_tokens, open(\"val_data_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n",
    "\n",
    "        # train set tokens\n",
    "        print (\"Tokenizing train data\")\n",
    "        pkl.dump(train_data_tokens, open(\"train_data_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n",
    "        pkl.dump(all_train_tokens, open(\"all_train_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.01, 1, 100000.0, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 128),\n",
       " (1, 1, 100000.0, 100, 64),\n",
       " (1, 1, 100000.0, 100, 128),\n",
       " (1, 1, 100000.0, 200, 64),\n",
       " (1, 1, 100000.0, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 128),\n",
       " (1, 2, 100000.0, 100, 64),\n",
       " (1, 2, 100000.0, 100, 128),\n",
       " (1, 2, 100000.0, 200, 64),\n",
       " (1, 2, 100000.0, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 128),\n",
       " (1, 3, 100000.0, 100, 64),\n",
       " (1, 3, 100000.0, 100, 128),\n",
       " (1, 3, 100000.0, 200, 64),\n",
       " (1, 3, 100000.0, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 128),\n",
       " (2, 1, 100000.0, 100, 64),\n",
       " (2, 1, 100000.0, 100, 128),\n",
       " (2, 1, 100000.0, 200, 64),\n",
       " (2, 1, 100000.0, 200, 128),\n",
       " (2, 1, 1000000.0, 100, 64),\n",
       " (2, 1, 1000000.0, 100, 128),\n",
       " (2, 1, 1000000.0, 200, 64),\n",
       " (2, 1, 1000000.0, 200, 128),\n",
       " (2, 2, 100000.0, 100, 64),\n",
       " (2, 2, 100000.0, 100, 128),\n",
       " (2, 2, 100000.0, 200, 64),\n",
       " (2, 2, 100000.0, 200, 128),\n",
       " (2, 2, 1000000.0, 100, 64),\n",
       " (2, 2, 1000000.0, 100, 128),\n",
       " (2, 2, 1000000.0, 200, 64),\n",
       " (2, 2, 1000000.0, 200, 128),\n",
       " (2, 3, 100000.0, 100, 64),\n",
       " (2, 3, 100000.0, 100, 128),\n",
       " (2, 3, 100000.0, 200, 64),\n",
       " (2, 3, 100000.0, 200, 128),\n",
       " (2, 3, 1000000.0, 100, 64),\n",
       " (2, 3, 1000000.0, 100, 128),\n",
       " (2, 3, 1000000.0, 200, 64),\n",
       " (2, 3, 1000000.0, 200, 128)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [[1e-2,1e-1,1,2], ## learning rates\n",
    "          list(range(1,4)), ## ngrams\n",
    "          [1e5,1e6], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "#          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "# params = [[1e-1,1,2,5], ## learning rates\n",
    "#           list(range(1,2)), ## ngrams\n",
    "#           [1e5], ## vocab size\n",
    "#           [100], ## embedding size\n",
    "#           [100], ## max sentence length\n",
    "#           [64] ## batch size\n",
    "#          ]\n",
    "\n",
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params,\n",
    "                          epochs=5,\n",
    "                          optimizer_name = \"Adam\",\n",
    "                          lemmatize = False):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    # counter for progress\n",
    "    count = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Parameter Combination = \" + str(count+1) + \" / \" + str(len(param_space)))\n",
    "        count = count + 1      \n",
    "        \n",
    "        NUM_EPOCHS = epochs\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = int(param_comb[2]) # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        #max_sentence_length = int(param_comb[4]) # max sentence length of data loader\n",
    "        BATCH_SIZE = param_comb[4]\n",
    "        \n",
    "        print(\"Learning Rate = \" + str(lr_rate))\n",
    "        print(\"Ngram = \" + str(grams))\n",
    "        print(\"Vocab Size = \" + str(max_vocab_size))\n",
    "        print(\"Embedding Dimension = \" + str(embed_dimension))\n",
    "        #print(\"Max Sentence Length = \" + str(max_sentence_length))\n",
    "        print(\"Batch Size = \" + str(BATCH_SIZE))\n",
    "\n",
    "        # Tokenization\n",
    "        # All tokens are created before the hyperparameter search loop\n",
    "        # Load the tokens here\n",
    "        if lemmatize == True:\n",
    "            grams = \"lemma_\" + str(grams)\n",
    "        \n",
    "        train_data_tokens = pkl.load(open(\"train_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        all_train_tokens = pkl.load(open(\"all_train_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "\n",
    "        val_data_tokens = pkl.load(open(\"val_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        \n",
    "        print(\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "        print(\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "        print(\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "        \n",
    "        # Building Vocabulary\n",
    "        # implicitly gets the max_vocab_size parameter\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                         max_vocab_size=max_vocab_size)\n",
    "        \n",
    "        # Lets check the dictionary by loading random token from it\n",
    "        random_token_id = random.randint(0, len(id2token)-1)\n",
    "        random_token = id2token[random_token_id]\n",
    "        print (\"Token id {} -> token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "        print (\"Token {} -> token id {}\".format(random_token, token2id[random_token]))\n",
    "        \n",
    "        train_data_indices = token2index_dataset(train_data_tokens, \n",
    "                                                 token2id = token2id)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens, \n",
    "                                               token2id = token2id)\n",
    "        # double checking\n",
    "        print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Load training and validation data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)  \n",
    "\n",
    "        # Initialize the N-gram Model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "        \n",
    "        # Both Adam and SGD will be tried\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "        else:\n",
    "            print(\"this optimizer is not implemented yet\")\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        # Caution: Two different optimizers\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "    #for optimizer in optimizers:\n",
    "        print(\"Optimization Start\")\n",
    "        print(optimizer)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Validate every 100 iterations\n",
    "                # Adjust it to accustom changing batch sizes\n",
    "                if i > 0 and i % (50 * (64 / BATCH_SIZE)) == 0:\n",
    "\n",
    "                    # Accuracy Calculations\n",
    "                    train_acc = test_model(train_loader, model)\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                    # Logging\n",
    "                    print('Epoch:[{}/{}],Step:[{}/{}],Training Acc:{},Validation Acc:{}'.format( \n",
    "                               epoch+1, NUM_EPOCHS, \n",
    "                                i+1, len(train_loader), \n",
    "                                train_acc, val_acc))\n",
    "                      \n",
    "    return val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 15430 -> token sandwich\n",
      "Token sandwich -> token id 15430\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:74.225,Validation Acc:72.4\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:83.86,Validation Acc:81.1\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:84.935,Validation Acc:82.1\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:88.36,Validation Acc:84.12\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:92.58,Validation Acc:86.28\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:93.81,Validation Acc:86.84\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:94.72,Validation Acc:86.94\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:95.385,Validation Acc:87.28\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:95.96,Validation Acc:87.12\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:96.495,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.395,Validation Acc:87.48\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:97.995,Validation Acc:87.18\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.545,Validation Acc:87.22\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.61,Validation Acc:87.36\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.745,Validation Acc:87.06\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.935,Validation Acc:87.0\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.135,Validation Acc:86.68\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.275,Validation Acc:86.16\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.665,Validation Acc:86.7\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.695,Validation Acc:86.88\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.73,Validation Acc:86.56\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.75,Validation Acc:86.24\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.745,Validation Acc:86.42\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.9,Validation Acc:86.24\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.935,Validation Acc:86.46\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.91,Validation Acc:85.96\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.965,Validation Acc:86.12\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.98,Validation Acc:86.1\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.96,Validation Acc:86.3\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.985,Validation Acc:86.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 2 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 22195 -> token pere\n",
      "Token pere -> token id 22195\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:65.24,Validation Acc:63.94\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:74.765,Validation Acc:72.38\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:81.305,Validation Acc:78.38\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:86.855,Validation Acc:82.94\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:89.42,Validation Acc:84.78\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:91.105,Validation Acc:85.68\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:92.38,Validation Acc:86.64\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:93.255,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:94.28,Validation Acc:86.98\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:95.155,Validation Acc:86.98\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:96.0,Validation Acc:87.28\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:96.955,Validation Acc:87.52\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:97.365,Validation Acc:87.22\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:97.635,Validation Acc:87.96\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:97.715,Validation Acc:87.66\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.25,Validation Acc:87.52\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:98.545,Validation Acc:87.4\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.01,Validation Acc:87.56\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.115,Validation Acc:87.46\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.195,Validation Acc:87.32\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.245,Validation Acc:87.34\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.395,Validation Acc:87.22\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.58,Validation Acc:87.02\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.725,Validation Acc:87.02\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.765,Validation Acc:87.14\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.81,Validation Acc:87.18\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.82,Validation Acc:87.02\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.83,Validation Acc:87.06\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.915,Validation Acc:86.96\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.92,Validation Acc:86.7\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 3 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 75998 -> token stinkpile\n",
      "Token stinkpile -> token id 75998\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:77.6,Validation Acc:75.94\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:82.405,Validation Acc:79.24\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:89.235,Validation Acc:85.34\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.03,Validation Acc:85.54\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:92.885,Validation Acc:86.58\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:94.705,Validation Acc:87.48\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:95.515,Validation Acc:87.52\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.155,Validation Acc:87.74\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.55,Validation Acc:87.84\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.065,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.8,Validation Acc:87.38\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.615,Validation Acc:86.92\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.85,Validation Acc:86.78\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.82,Validation Acc:86.76\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.245,Validation Acc:87.02\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.35,Validation Acc:86.72\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.21,Validation Acc:85.86\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.53,Validation Acc:86.9\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.755,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.79,Validation Acc:86.74\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.755,Validation Acc:86.5\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.835,Validation Acc:86.44\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.89,Validation Acc:86.22\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.945,Validation Acc:86.08\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.945,Validation Acc:86.04\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.96,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.965,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.97,Validation Acc:86.54\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.98,Validation Acc:86.12\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.34\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 4 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 55138 -> token job!as\n",
      "Token job!as -> token id 55138\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5],Step:[26/157],Training Acc:67.76,Validation Acc:66.32\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:78.605,Validation Acc:76.16\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:85.435,Validation Acc:82.56\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:88.655,Validation Acc:83.98\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:89.305,Validation Acc:83.74\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:92.62,Validation Acc:86.1\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:93.885,Validation Acc:86.44\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:94.565,Validation Acc:86.58\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:95.095,Validation Acc:86.66\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:95.98,Validation Acc:86.86\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:96.73,Validation Acc:87.32\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.73,Validation Acc:87.5\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.05,Validation Acc:87.54\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.125,Validation Acc:87.38\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.5,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.73,Validation Acc:87.02\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.14,Validation Acc:87.02\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.245,Validation Acc:86.7\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.47,Validation Acc:86.68\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.55,Validation Acc:87.1\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.59,Validation Acc:86.82\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.71,Validation Acc:86.84\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.695,Validation Acc:86.44\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.875,Validation Acc:86.2\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.915,Validation Acc:86.18\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.94,Validation Acc:86.42\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.94,Validation Acc:86.54\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.95,Validation Acc:86.32\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.975,Validation Acc:86.14\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.99,Validation Acc:86.24\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 5 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 41561 -> token dumbstruck\n",
      "Token dumbstruck -> token id 41561\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:67.965,Validation Acc:64.98\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:83.275,Validation Acc:80.64\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:86.82,Validation Acc:84.08\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:90.41,Validation Acc:85.4\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:91.42,Validation Acc:85.52\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:93.295,Validation Acc:86.22\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:94.605,Validation Acc:86.76\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:95.58,Validation Acc:86.86\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.09,Validation Acc:86.78\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:96.83,Validation Acc:87.2\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.4,Validation Acc:87.04\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.005,Validation Acc:86.86\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.4,Validation Acc:86.78\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.56,Validation Acc:87.06\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.82,Validation Acc:86.88\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.92,Validation Acc:87.2\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.24,Validation Acc:87.06\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.53,Validation Acc:86.38\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.65,Validation Acc:86.5\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.655,Validation Acc:86.24\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.725,Validation Acc:86.62\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.76,Validation Acc:86.48\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.8,Validation Acc:86.46\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.905,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.93,Validation Acc:86.1\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.935,Validation Acc:86.22\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.945,Validation Acc:86.06\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.95,Validation Acc:85.96\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.975,Validation Acc:85.8\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.985,Validation Acc:86.1\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 6 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 22384 -> token technologies\n",
      "Token technologies -> token id 22384\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:52.94,Validation Acc:51.88\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:74.78,Validation Acc:72.52\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:82.02,Validation Acc:79.18\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:86.635,Validation Acc:83.48\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:89.075,Validation Acc:85.04\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:91.075,Validation Acc:86.1\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:92.665,Validation Acc:86.52\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:93.565,Validation Acc:86.64\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:94.405,Validation Acc:86.78\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:95.075,Validation Acc:87.06\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:95.995,Validation Acc:87.38\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:96.905,Validation Acc:87.72\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:97.295,Validation Acc:87.54\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:97.51,Validation Acc:87.46\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.015,Validation Acc:87.5\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.09,Validation Acc:87.52\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:98.6,Validation Acc:87.84\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:98.96,Validation Acc:87.5\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.19,Validation Acc:87.32\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.285,Validation Acc:87.06\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.33,Validation Acc:87.08\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.445,Validation Acc:87.02\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.585,Validation Acc:87.12\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.78,Validation Acc:87.26\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.81,Validation Acc:86.9\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.79,Validation Acc:86.44\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.85,Validation Acc:86.82\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.885,Validation Acc:86.86\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.925,Validation Acc:86.68\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.94,Validation Acc:86.72\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 7 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 27967 -> token hoary\n",
      "Token hoary -> token id 27967\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:76.775,Validation Acc:74.52\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:86.235,Validation Acc:83.6\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:89.255,Validation Acc:85.24\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.45,Validation Acc:85.76\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:91.525,Validation Acc:85.0\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:94.525,Validation Acc:86.8\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:94.555,Validation Acc:85.98\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.08,Validation Acc:87.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/5],Step:[151/313],Training Acc:96.665,Validation Acc:87.12\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:96.29,Validation Acc:86.3\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.06,Validation Acc:87.78\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.675,Validation Acc:87.32\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.755,Validation Acc:87.2\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.0,Validation Acc:87.04\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.7,Validation Acc:85.54\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.25,Validation Acc:86.7\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.34,Validation Acc:86.58\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.63,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.695,Validation Acc:86.54\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.79,Validation Acc:86.48\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.84,Validation Acc:86.38\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.855,Validation Acc:86.22\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.865,Validation Acc:86.5\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.965,Validation Acc:86.18\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.96,Validation Acc:86.24\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.96,Validation Acc:86.2\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.96,Validation Acc:86.16\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.98,Validation Acc:85.96\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.975,Validation Acc:85.94\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.995,Validation Acc:85.84\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 8 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 66452 -> token mosquito\n",
      "Token mosquito -> token id 66452\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:63.335,Validation Acc:62.26\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:80.035,Validation Acc:78.04\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:84.605,Validation Acc:81.72\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:89.205,Validation Acc:84.88\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:90.985,Validation Acc:85.6\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:92.09,Validation Acc:86.3\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:93.74,Validation Acc:87.22\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:94.675,Validation Acc:87.22\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:95.555,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:96.115,Validation Acc:87.52\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:96.79,Validation Acc:87.22\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.73,Validation Acc:87.7\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:97.865,Validation Acc:87.32\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.235,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.485,Validation Acc:87.48\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.525,Validation Acc:87.44\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.09,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.25,Validation Acc:86.96\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.51,Validation Acc:87.32\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.475,Validation Acc:87.22\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.595,Validation Acc:86.76\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.725,Validation Acc:87.06\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.805,Validation Acc:86.8\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.805,Validation Acc:86.0\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.895,Validation Acc:86.74\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.885,Validation Acc:86.44\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.935,Validation Acc:86.74\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.96,Validation Acc:86.44\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.965,Validation Acc:86.44\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.985,Validation Acc:86.6\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 9 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 90062 -> token or directing\n",
      "Token or directing -> token id 90062\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:72.11,Validation Acc:70.36\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:84.025,Validation Acc:80.36\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:87.88,Validation Acc:84.0\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:89.885,Validation Acc:84.5\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:92.895,Validation Acc:86.4\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:93.94,Validation Acc:86.44\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:95.33,Validation Acc:86.8\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:95.865,Validation Acc:87.02\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.755,Validation Acc:87.44\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.21,Validation Acc:86.76\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.93,Validation Acc:87.32\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.49,Validation Acc:86.96\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.945,Validation Acc:87.28\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.08,Validation Acc:87.0\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.18,Validation Acc:86.96\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.36,Validation Acc:86.9\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.42,Validation Acc:86.52\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.68,Validation Acc:86.76\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.85,Validation Acc:86.76\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.835,Validation Acc:87.2\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.825,Validation Acc:86.92\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.875,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.925,Validation Acc:86.76\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.96,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.98,Validation Acc:86.58\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.98,Validation Acc:86.5\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.985,Validation Acc:86.32\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.99,Validation Acc:86.34\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:100.0,Validation Acc:86.54\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.82\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 10 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 92501 -> token good which\n",
      "Token good which -> token id 92501\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:64.345,Validation Acc:61.66\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:75.415,Validation Acc:72.38\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:81.365,Validation Acc:77.78\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:86.67,Validation Acc:82.64\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:89.465,Validation Acc:84.98\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:91.57,Validation Acc:86.24\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:92.755,Validation Acc:86.3\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:94.13,Validation Acc:87.12\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:94.665,Validation Acc:87.16\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:95.6,Validation Acc:87.36\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:96.605,Validation Acc:87.8\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.5,Validation Acc:88.1\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.045,Validation Acc:88.2\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.225,Validation Acc:88.0\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.555,Validation Acc:88.12\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.745,Validation Acc:88.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[3/5],Step:[126/157],Training Acc:98.89,Validation Acc:87.82\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.265,Validation Acc:88.18\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.46,Validation Acc:87.88\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.535,Validation Acc:87.78\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.55,Validation Acc:87.74\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.67,Validation Acc:87.66\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.695,Validation Acc:87.52\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.83,Validation Acc:87.52\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.84,Validation Acc:87.26\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.905,Validation Acc:87.36\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.925,Validation Acc:87.44\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.95,Validation Acc:87.42\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.97,Validation Acc:87.52\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.975,Validation Acc:87.6\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 11 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 98863 -> token attack with\n",
      "Token attack with -> token id 98863\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:72.01,Validation Acc:69.94\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:86.505,Validation Acc:83.98\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:89.93,Validation Acc:85.6\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.96,Validation Acc:86.66\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.54,Validation Acc:86.56\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:95.425,Validation Acc:86.62\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:96.435,Validation Acc:87.66\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.72,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.29,Validation Acc:87.26\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.9,Validation Acc:87.46\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.395,Validation Acc:87.34\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.95,Validation Acc:87.16\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.22,Validation Acc:86.88\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.34,Validation Acc:86.9\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.43,Validation Acc:86.92\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.535,Validation Acc:87.1\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.65,Validation Acc:87.02\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.82,Validation Acc:86.42\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.88,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.89,Validation Acc:86.86\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.85,Validation Acc:86.34\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.94,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.97,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.965,Validation Acc:85.98\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.995,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.995,Validation Acc:86.28\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.995,Validation Acc:86.42\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.995,Validation Acc:86.48\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.995,Validation Acc:86.6\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.995,Validation Acc:86.58\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 12 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 38710 -> token place this\n",
      "Token place this -> token id 38710\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:71.305,Validation Acc:69.76\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:79.665,Validation Acc:76.68\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:85.42,Validation Acc:81.74\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:89.375,Validation Acc:84.78\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:91.635,Validation Acc:85.84\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:93.315,Validation Acc:86.56\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:94.62,Validation Acc:87.02\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:95.365,Validation Acc:87.28\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:96.065,Validation Acc:86.98\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:96.855,Validation Acc:87.6\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.585,Validation Acc:87.64\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.755,Validation Acc:86.84\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.495,Validation Acc:87.44\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.815,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.035,Validation Acc:87.48\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.065,Validation Acc:86.96\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.375,Validation Acc:87.7\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.61,Validation Acc:87.7\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.68,Validation Acc:87.28\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.7,Validation Acc:87.4\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.76,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.795,Validation Acc:87.14\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.85,Validation Acc:86.48\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.93,Validation Acc:86.98\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.97,Validation Acc:87.08\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.975,Validation Acc:87.18\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.98,Validation Acc:86.92\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.98,Validation Acc:87.28\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.985,Validation Acc:87.02\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.995,Validation Acc:87.06\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 13 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 690120 -> token is pandering\n",
      "Token is pandering -> token id 690120\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:62.195,Validation Acc:58.54\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:82.16,Validation Acc:78.7\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:88.26,Validation Acc:84.3\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.075,Validation Acc:85.58\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.465,Validation Acc:86.88\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:95.13,Validation Acc:87.8\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:96.35,Validation Acc:87.92\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.705,Validation Acc:87.46\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.25,Validation Acc:87.44\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.825,Validation Acc:87.44\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.555,Validation Acc:87.52\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:99.245,Validation Acc:87.46\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.48,Validation Acc:88.08\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.57,Validation Acc:88.04\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.58,Validation Acc:87.5\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.655,Validation Acc:87.6\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.8,Validation Acc:87.72\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.89,Validation Acc:86.84\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.95,Validation Acc:87.76\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.955,Validation Acc:87.62\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.945,Validation Acc:87.58\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.97,Validation Acc:87.5\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.98,Validation Acc:87.04\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.995,Validation Acc:87.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[5/5],Step:[51/313],Training Acc:99.995,Validation Acc:87.52\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.995,Validation Acc:87.54\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.995,Validation Acc:87.54\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.995,Validation Acc:87.54\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.995,Validation Acc:87.32\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:87.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 14 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 245036 -> token a monetary\n",
      "Token a monetary -> token id 245036\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:55.265,Validation Acc:55.12\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:69.01,Validation Acc:64.16\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:81.59,Validation Acc:77.28\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:87.185,Validation Acc:82.7\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:90.71,Validation Acc:84.92\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:92.82,Validation Acc:86.04\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:94.27,Validation Acc:86.74\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:95.185,Validation Acc:87.02\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:95.905,Validation Acc:87.48\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:96.845,Validation Acc:87.7\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.62,Validation Acc:87.82\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:98.165,Validation Acc:87.58\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.82,Validation Acc:87.68\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.96,Validation Acc:88.12\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.065,Validation Acc:87.88\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.32,Validation Acc:88.08\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.43,Validation Acc:87.62\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.675,Validation Acc:87.5\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.745,Validation Acc:87.68\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.82,Validation Acc:87.86\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.77,Validation Acc:87.24\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.85,Validation Acc:87.72\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.88,Validation Acc:87.36\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.925,Validation Acc:87.6\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.93,Validation Acc:86.48\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.97,Validation Acc:87.44\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.98,Validation Acc:87.5\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.985,Validation Acc:87.44\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.985,Validation Acc:87.34\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.995,Validation Acc:87.18\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 15 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 224000 -> token bad could\n",
      "Token bad could -> token id 224000\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:74.26,Validation Acc:71.26\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:87.07,Validation Acc:83.38\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:90.085,Validation Acc:84.68\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:92.71,Validation Acc:86.04\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:94.44,Validation Acc:86.58\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:96.585,Validation Acc:87.28\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.355,Validation Acc:87.62\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.76,Validation Acc:87.4\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:98.34,Validation Acc:87.52\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:98.595,Validation Acc:87.14\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.955,Validation Acc:86.52\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:99.54,Validation Acc:87.06\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.72,Validation Acc:87.04\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.57,Validation Acc:86.5\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.815,Validation Acc:87.36\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.855,Validation Acc:87.48\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.85,Validation Acc:86.78\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.95,Validation Acc:87.04\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.985,Validation Acc:86.86\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.98,Validation Acc:87.06\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.995,Validation Acc:87.0\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.99,Validation Acc:86.92\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.995,Validation Acc:87.04\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.99,Validation Acc:87.0\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.995,Validation Acc:87.14\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.995,Validation Acc:87.12\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.995,Validation Acc:87.1\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.995,Validation Acc:87.08\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:100.0,Validation Acc:86.9\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.96\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 16 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 560756 -> token however became\n",
      "Token however became -> token id 560756\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:70.345,Validation Acc:67.94\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:77.485,Validation Acc:73.68\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:85.805,Validation Acc:82.04\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:89.45,Validation Acc:84.4\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:92.7,Validation Acc:86.0\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:94.59,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:95.62,Validation Acc:87.24\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:96.325,Validation Acc:87.44\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:97.125,Validation Acc:87.66\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:97.75,Validation Acc:88.16\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:98.48,Validation Acc:88.24\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.08,Validation Acc:87.74\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.355,Validation Acc:87.94\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.405,Validation Acc:87.62\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.485,Validation Acc:87.8\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.545,Validation Acc:87.76\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.725,Validation Acc:87.72\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.84,Validation Acc:87.66\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.93,Validation Acc:87.34\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.935,Validation Acc:87.46\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.955,Validation Acc:87.56\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.955,Validation Acc:87.58\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.96,Validation Acc:87.46\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.98,Validation Acc:87.2\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.985,Validation Acc:87.48\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.985,Validation Acc:87.46\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.99,Validation Acc:87.48\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.995,Validation Acc:87.44\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:100.0,Validation Acc:87.38\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:100.0,Validation Acc:87.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 17 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 14824 -> token ... this is\n",
      "Token ... this is -> token id 14824\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:67.185,Validation Acc:65.88\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:81.335,Validation Acc:78.62\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:86.24,Validation Acc:82.26\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:90.18,Validation Acc:85.38\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:92.45,Validation Acc:86.42\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:94.17,Validation Acc:87.0\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:94.78,Validation Acc:87.02\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:95.5,Validation Acc:86.6\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.11,Validation Acc:87.04\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:96.8,Validation Acc:87.14\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.66,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.455,Validation Acc:87.18\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.805,Validation Acc:87.0\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.81,Validation Acc:87.28\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.09,Validation Acc:86.8\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.225,Validation Acc:86.98\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.425,Validation Acc:87.12\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.605,Validation Acc:86.92\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.75,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.785,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.765,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.805,Validation Acc:86.58\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.865,Validation Acc:86.44\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.95,Validation Acc:86.48\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.97,Validation Acc:86.4\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.985,Validation Acc:86.24\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.965,Validation Acc:86.52\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.975,Validation Acc:86.28\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.975,Validation Acc:86.26\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.99,Validation Acc:86.04\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 18 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 75816 -> token across as an\n",
      "Token across as an -> token id 75816\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:62.335,Validation Acc:60.7\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:76.715,Validation Acc:74.06\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:81.35,Validation Acc:78.32\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:84.895,Validation Acc:81.06\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:88.985,Validation Acc:84.02\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:90.97,Validation Acc:85.72\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:92.525,Validation Acc:86.22\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:93.485,Validation Acc:86.36\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:94.245,Validation Acc:86.98\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:94.93,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:96.195,Validation Acc:87.26\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.04,Validation Acc:87.62\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:97.495,Validation Acc:87.66\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:97.44,Validation Acc:87.18\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.005,Validation Acc:87.78\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.38,Validation Acc:87.6\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:98.725,Validation Acc:87.54\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.015,Validation Acc:87.42\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.21,Validation Acc:87.26\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.23,Validation Acc:87.28\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.4,Validation Acc:87.28\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.485,Validation Acc:87.1\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.665,Validation Acc:87.06\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.71,Validation Acc:87.22\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.775,Validation Acc:86.76\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.815,Validation Acc:86.58\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.805,Validation Acc:86.88\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.845,Validation Acc:86.76\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.91,Validation Acc:86.62\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.945,Validation Acc:86.8\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 19 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 28748 -> token awry\n",
      "Token awry -> token id 28748\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:75.77,Validation Acc:73.88\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:86.125,Validation Acc:83.2\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:89.285,Validation Acc:84.66\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.85,Validation Acc:86.24\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.01,Validation Acc:85.7\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:95.34,Validation Acc:87.04\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:96.1,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.47,Validation Acc:87.34\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.935,Validation Acc:87.7\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.215,Validation Acc:86.48\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.125,Validation Acc:86.9\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.675,Validation Acc:86.48\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.98,Validation Acc:86.76\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.05,Validation Acc:86.7\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.305,Validation Acc:86.84\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.345,Validation Acc:86.46\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.565,Validation Acc:86.72\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:98.835,Validation Acc:84.34\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.835,Validation Acc:86.56\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.835,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.85,Validation Acc:86.14\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.925,Validation Acc:86.32\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.92,Validation Acc:86.46\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.98,Validation Acc:86.32\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.99,Validation Acc:86.2\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.985,Validation Acc:86.1\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.975,Validation Acc:86.12\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:100.0,Validation Acc:86.32\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:100.0,Validation Acc:86.4\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 20 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 71169 -> token hilarious moments\n",
      "Token hilarious moments -> token id 71169\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:65.71,Validation Acc:64.7\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:79.16,Validation Acc:76.54\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:85.56,Validation Acc:82.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5],Step:[101/157],Training Acc:88.805,Validation Acc:83.94\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:91.515,Validation Acc:85.8\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:93.255,Validation Acc:86.44\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:94.115,Validation Acc:86.8\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:95.15,Validation Acc:86.92\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:95.72,Validation Acc:87.04\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:96.635,Validation Acc:87.2\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.44,Validation Acc:87.32\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:97.04,Validation Acc:85.92\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.45,Validation Acc:87.18\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.545,Validation Acc:87.04\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.77,Validation Acc:87.2\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.985,Validation Acc:86.92\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.23,Validation Acc:87.18\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.5,Validation Acc:86.88\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.585,Validation Acc:87.08\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.45,Validation Acc:86.56\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.685,Validation Acc:86.76\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.69,Validation Acc:86.78\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.815,Validation Acc:86.54\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.84,Validation Acc:86.5\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.895,Validation Acc:86.76\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.94,Validation Acc:86.76\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.925,Validation Acc:86.46\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.94,Validation Acc:86.68\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.96,Validation Acc:86.5\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.99,Validation Acc:86.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 21 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 922348 -> token truck sport a\n",
      "Token truck sport a -> token id 922348\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:70.675,Validation Acc:69.14\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:83.26,Validation Acc:79.0\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:88.005,Validation Acc:83.16\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:90.75,Validation Acc:84.48\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.84,Validation Acc:86.7\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:95.49,Validation Acc:87.5\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:96.005,Validation Acc:87.2\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.905,Validation Acc:87.98\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.54,Validation Acc:87.46\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.925,Validation Acc:87.18\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.58,Validation Acc:87.22\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:99.155,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.335,Validation Acc:87.28\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.44,Validation Acc:87.56\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.485,Validation Acc:87.48\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.625,Validation Acc:87.24\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.73,Validation Acc:87.24\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.915,Validation Acc:87.22\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.945,Validation Acc:87.06\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.94,Validation Acc:87.0\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.945,Validation Acc:86.88\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.94,Validation Acc:86.68\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.965,Validation Acc:86.7\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.99,Validation Acc:86.92\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.995,Validation Acc:86.96\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.995,Validation Acc:86.7\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.995,Validation Acc:87.0\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.99,Validation Acc:86.96\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.995,Validation Acc:87.12\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.68\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 22 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 422398 -> token show more of\n",
      "Token show more of -> token id 422398\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:57.35,Validation Acc:57.62\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:73.735,Validation Acc:70.92\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:81.555,Validation Acc:77.82\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:85.68,Validation Acc:80.96\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:89.89,Validation Acc:84.48\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:92.24,Validation Acc:85.38\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:93.845,Validation Acc:86.38\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:94.845,Validation Acc:86.74\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:95.48,Validation Acc:86.62\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:96.38,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.435,Validation Acc:87.64\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:98.37,Validation Acc:87.62\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.645,Validation Acc:87.2\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.73,Validation Acc:87.3\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.015,Validation Acc:87.6\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.215,Validation Acc:87.42\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.435,Validation Acc:87.46\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.645,Validation Acc:87.22\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.72,Validation Acc:87.28\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.745,Validation Acc:87.08\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.82,Validation Acc:87.1\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.875,Validation Acc:87.34\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.885,Validation Acc:87.32\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.94,Validation Acc:87.3\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.965,Validation Acc:87.26\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.965,Validation Acc:87.48\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.985,Validation Acc:87.28\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.995,Validation Acc:87.2\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.995,Validation Acc:87.42\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.99,Validation Acc:87.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 23 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 955411 -> token adult brandon maggart\n",
      "Token adult brandon maggart -> token id 955411\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:74.385,Validation Acc:71.36\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:86.435,Validation Acc:83.42\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:90.245,Validation Acc:85.26\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:91.68,Validation Acc:85.74\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.975,Validation Acc:85.9\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:96.13,Validation Acc:87.16\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:96.815,Validation Acc:87.38\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.23,Validation Acc:87.48\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.815,Validation Acc:87.06\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.7,Validation Acc:86.38\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.96,Validation Acc:86.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/5],Step:[301/313],Training Acc:99.365,Validation Acc:86.6\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.655,Validation Acc:86.82\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.66,Validation Acc:87.1\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.69,Validation Acc:87.3\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.72,Validation Acc:86.74\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.83,Validation Acc:86.9\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.94,Validation Acc:86.96\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.955,Validation Acc:86.34\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.985,Validation Acc:86.74\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.985,Validation Acc:86.54\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.995,Validation Acc:86.96\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:100.0,Validation Acc:86.9\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.82\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.995,Validation Acc:87.02\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:100.0,Validation Acc:86.66\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.995,Validation Acc:86.74\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.995,Validation Acc:86.76\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:100.0,Validation Acc:86.7\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:100.0,Validation Acc:86.76\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 24 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14220114\n",
      "Token id 396019 -> token 25 but\n",
      "Token 25 but -> token id 396019\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:68.185,Validation Acc:67.22\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:80.755,Validation Acc:77.6\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:86.475,Validation Acc:82.12\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:89.575,Validation Acc:84.1\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:92.53,Validation Acc:86.38\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:94.42,Validation Acc:86.72\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:95.355,Validation Acc:86.82\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:95.935,Validation Acc:87.16\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:96.99,Validation Acc:87.74\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:97.685,Validation Acc:87.66\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:98.235,Validation Acc:87.68\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.005,Validation Acc:88.04\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.185,Validation Acc:87.7\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.37,Validation Acc:87.9\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.26,Validation Acc:87.02\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.565,Validation Acc:87.8\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.625,Validation Acc:87.58\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.805,Validation Acc:87.12\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.925,Validation Acc:87.54\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.935,Validation Acc:87.82\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.91,Validation Acc:87.46\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.88,Validation Acc:87.34\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.97,Validation Acc:87.68\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.99,Validation Acc:87.46\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:100.0,Validation Acc:87.28\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:100.0,Validation Acc:87.24\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:100.0,Validation Acc:87.2\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:100.0,Validation Acc:87.5\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:100.0,Validation Acc:87.36\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:100.0,Validation Acc:87.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 25 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 26209 -> token sidewalks\n",
      "Token sidewalks -> token id 26209\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:85.95,Validation Acc:83.24\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:88.245,Validation Acc:83.18\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:90.965,Validation Acc:84.64\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.65,Validation Acc:86.12\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:93.92,Validation Acc:85.3\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:96.385,Validation Acc:86.48\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.225,Validation Acc:86.54\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.415,Validation Acc:85.74\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.68,Validation Acc:85.84\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.87,Validation Acc:85.7\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.425,Validation Acc:86.08\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.875,Validation Acc:85.64\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.13,Validation Acc:85.34\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:97.08,Validation Acc:82.92\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.495,Validation Acc:84.42\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.995,Validation Acc:84.92\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:98.38,Validation Acc:84.56\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:98.73,Validation Acc:85.5\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.16,Validation Acc:85.16\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:98.235,Validation Acc:83.82\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:98.475,Validation Acc:83.36\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.05,Validation Acc:84.38\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:98.965,Validation Acc:84.02\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.06,Validation Acc:83.96\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.425,Validation Acc:84.92\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.43,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.4,Validation Acc:83.94\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.465,Validation Acc:84.5\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.545,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.75,Validation Acc:82.84\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 26 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 25751 -> token constrictor\n",
      "Token constrictor -> token id 25751\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:81.22,Validation Acc:77.44\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:88.725,Validation Acc:84.42\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:91.665,Validation Acc:85.52\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:93.435,Validation Acc:86.38\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:94.83,Validation Acc:86.16\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:96.855,Validation Acc:86.74\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:97.72,Validation Acc:86.96\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.735,Validation Acc:86.5\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:97.415,Validation Acc:85.12\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:98.46,Validation Acc:85.74\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:98.445,Validation Acc:85.44\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.125,Validation Acc:85.34\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.46,Validation Acc:85.48\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.445,Validation Acc:85.54\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.485,Validation Acc:84.96\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.325,Validation Acc:84.54\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.23,Validation Acc:84.0\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.275,Validation Acc:84.4\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.45,Validation Acc:84.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[4/5],Step:[51/157],Training Acc:99.475,Validation Acc:84.7\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.51,Validation Acc:84.44\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.245,Validation Acc:84.34\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.04,Validation Acc:83.7\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.45,Validation Acc:84.4\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.435,Validation Acc:84.06\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.46,Validation Acc:84.0\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.46,Validation Acc:83.92\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.59,Validation Acc:84.14\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.71,Validation Acc:84.74\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.495,Validation Acc:83.96\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 27 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 81529 -> token you?!book\n",
      "Token you?!book -> token id 81529\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:86.195,Validation Acc:82.88\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:88.75,Validation Acc:84.96\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:88.99,Validation Acc:83.18\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.33,Validation Acc:86.04\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:94.545,Validation Acc:85.88\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:94.555,Validation Acc:85.16\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:95.68,Validation Acc:84.88\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:96.535,Validation Acc:85.64\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:95.96,Validation Acc:84.44\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.48,Validation Acc:84.94\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.45,Validation Acc:85.48\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:95.635,Validation Acc:82.96\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:97.995,Validation Acc:84.44\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:96.695,Validation Acc:83.1\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:97.42,Validation Acc:84.1\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:96.955,Validation Acc:84.2\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:97.825,Validation Acc:83.88\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:97.915,Validation Acc:84.18\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:97.905,Validation Acc:83.76\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:96.72,Validation Acc:83.32\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:98.07,Validation Acc:84.3\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:97.825,Validation Acc:83.78\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:97.97,Validation Acc:83.44\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:97.245,Validation Acc:83.4\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:94.51,Validation Acc:80.36\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:97.85,Validation Acc:83.12\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:98.08,Validation Acc:83.74\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:98.415,Validation Acc:84.1\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:95.995,Validation Acc:81.88\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.1,Validation Acc:83.16\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 28 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 37555 -> token limber\n",
      "Token limber -> token id 37555\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:84.76,Validation Acc:82.36\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:89.025,Validation Acc:84.66\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:91.62,Validation Acc:86.3\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:92.845,Validation Acc:85.7\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:95.065,Validation Acc:87.2\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:96.75,Validation Acc:87.68\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:97.465,Validation Acc:87.2\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.585,Validation Acc:86.66\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:97.82,Validation Acc:86.4\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:98.08,Validation Acc:86.56\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.19,Validation Acc:84.46\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:98.54,Validation Acc:85.28\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.205,Validation Acc:85.6\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:98.825,Validation Acc:84.72\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.88,Validation Acc:85.02\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.24,Validation Acc:84.44\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:98.96,Validation Acc:85.26\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:98.79,Validation Acc:84.68\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:98.845,Validation Acc:84.46\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:98.35,Validation Acc:83.88\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:98.8,Validation Acc:84.62\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:98.505,Validation Acc:84.06\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.195,Validation Acc:84.52\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:98.87,Validation Acc:83.56\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.595,Validation Acc:84.78\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.695,Validation Acc:85.02\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.64,Validation Acc:84.52\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.57,Validation Acc:84.76\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.525,Validation Acc:83.98\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.455,Validation Acc:84.14\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 29 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 74231 -> token chairperson\n",
      "Token chairperson -> token id 74231\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:85.01,Validation Acc:81.78\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:89.48,Validation Acc:85.12\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:91.74,Validation Acc:86.34\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.9,Validation Acc:86.34\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:95.01,Validation Acc:86.3\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:96.71,Validation Acc:87.16\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.015,Validation Acc:86.26\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.38,Validation Acc:85.68\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.27,Validation Acc:85.72\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.545,Validation Acc:85.06\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:98.085,Validation Acc:85.48\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.37,Validation Acc:85.32\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.75,Validation Acc:84.86\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.405,Validation Acc:84.1\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.82,Validation Acc:84.72\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.555,Validation Acc:84.56\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:98.875,Validation Acc:84.64\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:98.98,Validation Acc:84.66\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.345,Validation Acc:85.42\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.22,Validation Acc:84.72\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:98.795,Validation Acc:83.7\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:98.65,Validation Acc:83.58\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:98.82,Validation Acc:83.78\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:98.735,Validation Acc:84.0\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.105,Validation Acc:83.86\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.23,Validation Acc:84.4\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.18,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.19,Validation Acc:84.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[5/5],Step:[251/313],Training Acc:98.395,Validation Acc:82.96\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.885,Validation Acc:83.78\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 30 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 42577 -> token reclaimed\n",
      "Token reclaimed -> token id 42577\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:85.715,Validation Acc:83.68\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:88.36,Validation Acc:83.6\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:90.59,Validation Acc:84.26\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:93.755,Validation Acc:86.42\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:95.275,Validation Acc:86.5\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:96.815,Validation Acc:87.06\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:97.51,Validation Acc:87.02\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.49,Validation Acc:86.12\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:97.585,Validation Acc:85.82\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:97.5,Validation Acc:85.28\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.92,Validation Acc:84.7\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.155,Validation Acc:86.42\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:98.91,Validation Acc:85.08\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.435,Validation Acc:85.92\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.375,Validation Acc:85.74\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.2,Validation Acc:84.86\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.375,Validation Acc:85.18\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.47,Validation Acc:85.14\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.46,Validation Acc:84.7\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.53,Validation Acc:84.92\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.425,Validation Acc:84.22\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.635,Validation Acc:85.1\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.46,Validation Acc:84.14\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.63,Validation Acc:84.86\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.57,Validation Acc:84.4\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.645,Validation Acc:84.3\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.47,Validation Acc:84.18\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.62,Validation Acc:84.16\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.74,Validation Acc:84.36\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.835,Validation Acc:84.8\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 31 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 8205 -> token surgeon\n",
      "Token surgeon -> token id 8205\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:83.975,Validation Acc:80.46\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:88.955,Validation Acc:83.82\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:90.835,Validation Acc:84.84\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.22,Validation Acc:85.98\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:92.92,Validation Acc:84.66\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:95.57,Validation Acc:86.42\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:95.775,Validation Acc:85.5\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:94.755,Validation Acc:83.34\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:96.11,Validation Acc:84.14\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:97.115,Validation Acc:85.38\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.715,Validation Acc:85.5\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:94.455,Validation Acc:81.64\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:97.41,Validation Acc:84.28\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:95.93,Validation Acc:83.48\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:97.3,Validation Acc:84.18\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:96.45,Validation Acc:83.28\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:92.98,Validation Acc:79.88\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:97.08,Validation Acc:84.26\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:97.795,Validation Acc:84.1\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:98.085,Validation Acc:85.24\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:97.925,Validation Acc:84.44\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:97.645,Validation Acc:83.62\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:98.38,Validation Acc:84.94\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:97.66,Validation Acc:84.38\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:98.2,Validation Acc:84.4\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:97.705,Validation Acc:84.02\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:97.36,Validation Acc:83.54\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:98.105,Validation Acc:84.08\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:98.39,Validation Acc:84.02\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.16,Validation Acc:83.7\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 32 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4760038\n",
      "Token id 76919 -> token boooo\n",
      "Token boooo -> token id 76919\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:82.57,Validation Acc:79.42\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:85.11,Validation Acc:81.28\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:91.495,Validation Acc:85.28\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:93.315,Validation Acc:86.58\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:95.29,Validation Acc:86.72\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:96.29,Validation Acc:87.18\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:96.785,Validation Acc:85.88\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.49,Validation Acc:86.04\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:97.53,Validation Acc:85.4\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:98.15,Validation Acc:86.16\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:97.705,Validation Acc:84.58\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:98.685,Validation Acc:85.08\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.09,Validation Acc:85.4\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.125,Validation Acc:85.34\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:98.98,Validation Acc:84.94\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.115,Validation Acc:84.94\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.08,Validation Acc:84.36\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.35,Validation Acc:84.98\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.125,Validation Acc:85.1\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:98.825,Validation Acc:83.84\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.155,Validation Acc:84.52\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.165,Validation Acc:84.56\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.155,Validation Acc:84.42\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:98.615,Validation Acc:82.66\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.205,Validation Acc:84.28\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.18,Validation Acc:84.88\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.11,Validation Acc:84.18\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:98.1,Validation Acc:83.18\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:98.835,Validation Acc:84.3\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:98.55,Validation Acc:83.6\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 33 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 73408 -> token any self\n",
      "Token any self -> token id 73408\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5],Step:[51/313],Training Acc:85.595,Validation Acc:82.66\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:88.05,Validation Acc:83.28\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:89.87,Validation Acc:83.2\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.905,Validation Acc:86.48\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:95.565,Validation Acc:86.68\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:97.135,Validation Acc:86.72\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.71,Validation Acc:85.84\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.8,Validation Acc:86.02\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:98.05,Validation Acc:85.96\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:98.22,Validation Acc:86.16\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.915,Validation Acc:84.12\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.065,Validation Acc:84.7\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.245,Validation Acc:85.62\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.275,Validation Acc:85.42\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.645,Validation Acc:84.66\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.505,Validation Acc:84.48\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.16,Validation Acc:85.36\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.32,Validation Acc:85.7\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.19,Validation Acc:84.98\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:98.525,Validation Acc:83.56\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.3,Validation Acc:84.72\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.39,Validation Acc:84.4\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.12,Validation Acc:84.24\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.215,Validation Acc:83.74\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.33,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.33,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.44,Validation Acc:84.54\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:98.885,Validation Acc:84.32\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:99.08,Validation Acc:83.96\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:99.155,Validation Acc:83.4\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 34 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 76385 -> token deem\n",
      "Token deem -> token id 76385\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:85.945,Validation Acc:83.14\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:89.285,Validation Acc:84.48\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:89.18,Validation Acc:81.98\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:94.07,Validation Acc:86.16\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:95.445,Validation Acc:85.96\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:97.425,Validation Acc:87.22\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:97.625,Validation Acc:86.86\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.995,Validation Acc:87.06\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:98.22,Validation Acc:85.94\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:98.44,Validation Acc:85.64\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:98.845,Validation Acc:85.86\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.31,Validation Acc:85.36\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.495,Validation Acc:85.82\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.58,Validation Acc:85.46\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.665,Validation Acc:85.58\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.73,Validation Acc:85.6\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.705,Validation Acc:85.46\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.76,Validation Acc:85.42\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.735,Validation Acc:85.2\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.685,Validation Acc:85.18\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.725,Validation Acc:85.36\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.795,Validation Acc:84.78\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.6,Validation Acc:84.82\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:99.81,Validation Acc:84.98\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.55,Validation Acc:84.48\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.67,Validation Acc:84.7\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:99.735,Validation Acc:84.6\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.715,Validation Acc:84.78\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.675,Validation Acc:84.74\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.65,Validation Acc:84.12\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 35 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 21612 -> token was mostly\n",
      "Token was mostly -> token id 21612\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:83.2,Validation Acc:79.52\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:89.595,Validation Acc:84.66\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:91.88,Validation Acc:85.42\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.23,Validation Acc:85.74\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:94.87,Validation Acc:85.4\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:96.15,Validation Acc:86.4\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.485,Validation Acc:86.08\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:97.33,Validation Acc:85.74\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:97.29,Validation Acc:84.62\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:98.035,Validation Acc:85.18\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.345,Validation Acc:84.98\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:98.295,Validation Acc:85.18\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:97.905,Validation Acc:85.0\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.365,Validation Acc:85.46\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.365,Validation Acc:85.06\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:98.055,Validation Acc:85.1\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:97.97,Validation Acc:84.38\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:98.2,Validation Acc:84.74\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:97.695,Validation Acc:83.18\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:97.645,Validation Acc:83.52\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:98.295,Validation Acc:85.12\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:98.16,Validation Acc:84.1\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:98.045,Validation Acc:84.46\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:96.015,Validation Acc:82.18\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:98.355,Validation Acc:84.12\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:98.7,Validation Acc:84.22\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:98.225,Validation Acc:83.86\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:98.24,Validation Acc:83.9\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:95.955,Validation Acc:81.62\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.455,Validation Acc:84.18\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 36 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 4911 -> token anderson\n",
      "Token anderson -> token id 4911\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:85.035,Validation Acc:81.86\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:89.51,Validation Acc:85.06\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:92.47,Validation Acc:85.82\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:94.305,Validation Acc:86.6\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:95.175,Validation Acc:85.7\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:97.425,Validation Acc:87.06\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:98.095,Validation Acc:87.42\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:97.67,Validation Acc:85.8\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:96.835,Validation Acc:84.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/5],Step:[101/157],Training Acc:97.675,Validation Acc:84.6\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:98.85,Validation Acc:85.6\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.285,Validation Acc:85.42\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.41,Validation Acc:85.22\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.365,Validation Acc:85.5\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.385,Validation Acc:84.92\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:98.875,Validation Acc:84.74\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.39,Validation Acc:85.22\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.18,Validation Acc:84.52\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.49,Validation Acc:85.56\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.625,Validation Acc:85.26\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.56,Validation Acc:85.0\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.56,Validation Acc:84.76\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.525,Validation Acc:84.28\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:98.995,Validation Acc:84.12\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:99.365,Validation Acc:84.26\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:98.995,Validation Acc:82.9\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:98.795,Validation Acc:83.32\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:99.49,Validation Acc:83.96\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:99.345,Validation Acc:84.44\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:99.575,Validation Acc:84.66\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 37 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 487055 -> token scarecrow batgirl\n",
      "Token scarecrow batgirl -> token id 487055\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:86.225,Validation Acc:83.22\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:87.05,Validation Acc:79.68\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:92.105,Validation Acc:85.98\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:92.715,Validation Acc:82.96\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:95.95,Validation Acc:85.54\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:98.045,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:98.81,Validation Acc:87.46\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:98.905,Validation Acc:86.56\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:98.905,Validation Acc:85.7\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:99.27,Validation Acc:86.84\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:99.465,Validation Acc:86.58\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:99.62,Validation Acc:86.18\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:99.625,Validation Acc:85.74\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:99.615,Validation Acc:85.8\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:99.765,Validation Acc:85.82\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:99.66,Validation Acc:85.06\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:99.74,Validation Acc:85.14\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:99.7,Validation Acc:84.98\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.795,Validation Acc:85.12\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:99.855,Validation Acc:85.36\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:99.6,Validation Acc:84.88\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:99.68,Validation Acc:84.96\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:99.66,Validation Acc:84.52\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.495,Validation Acc:84.12\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.63,Validation Acc:84.76\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.625,Validation Acc:84.36\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.715,Validation Acc:84.12\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.7,Validation Acc:83.88\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:98.425,Validation Acc:80.4\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.425,Validation Acc:82.6\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 38 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 221302 -> token embarrassment that\n",
      "Token embarrassment that -> token id 221302\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:83.91,Validation Acc:80.96\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:88.275,Validation Acc:83.42\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:91.57,Validation Acc:85.0\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:94.555,Validation Acc:86.7\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:96.415,Validation Acc:86.9\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:98.225,Validation Acc:87.54\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:98.86,Validation Acc:87.4\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:98.98,Validation Acc:87.46\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:99.205,Validation Acc:86.86\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:99.145,Validation Acc:85.7\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:99.43,Validation Acc:86.12\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.04,Validation Acc:85.16\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.81,Validation Acc:86.68\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.835,Validation Acc:86.56\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.86,Validation Acc:86.3\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.74,Validation Acc:85.62\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.89,Validation Acc:86.36\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.9,Validation Acc:86.14\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.89,Validation Acc:86.16\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:99.925,Validation Acc:86.18\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:99.975,Validation Acc:86.0\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:99.98,Validation Acc:86.1\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:99.99,Validation Acc:86.02\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:100.0,Validation Acc:85.98\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:100.0,Validation Acc:86.3\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:99.995,Validation Acc:86.44\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:100.0,Validation Acc:86.1\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:100.0,Validation Acc:86.36\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:100.0,Validation Acc:86.4\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:100.0,Validation Acc:86.36\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 39 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 667106 -> token up transpiring\n",
      "Token up transpiring -> token id 667106\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:84.93,Validation Acc:81.82\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:89.41,Validation Acc:84.54\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:92.24,Validation Acc:85.14\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:93.695,Validation Acc:84.92\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:95.865,Validation Acc:86.56\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:97.015,Validation Acc:85.9\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:97.9,Validation Acc:85.84\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:98.445,Validation Acc:86.5\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:98.625,Validation Acc:86.44\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:96.8,Validation Acc:83.02\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:97.125,Validation Acc:83.6\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:96.985,Validation Acc:80.68\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:98.715,Validation Acc:84.86\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:98.81,Validation Acc:84.74\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:98.94,Validation Acc:85.16\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:97.435,Validation Acc:81.92\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:98.935,Validation Acc:84.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[3/5],Step:[301/313],Training Acc:99.195,Validation Acc:84.24\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:99.095,Validation Acc:83.42\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:97.745,Validation Acc:81.66\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:98.525,Validation Acc:83.48\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:98.185,Validation Acc:82.96\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:98.545,Validation Acc:83.66\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:99.19,Validation Acc:84.62\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:99.38,Validation Acc:84.28\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:99.155,Validation Acc:83.1\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:99.325,Validation Acc:84.18\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:99.375,Validation Acc:84.14\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:98.57,Validation Acc:83.14\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:98.91,Validation Acc:83.92\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 40 / 96\n",
      "Learning Rate = 0.1\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9500076\n",
      "Token id 203892 -> token seldom been\n",
      "Token seldom been -> token id 203892\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:85.39,Validation Acc:82.18\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:89.82,Validation Acc:85.36\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:92.465,Validation Acc:85.32\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:94.48,Validation Acc:86.42\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:96.385,Validation Acc:86.6\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:97.91,Validation Acc:86.58\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:98.75,Validation Acc:87.38\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:98.935,Validation Acc:87.3\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:99.05,Validation Acc:86.42\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:99.105,Validation Acc:85.3\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:99.44,Validation Acc:86.34\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:99.655,Validation Acc:85.74\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:99.82,Validation Acc:86.1\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:99.855,Validation Acc:86.12\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:99.765,Validation Acc:85.54\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:99.645,Validation Acc:84.88\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:99.75,Validation Acc:84.86\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:99.7,Validation Acc:84.44\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:99.76,Validation Acc:84.8\n"
     ]
    }
   ],
   "source": [
    "param_val_losses_adam_nolemma = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"Adam\",\n",
    "                                          lemmatize = False)\n",
    "pkl.dump(param_val_losses_adam_nolemma, \n",
    "         open(\"param_val_losses_adam_nolemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_val_losses_adam_lemma = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"Adam\",\n",
    "                                          lemmatize = True)\n",
    "pkl.dump(param_val_losses_adam_lemma, \n",
    "         open(\"param_val_losses_adam_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_val_losses_sgd_nolemma = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"SGD\",\n",
    "                                          lemmatize = False)\n",
    "pkl.dump(param_val_losses_sgd_nolemma,\n",
    "         open(\"param_val_losses_sgd_nolemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_val_losses_sgd_lemma = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"SGD\",\n",
    "                                          lemmatize = True)\n",
    "pkl.dump(param_val_losses_sgd_lemma,\n",
    "         open(\"param_val_losses_sgd_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
