{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import spacy\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_loc = \"data/imdb_reviews/\"\n",
    "\n",
    "# def read_txt_files(folder_path):\n",
    "#     \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "#     file_list = os.listdir(folder_path)\n",
    "#     # for debugging, printing out the folder path and some files in it\n",
    "#     print(folder_path)\n",
    "#     print(file_list[:10])\n",
    "    \n",
    "#     all_reviews = []\n",
    "#     for file_path in file_list:\n",
    "#         f = open(folder_path + file_path,\"r\")\n",
    "#         all_reviews.append(f.readline())\n",
    "        \n",
    "#     return all_reviews\n",
    "\n",
    "# train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "# print(len(train_pos))\n",
    "# train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "# print(len(train_neg))\n",
    "# test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "# print(len(test_pos))\n",
    "# test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "# print(len(test_neg))\n",
    "\n",
    "# print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "# print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "# print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "# print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "# train_pos_labels\n",
    "\n",
    "# train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "# train_neg_labels\n",
    "\n",
    "# train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "# print(len(train_data_labels))\n",
    "# print(train_data_labels)\n",
    "\n",
    "# test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "# test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "# test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "# print(len(test_data_labels))\n",
    "# print(test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def cleanhtml(raw_html):\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', raw_html)\n",
    "#     return cleantext\n",
    "\n",
    "# train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "# train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "# test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "# test_neg_clean = [cleanhtml(x) for x in test_neg]\n",
    "\n",
    "# train_all_clean = train_pos_clean + train_neg_clean\n",
    "# len(train_all_clean)\n",
    "\n",
    "# test_all_clean = test_pos_clean + test_neg_clean\n",
    "# len(test_all_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training_size = 20000\n",
    "\n",
    "# assert training_size < 25000\n",
    "\n",
    "# shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "# print(len(shuffled_index))\n",
    "# print(shuffled_index)\n",
    "\n",
    "# training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "# training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "# print(len(training_all_clean))\n",
    "# print(len(training_labels))\n",
    "\n",
    "# validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "# validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "# print(len(validation_all_clean))\n",
    "# print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pkl.dump(shuffled_index, open(\"shuffled_index.p\", \"wb\"))\n",
    "# pkl.dump(training_all_clean, open(\"training_all_clean.p\", \"wb\"))\n",
    "# pkl.dump(training_labels,  open(\"training_labels.p\", \"wb\"))\n",
    "# pkl.dump(validation_all_clean, open(\"validation_all_clean.p\", \"wb\"))\n",
    "# pkl.dump(validation_labels,  open(\"validation_labels.p\", \"wb\"))\n",
    "\n",
    "shuffled_index = pkl.load(open(\"shuffled_index.p\", \"rb\"))\n",
    "training_all_clean = pkl.load(open(\"training_all_clean.p\", \"rb\"))\n",
    "training_labels = pkl.load(open(\"training_labels.p\", \"rb\"))\n",
    "validation_all_clean = pkl.load(open(\"validation_all_clean.p\", \"rb\"))\n",
    "validation_labels = pkl.load(open(\"validation_labels.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import string\n",
    "\n",
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation\n",
    "\n",
    "# # This is word tokenizer\n",
    "# # # lowercase and remove punctuation\n",
    "# # def tokenize(sent):\n",
    "# #     tokens = tokenizer(sent)\n",
    "# #     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "# #     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# # Modified for n-grams\n",
    "# def tokenize(sent, n_gram = 0, lemmatize = False):\n",
    "    \n",
    "#     tokens = tokenizer(sent)\n",
    "    \n",
    "#     # unigrams\n",
    "#     if lemmatize == False:\n",
    "#         unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     else:\n",
    "#         #LEMMATIZED\n",
    "#         unigrams = [token.lemma_.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    \n",
    "    \n",
    "#     output = []\n",
    "#     output.extend(unigrams)\n",
    "    \n",
    "#     n = 2\n",
    "#     while n <= n_gram:\n",
    "#         ngram_tokens = [\" \".join(unigrams[x:x+n]) for x in range(len(unigrams)-n+1)]\n",
    "#         output.extend(ngram_tokens)\n",
    "#         n = n + 1\n",
    "        \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def lower_case_remove_punc(parsed):\n",
    "#     return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "# def tokenize_dataset(dataset, n_gram, lemmatize = True):\n",
    "#     token_dataset = []\n",
    "#     # we are keeping track of all tokens in dataset\n",
    "#     # in order to create vocabulary later\n",
    "#     all_tokens = []\n",
    "\n",
    "# #     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "# #                                                disable=['parser', 'tagger', 'ner'], \n",
    "# #                                                batch_size=512, \n",
    "# #                                                n_threads=4)):\n",
    "\n",
    "#     itr = 0\n",
    "#     for sample in dataset:\n",
    "        \n",
    "#         if itr % 50 == 0:\n",
    "#             print(str(itr) + \" / \" + str(len(dataset)))\n",
    "#         # unigram version\n",
    "#         #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "#         # n-gram version\n",
    "#         tokens = tokenize(sample,n_gram, lemmatize = lemmatize)\n",
    "        \n",
    "#         token_dataset.append(tokens)\n",
    "#         all_tokens += tokens\n",
    "        \n",
    "#         itr = itr + 1\n",
    "\n",
    "#     return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = [1,2,3]\n",
    "lemmatize_list = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for lemmatize_arg in lemmatize_list:\n",
    "#     for gram_no in grams:\n",
    "#         print(str(gram_no))\n",
    "\n",
    "#         train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "#                                                                n_gram=gram_no, \n",
    "#                                                                lemmatize = lemmatize_arg)\n",
    "\n",
    "#         # Tokenize Validation\n",
    "#         val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "#                                               n_gram=gram_no, \n",
    "#                                               lemmatize = lemmatize_arg)\n",
    "\n",
    "#         if lemmatize_arg == True:\n",
    "#             gram_no = str(gram_no) + \"_lemma\"\n",
    "#         else:\n",
    "#             gram_no = str(gram_no)\n",
    "#         print(gram_no)\n",
    "\n",
    "#         # val set tokens\n",
    "#         print (\"Tokenizing val data\")\n",
    "#         pkl.dump(val_data_tokens, open(\"val_data_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n",
    "\n",
    "#         # train set tokens\n",
    "#         print (\"Tokenizing train data\")\n",
    "#         pkl.dump(train_data_tokens, open(\"train_data_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n",
    "#         pkl.dump(all_train_tokens, open(\"all_train_tokens_\"+str(gram_no)+\".p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.01, 1, 100000.0, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 128),\n",
       " (1, 1, 100000.0, 100, 64),\n",
       " (1, 1, 100000.0, 100, 128),\n",
       " (1, 1, 100000.0, 200, 64),\n",
       " (1, 1, 100000.0, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 128),\n",
       " (1, 2, 100000.0, 100, 64),\n",
       " (1, 2, 100000.0, 100, 128),\n",
       " (1, 2, 100000.0, 200, 64),\n",
       " (1, 2, 100000.0, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 128),\n",
       " (1, 3, 100000.0, 100, 64),\n",
       " (1, 3, 100000.0, 100, 128),\n",
       " (1, 3, 100000.0, 200, 64),\n",
       " (1, 3, 100000.0, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 128),\n",
       " (2, 1, 100000.0, 100, 64),\n",
       " (2, 1, 100000.0, 100, 128),\n",
       " (2, 1, 100000.0, 200, 64),\n",
       " (2, 1, 100000.0, 200, 128),\n",
       " (2, 1, 1000000.0, 100, 64),\n",
       " (2, 1, 1000000.0, 100, 128),\n",
       " (2, 1, 1000000.0, 200, 64),\n",
       " (2, 1, 1000000.0, 200, 128),\n",
       " (2, 2, 100000.0, 100, 64),\n",
       " (2, 2, 100000.0, 100, 128),\n",
       " (2, 2, 100000.0, 200, 64),\n",
       " (2, 2, 100000.0, 200, 128),\n",
       " (2, 2, 1000000.0, 100, 64),\n",
       " (2, 2, 1000000.0, 100, 128),\n",
       " (2, 2, 1000000.0, 200, 64),\n",
       " (2, 2, 1000000.0, 200, 128),\n",
       " (2, 3, 100000.0, 100, 64),\n",
       " (2, 3, 100000.0, 100, 128),\n",
       " (2, 3, 100000.0, 200, 64),\n",
       " (2, 3, 100000.0, 200, 128),\n",
       " (2, 3, 1000000.0, 100, 64),\n",
       " (2, 3, 1000000.0, 100, 128),\n",
       " (2, 3, 1000000.0, 200, 64),\n",
       " (2, 3, 1000000.0, 200, 128)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [[1e-2,1e-1,1,2], ## learning rates\n",
    "          list(range(1,4)), ## ngrams\n",
    "          [1e5,1e6], ## vocab size\n",
    "          [100,200], ## embedding size\n",
    "#          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "# params = [[1e-1,1,2,5], ## learning rates\n",
    "#           list(range(1,2)), ## ngrams\n",
    "#           [1e5], ## vocab size\n",
    "#           [100], ## embedding size\n",
    "#           [100], ## max sentence length\n",
    "#           [64] ## batch size\n",
    "#          ]\n",
    "\n",
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params,\n",
    "                          epochs=5,\n",
    "                          optimizer_name = \"Adam\",\n",
    "                          lemmatize = False):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    # counter for progress\n",
    "    count = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Parameter Combination = \" + str(count+1) + \" / \" + str(len(param_space)))\n",
    "        count = count + 1 \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        NUM_EPOCHS = epochs\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = int(param_comb[2]) # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        #max_sentence_length = int(param_comb[4]) # max sentence length of data loader\n",
    "        BATCH_SIZE = param_comb[4]\n",
    "        \n",
    "        print(\"Learning Rate = \" + str(lr_rate))\n",
    "        print(\"Ngram = \" + str(grams))\n",
    "        print(\"Vocab Size = \" + str(max_vocab_size))\n",
    "        print(\"Embedding Dimension = \" + str(embed_dimension))\n",
    "        #print(\"Max Sentence Length = \" + str(max_sentence_length))\n",
    "        print(\"Batch Size = \" + str(BATCH_SIZE))\n",
    "\n",
    "        # Tokenization\n",
    "        # All tokens are created before the hyperparameter search loop\n",
    "        # Load the tokens here\n",
    "        if lemmatize == True:\n",
    "            grams = str(grams) +\"_lemma\"\n",
    "        \n",
    "        train_data_tokens = pkl.load(open(\"train_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        all_train_tokens = pkl.load(open(\"all_train_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "\n",
    "        val_data_tokens = pkl.load(open(\"val_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        \n",
    "        print(\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "        print(\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "        print(\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "        \n",
    "        # Building Vocabulary\n",
    "        # implicitly gets the max_vocab_size parameter\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                         max_vocab_size=max_vocab_size)\n",
    "        \n",
    "        # Lets check the dictionary by loading random token from it\n",
    "        random_token_id = random.randint(0, len(id2token)-1)\n",
    "        random_token = id2token[random_token_id]\n",
    "        print (\"Token id {} -> token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "        print (\"Token {} -> token id {}\".format(random_token, token2id[random_token]))\n",
    "        \n",
    "        train_data_indices = token2index_dataset(train_data_tokens, \n",
    "                                                 token2id = token2id)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens, \n",
    "                                               token2id = token2id)\n",
    "        # double checking\n",
    "        print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Load training and validation data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)  \n",
    "\n",
    "        # Initialize the N-gram Model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "        \n",
    "        # Both Adam and SGD will be tried\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "        else:\n",
    "            print(\"this optimizer is not implemented yet\")\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        # Caution: Two different optimizers\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "    #for optimizer in optimizers:\n",
    "        print(\"Optimization Start\")\n",
    "        print(optimizer)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Validate every 100 iterations\n",
    "                # Adjust it to accustom changing batch sizes\n",
    "                if i > 0 and i % (50 * (64 / BATCH_SIZE)) == 0:\n",
    "\n",
    "                    # Accuracy Calculations\n",
    "                    train_acc = test_model(train_loader, model)\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                    # Logging\n",
    "                    print('Epoch:[{}/{}],Step:[{}/{}],Training Acc:{},Validation Acc:{}'.format( \n",
    "                               epoch+1, NUM_EPOCHS, \n",
    "                                i+1, len(train_loader), \n",
    "                                train_acc, val_acc))\n",
    "        fin_time = time.time()\n",
    "        print(\"This loop took = \" + str(fin_time-start_time) + \" seconds!\")\n",
    "\n",
    "                      \n",
    "    return val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_val_losses_adam_nolemma = hyperparameter_search(hyperparameter_space = params,\n",
    "#                                          epochs = 5,\n",
    "#                                          optimizer_name = \"Adam\",\n",
    "#                                           lemmatize = False)\n",
    "# pkl.dump(param_val_losses_adam_nolemma, \n",
    "#          open(\"param_val_losses_adam_nolemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_val_losses_adam_lemma = hyperparameter_search(hyperparameter_space = params,\n",
    "#                                          epochs = 5,\n",
    "#                                          optimizer_name = \"Adam\",\n",
    "#                                           lemmatize = True)\n",
    "# pkl.dump(param_val_losses_adam_lemma, \n",
    "#          open(\"param_val_losses_adam_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_val_losses_sgd_nolemma = hyperparameter_search(hyperparameter_space = params,\n",
    "#                                          epochs = 5,\n",
    "#                                          optimizer_name = \"SGD\",\n",
    "#                                           lemmatize = False)\n",
    "# pkl.dump(param_val_losses_sgd_nolemma,\n",
    "#          open(\"param_val_losses_sgd_nolemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 9856 -> token archetype\n",
      "Token archetype -> token id 9856\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:53.475,Validation Acc:54.1\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:54.33,Validation Acc:55.4\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:54.835,Validation Acc:54.98\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:55.55,Validation Acc:56.48\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:56.085,Validation Acc:56.9\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:56.735,Validation Acc:57.62\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:57.58,Validation Acc:58.5\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:58.05,Validation Acc:59.0\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:58.71,Validation Acc:58.94\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:59.25,Validation Acc:59.48\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:59.85,Validation Acc:60.22\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:60.19,Validation Acc:60.66\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:60.625,Validation Acc:60.78\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:60.91,Validation Acc:60.82\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:61.365,Validation Acc:61.48\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:61.435,Validation Acc:61.16\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:61.7,Validation Acc:61.44\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:61.87,Validation Acc:61.56\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:62.26,Validation Acc:61.88\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:62.48,Validation Acc:62.06\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:62.67,Validation Acc:61.94\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:62.6,Validation Acc:62.2\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:62.93,Validation Acc:62.26\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:62.905,Validation Acc:62.22\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:63.05,Validation Acc:62.32\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:63.29,Validation Acc:62.68\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:63.32,Validation Acc:62.52\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:63.185,Validation Acc:62.54\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:63.31,Validation Acc:62.6\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:63.72,Validation Acc:62.98\n",
      "This loop took = 165.9041941165924 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 2 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 41894 -> token heuy\n",
      "Token heuy -> token id 41894\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:53.915,Validation Acc:52.32\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:54.76,Validation Acc:53.18\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:54.905,Validation Acc:53.22\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:55.4,Validation Acc:53.66\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:55.56,Validation Acc:53.74\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:55.925,Validation Acc:54.24\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:55.955,Validation Acc:54.34\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:56.13,Validation Acc:54.44\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:56.82,Validation Acc:55.08\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:56.99,Validation Acc:55.2\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:57.93,Validation Acc:56.12\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:58.22,Validation Acc:56.36\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:58.12,Validation Acc:55.98\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:58.465,Validation Acc:56.66\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:59.215,Validation Acc:57.4\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:59.475,Validation Acc:57.52\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:59.16,Validation Acc:57.06\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:59.635,Validation Acc:57.76\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:59.575,Validation Acc:57.44\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:59.61,Validation Acc:57.56\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:59.995,Validation Acc:57.98\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:60.455,Validation Acc:58.78\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:60.53,Validation Acc:59.14\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:60.68,Validation Acc:59.02\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:60.895,Validation Acc:59.2\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:60.98,Validation Acc:59.7\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:61.18,Validation Acc:59.68\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:61.32,Validation Acc:59.8\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:61.345,Validation Acc:59.58\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:61.4,Validation Acc:59.76\n",
      "This loop took = 151.29188108444214 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 3 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 27090 -> token trotter\n",
      "Token trotter -> token id 27090\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:50.78,Validation Acc:50.38\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:52.615,Validation Acc:51.9\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:54.805,Validation Acc:53.5\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:55.59,Validation Acc:54.18\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:57.24,Validation Acc:56.32\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:58.585,Validation Acc:58.1\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:59.425,Validation Acc:58.84\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:59.915,Validation Acc:59.32\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:60.385,Validation Acc:60.1\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:60.555,Validation Acc:59.04\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:61.1,Validation Acc:59.62\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:61.39,Validation Acc:59.92\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:61.63,Validation Acc:60.84\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:62.185,Validation Acc:60.74\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:62.23,Validation Acc:60.74\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:62.505,Validation Acc:61.04\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:62.705,Validation Acc:61.38\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:63.06,Validation Acc:61.78\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:63.01,Validation Acc:62.02\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:63.05,Validation Acc:62.22\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:63.625,Validation Acc:62.2\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:63.62,Validation Acc:62.58\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:63.78,Validation Acc:62.34\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:63.72,Validation Acc:62.2\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:63.915,Validation Acc:62.62\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:64.07,Validation Acc:62.4\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:64.2,Validation Acc:62.86\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:64.29,Validation Acc:62.96\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:64.36,Validation Acc:62.72\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:64.36,Validation Acc:62.84\n",
      "This loop took = 271.59548115730286 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 4 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 9431 -> token garish\n",
      "Token garish -> token id 9431\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5],Step:[26/157],Training Acc:47.47,Validation Acc:47.76\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:48.385,Validation Acc:49.22\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:49.305,Validation Acc:49.58\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:50.325,Validation Acc:50.52\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:50.97,Validation Acc:51.76\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:52.255,Validation Acc:51.76\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:53.15,Validation Acc:53.6\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:54.075,Validation Acc:54.14\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:54.77,Validation Acc:54.94\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:56.075,Validation Acc:56.1\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:56.185,Validation Acc:56.54\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:57.38,Validation Acc:57.46\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:58.16,Validation Acc:57.86\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:58.615,Validation Acc:58.68\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:59.105,Validation Acc:58.14\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:59.72,Validation Acc:59.18\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:60.3,Validation Acc:59.8\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:60.5,Validation Acc:60.2\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:61.145,Validation Acc:60.28\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:60.93,Validation Acc:60.3\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:61.875,Validation Acc:61.0\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:61.785,Validation Acc:60.78\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:62.355,Validation Acc:61.48\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:62.5,Validation Acc:61.82\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:62.935,Validation Acc:61.86\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:63.19,Validation Acc:62.2\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:62.96,Validation Acc:61.66\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:63.445,Validation Acc:61.9\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:63.19,Validation Acc:61.96\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:63.635,Validation Acc:62.3\n",
      "This loop took = 231.60648703575134 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 5 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 6195 -> token heir\n",
      "Token heir -> token id 6195\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:50.12,Validation Acc:50.2\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:50.59,Validation Acc:50.6\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:50.615,Validation Acc:50.74\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:52.84,Validation Acc:52.6\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:54.56,Validation Acc:53.24\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:55.165,Validation Acc:54.14\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:56.365,Validation Acc:55.16\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:56.53,Validation Acc:54.98\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:57.52,Validation Acc:56.24\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:57.625,Validation Acc:55.94\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:58.185,Validation Acc:56.36\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:58.95,Validation Acc:57.36\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:59.125,Validation Acc:57.58\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:59.59,Validation Acc:57.92\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:59.515,Validation Acc:57.86\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:59.915,Validation Acc:58.38\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:60.61,Validation Acc:59.54\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:61.01,Validation Acc:59.84\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:61.05,Validation Acc:59.72\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:61.165,Validation Acc:59.92\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:61.485,Validation Acc:60.7\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:61.745,Validation Acc:61.0\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:61.885,Validation Acc:61.1\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:61.795,Validation Acc:60.68\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:62.185,Validation Acc:61.18\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:62.14,Validation Acc:60.86\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:62.38,Validation Acc:61.42\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:62.17,Validation Acc:60.92\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:62.535,Validation Acc:61.64\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:62.295,Validation Acc:61.5\n",
      "This loop took = 167.948410987854 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 6 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 54244 -> token mraovich-\n",
      "Token mraovich- -> token id 54244\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:48.6,Validation Acc:49.08\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:49.355,Validation Acc:49.94\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:49.9,Validation Acc:50.12\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:50.215,Validation Acc:50.38\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:50.51,Validation Acc:51.14\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:51.015,Validation Acc:52.28\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:51.71,Validation Acc:52.66\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:52.495,Validation Acc:53.16\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:52.835,Validation Acc:53.7\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:53.545,Validation Acc:54.42\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:53.655,Validation Acc:54.66\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:54.095,Validation Acc:55.22\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:55.385,Validation Acc:55.1\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:55.7,Validation Acc:55.66\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:56.28,Validation Acc:56.12\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:56.18,Validation Acc:56.5\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:56.765,Validation Acc:57.2\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:57.035,Validation Acc:57.2\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:56.945,Validation Acc:57.28\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:57.0,Validation Acc:57.28\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:57.685,Validation Acc:57.8\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:58.525,Validation Acc:58.58\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:58.505,Validation Acc:58.34\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:59.12,Validation Acc:58.94\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:59.265,Validation Acc:59.42\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:59.49,Validation Acc:59.62\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:59.7,Validation Acc:59.78\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:59.825,Validation Acc:59.98\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:59.79,Validation Acc:59.74\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:60.165,Validation Acc:59.88\n",
      "This loop took = 144.0776128768921 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 7 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 33629 -> token klien\n",
      "Token klien -> token id 33629\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:53.23,Validation Acc:51.94\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:54.19,Validation Acc:52.66\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:54.835,Validation Acc:53.3\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:56.865,Validation Acc:56.06\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:57.805,Validation Acc:56.7\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:56.735,Validation Acc:55.2\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:57.87,Validation Acc:56.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[2/5],Step:[101/313],Training Acc:58.13,Validation Acc:56.6\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:60.185,Validation Acc:58.76\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:60.535,Validation Acc:59.28\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:61.065,Validation Acc:59.4\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:61.45,Validation Acc:60.3\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:61.895,Validation Acc:60.82\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:62.085,Validation Acc:61.26\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:62.355,Validation Acc:61.44\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:62.77,Validation Acc:61.82\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:62.92,Validation Acc:61.86\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:63.02,Validation Acc:61.98\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:63.25,Validation Acc:62.42\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:63.6,Validation Acc:62.88\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:63.615,Validation Acc:63.06\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:63.805,Validation Acc:63.1\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:63.81,Validation Acc:63.06\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:64.125,Validation Acc:62.94\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:64.285,Validation Acc:62.92\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:64.185,Validation Acc:63.52\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:64.29,Validation Acc:63.7\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:64.66,Validation Acc:63.74\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:64.61,Validation Acc:63.88\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:64.66,Validation Acc:64.02\n",
      "This loop took = 261.5942268371582 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 8 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4764439\n",
      "Token id 27701 -> token tywker\n",
      "Token tywker -> token id 27701\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:49.955,Validation Acc:50.3\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:50.495,Validation Acc:50.92\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:51.175,Validation Acc:51.2\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:51.835,Validation Acc:52.04\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:53.19,Validation Acc:53.24\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:54.345,Validation Acc:54.12\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:55.335,Validation Acc:55.16\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:55.7,Validation Acc:55.52\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:56.655,Validation Acc:56.28\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:57.565,Validation Acc:57.18\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:57.835,Validation Acc:56.88\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:58.085,Validation Acc:56.94\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:59.3,Validation Acc:58.24\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:59.935,Validation Acc:58.68\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:60.275,Validation Acc:59.2\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:60.8,Validation Acc:59.62\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:60.905,Validation Acc:59.58\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:61.505,Validation Acc:60.1\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:61.915,Validation Acc:60.94\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:62.235,Validation Acc:61.8\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:62.35,Validation Acc:61.76\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:62.585,Validation Acc:61.86\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:62.825,Validation Acc:61.98\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:62.995,Validation Acc:62.06\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:63.145,Validation Acc:61.96\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:63.32,Validation Acc:62.56\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:63.545,Validation Acc:62.46\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:63.06,Validation Acc:62.28\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:63.8,Validation Acc:62.66\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:63.595,Validation Acc:62.84\n",
      "This loop took = 223.1553328037262 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 9 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 87721 -> token dr. morbius\n",
      "Token dr. morbius -> token id 87721\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:50.78,Validation Acc:50.76\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:51.58,Validation Acc:50.88\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:52.145,Validation Acc:52.04\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:53.06,Validation Acc:53.48\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:53.72,Validation Acc:54.12\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:53.995,Validation Acc:54.32\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:54.08,Validation Acc:54.12\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:54.845,Validation Acc:54.52\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:55.535,Validation Acc:56.1\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:55.87,Validation Acc:56.32\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:55.94,Validation Acc:55.5\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:56.285,Validation Acc:55.86\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:56.845,Validation Acc:56.36\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:57.025,Validation Acc:56.54\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:57.075,Validation Acc:56.74\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:57.605,Validation Acc:56.78\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:58.095,Validation Acc:57.1\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:58.125,Validation Acc:57.74\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:58.405,Validation Acc:58.18\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:58.705,Validation Acc:58.0\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:58.66,Validation Acc:58.0\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:58.985,Validation Acc:58.04\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:59.235,Validation Acc:58.42\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:59.45,Validation Acc:58.52\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:59.545,Validation Acc:58.92\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:59.705,Validation Acc:58.6\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:60.055,Validation Acc:59.06\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:60.165,Validation Acc:59.18\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:60.165,Validation Acc:59.26\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:60.465,Validation Acc:59.54\n",
      "This loop took = 220.9469439983368 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 10 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 38997 -> token for black\n",
      "Token for black -> token id 38997\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:50.06,Validation Acc:49.78\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:50.075,Validation Acc:49.76\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:50.095,Validation Acc:49.78\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:50.11,Validation Acc:49.84\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:50.08,Validation Acc:49.92\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:50.15,Validation Acc:49.98\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:50.205,Validation Acc:50.12\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:50.455,Validation Acc:50.32\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:50.985,Validation Acc:50.92\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:51.69,Validation Acc:51.48\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:52.12,Validation Acc:51.8\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:52.28,Validation Acc:52.0\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:52.825,Validation Acc:52.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[3/5],Step:[51/157],Training Acc:54.005,Validation Acc:53.42\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:54.26,Validation Acc:53.86\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:55.505,Validation Acc:54.76\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:55.485,Validation Acc:54.72\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:56.575,Validation Acc:55.54\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:57.345,Validation Acc:56.0\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:57.62,Validation Acc:56.14\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:57.8,Validation Acc:56.64\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:58.03,Validation Acc:56.58\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:58.12,Validation Acc:56.8\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:58.475,Validation Acc:57.16\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:58.615,Validation Acc:57.12\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:58.97,Validation Acc:57.3\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:59.19,Validation Acc:57.58\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:59.19,Validation Acc:57.72\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:59.265,Validation Acc:57.78\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:59.645,Validation Acc:58.3\n",
      "This loop took = 154.65427899360657 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 11 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 3222 -> token -pron- probably\n",
      "Token -pron- probably -> token id 3222\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:51.11,Validation Acc:51.44\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:53.39,Validation Acc:53.44\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:54.85,Validation Acc:54.64\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:54.945,Validation Acc:54.28\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:54.975,Validation Acc:54.4\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:57.435,Validation Acc:56.8\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:58.41,Validation Acc:57.6\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:58.66,Validation Acc:58.12\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:59.59,Validation Acc:59.18\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:60.255,Validation Acc:59.68\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:60.58,Validation Acc:60.26\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:60.335,Validation Acc:59.42\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:61.0,Validation Acc:60.98\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:60.505,Validation Acc:58.56\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:61.65,Validation Acc:60.74\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:62.105,Validation Acc:61.52\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:62.21,Validation Acc:61.56\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:62.255,Validation Acc:61.54\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:62.11,Validation Acc:60.84\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:62.815,Validation Acc:62.06\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:62.84,Validation Acc:61.82\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:62.925,Validation Acc:61.98\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:63.22,Validation Acc:62.22\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:63.25,Validation Acc:62.56\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:63.395,Validation Acc:62.22\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:63.77,Validation Acc:62.48\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:63.16,Validation Acc:62.16\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:63.91,Validation Acc:62.94\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:63.885,Validation Acc:62.8\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:64.07,Validation Acc:63.14\n",
      "This loop took = 298.0258688926697 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 12 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 76349 -> token photograph the\n",
      "Token photograph the -> token id 76349\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:48.25,Validation Acc:48.56\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:48.555,Validation Acc:48.82\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:49.285,Validation Acc:49.5\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:49.93,Validation Acc:50.24\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:50.335,Validation Acc:50.8\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:51.0,Validation Acc:51.38\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:51.805,Validation Acc:52.4\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:53.205,Validation Acc:53.46\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:53.485,Validation Acc:53.98\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:54.135,Validation Acc:54.66\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:54.995,Validation Acc:54.82\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:55.645,Validation Acc:55.7\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:56.28,Validation Acc:55.96\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:56.84,Validation Acc:56.72\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:57.54,Validation Acc:56.76\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:58.025,Validation Acc:57.12\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:58.57,Validation Acc:58.02\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:58.985,Validation Acc:58.4\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:59.44,Validation Acc:58.8\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:59.685,Validation Acc:59.02\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:60.12,Validation Acc:59.6\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:60.395,Validation Acc:59.2\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:60.535,Validation Acc:59.28\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:60.91,Validation Acc:60.52\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:60.85,Validation Acc:60.48\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:61.45,Validation Acc:60.8\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:61.73,Validation Acc:60.92\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:61.9,Validation Acc:61.14\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:62.025,Validation Acc:61.18\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:61.84,Validation Acc:60.64\n",
      "This loop took = 254.29892015457153 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 13 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 364211 -> token great commando\n",
      "Token great commando -> token id 364211\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:48.305,Validation Acc:48.46\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:48.775,Validation Acc:48.7\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:50.06,Validation Acc:49.14\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:50.65,Validation Acc:49.94\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:51.6,Validation Acc:50.36\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:52.2,Validation Acc:51.0\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:53.08,Validation Acc:51.42\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:53.855,Validation Acc:52.4\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:54.515,Validation Acc:53.14\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:55.055,Validation Acc:52.94\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:55.695,Validation Acc:53.64\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:56.405,Validation Acc:54.06\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:56.82,Validation Acc:55.18\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:57.385,Validation Acc:55.48\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:57.835,Validation Acc:55.2\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:58.48,Validation Acc:55.48\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:58.76,Validation Acc:55.94\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:59.13,Validation Acc:56.24\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:59.535,Validation Acc:56.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[4/5],Step:[101/313],Training Acc:59.8,Validation Acc:58.06\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:60.08,Validation Acc:57.3\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:60.505,Validation Acc:57.56\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:60.795,Validation Acc:57.82\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:60.84,Validation Acc:59.16\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:61.115,Validation Acc:59.3\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:61.38,Validation Acc:59.52\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:61.62,Validation Acc:59.66\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:61.87,Validation Acc:59.94\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:61.84,Validation Acc:59.28\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:61.89,Validation Acc:59.1\n",
      "This loop took = 921.68084192276 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 14 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 224103 -> token something sinister\n",
      "Token something sinister -> token id 224103\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:38.965,Validation Acc:38.48\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:48.525,Validation Acc:47.96\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:48.89,Validation Acc:49.1\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:49.025,Validation Acc:48.84\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:49.525,Validation Acc:49.38\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:49.71,Validation Acc:49.54\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:50.035,Validation Acc:49.64\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:50.255,Validation Acc:49.38\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:50.625,Validation Acc:49.48\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:50.905,Validation Acc:49.72\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:51.275,Validation Acc:50.4\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:51.505,Validation Acc:50.28\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:51.84,Validation Acc:50.7\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:52.21,Validation Acc:51.0\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:52.26,Validation Acc:51.22\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:52.585,Validation Acc:51.34\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:52.78,Validation Acc:51.6\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:52.98,Validation Acc:51.9\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:53.18,Validation Acc:52.18\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:53.57,Validation Acc:52.36\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:53.85,Validation Acc:52.44\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:53.87,Validation Acc:52.64\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:54.085,Validation Acc:52.94\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:54.65,Validation Acc:53.54\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:54.855,Validation Acc:54.02\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:55.015,Validation Acc:54.06\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:55.115,Validation Acc:54.36\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:55.405,Validation Acc:54.06\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:55.555,Validation Acc:54.6\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:55.51,Validation Acc:54.32\n",
      "This loop took = 595.3422720432281 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 15 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 211137 -> token fascinating one\n",
      "Token fascinating one -> token id 211137\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:48.99,Validation Acc:49.58\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:49.965,Validation Acc:50.64\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:50.435,Validation Acc:51.12\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:52.075,Validation Acc:52.82\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:53.215,Validation Acc:53.3\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:54.59,Validation Acc:54.56\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:56.28,Validation Acc:56.24\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:56.775,Validation Acc:56.7\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:57.615,Validation Acc:57.38\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:58.705,Validation Acc:58.72\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:59.51,Validation Acc:59.2\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:60.035,Validation Acc:59.3\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:60.64,Validation Acc:59.98\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:61.085,Validation Acc:60.52\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:60.225,Validation Acc:59.5\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:61.45,Validation Acc:60.78\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:62.095,Validation Acc:61.22\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:62.44,Validation Acc:61.42\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:62.665,Validation Acc:61.52\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:62.835,Validation Acc:62.2\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:62.705,Validation Acc:62.28\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:62.555,Validation Acc:61.84\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:63.055,Validation Acc:61.82\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:63.18,Validation Acc:61.88\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:63.615,Validation Acc:62.5\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:63.49,Validation Acc:62.38\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:63.825,Validation Acc:62.58\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:63.895,Validation Acc:62.84\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:63.74,Validation Acc:62.56\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:63.915,Validation Acc:62.6\n",
      "This loop took = 1782.7878201007843 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 16 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 2\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 9508878\n",
      "Token id 653547 -> token likewise great\n",
      "Token likewise great -> token id 653547\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:50.235,Validation Acc:50.86\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:50.67,Validation Acc:51.24\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:51.295,Validation Acc:51.44\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:51.715,Validation Acc:51.42\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:52.135,Validation Acc:51.76\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:52.68,Validation Acc:52.08\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:53.83,Validation Acc:53.42\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:53.97,Validation Acc:52.74\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:54.97,Validation Acc:54.36\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:55.7,Validation Acc:55.4\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:56.155,Validation Acc:55.56\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:56.45,Validation Acc:55.66\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:57.25,Validation Acc:56.1\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:57.475,Validation Acc:57.42\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:57.79,Validation Acc:57.6\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:58.43,Validation Acc:57.5\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:58.465,Validation Acc:58.1\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:59.035,Validation Acc:58.38\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:59.095,Validation Acc:58.82\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:59.68,Validation Acc:59.02\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:60.015,Validation Acc:58.78\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:60.06,Validation Acc:59.58\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:60.345,Validation Acc:59.88\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:60.68,Validation Acc:59.82\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:60.71,Validation Acc:59.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[5/5],Step:[51/157],Training Acc:60.82,Validation Acc:59.68\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:61.26,Validation Acc:60.24\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:61.395,Validation Acc:60.38\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:61.455,Validation Acc:60.54\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:61.63,Validation Acc:60.8\n",
      "This loop took = 1068.7175159454346 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 17 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 31666 -> token see what the\n",
      "Token see what the -> token id 31666\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:49.48,Validation Acc:48.74\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:50.12,Validation Acc:49.64\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:51.01,Validation Acc:50.42\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:52.175,Validation Acc:51.78\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:52.73,Validation Acc:52.34\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:52.92,Validation Acc:52.6\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:54.405,Validation Acc:54.02\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:55.095,Validation Acc:53.96\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:55.415,Validation Acc:54.12\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:55.79,Validation Acc:54.3\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:56.01,Validation Acc:55.22\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:56.44,Validation Acc:55.18\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:56.78,Validation Acc:55.46\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:56.785,Validation Acc:56.12\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:57.41,Validation Acc:56.3\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:57.725,Validation Acc:57.02\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:58.09,Validation Acc:57.28\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:57.905,Validation Acc:56.68\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:58.56,Validation Acc:57.42\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:58.675,Validation Acc:57.3\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:59.145,Validation Acc:58.06\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:58.63,Validation Acc:57.7\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:58.925,Validation Acc:57.88\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:59.735,Validation Acc:58.38\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:59.785,Validation Acc:58.6\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:60.095,Validation Acc:58.66\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:59.83,Validation Acc:58.58\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:59.725,Validation Acc:58.6\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:60.315,Validation Acc:58.92\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:61.605,Validation Acc:60.08\n",
      "This loop took = 264.57349395751953 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 18 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 16948 -> token in all of\n",
      "Token in all of -> token id 16948\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:46.07,Validation Acc:46.38\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:49.39,Validation Acc:49.34\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:49.56,Validation Acc:49.5\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:49.86,Validation Acc:49.94\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:50.345,Validation Acc:50.44\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:50.565,Validation Acc:50.64\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:50.81,Validation Acc:50.88\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:51.11,Validation Acc:51.12\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:51.405,Validation Acc:51.26\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:51.465,Validation Acc:51.46\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:51.875,Validation Acc:51.58\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:52.26,Validation Acc:52.02\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:52.61,Validation Acc:53.1\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:52.95,Validation Acc:53.3\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:52.935,Validation Acc:53.32\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:53.355,Validation Acc:53.64\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:53.6,Validation Acc:53.68\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:53.815,Validation Acc:53.72\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:53.825,Validation Acc:53.44\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:54.25,Validation Acc:54.36\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:54.27,Validation Acc:54.12\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:54.35,Validation Acc:54.42\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:54.84,Validation Acc:54.58\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:54.785,Validation Acc:54.32\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:55.14,Validation Acc:54.68\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:54.955,Validation Acc:54.26\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:55.26,Validation Acc:54.7\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:55.385,Validation Acc:55.06\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:55.645,Validation Acc:55.16\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:55.895,Validation Acc:55.28\n",
      "This loop took = 202.60230803489685 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 19 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 75261 -> token be do what\n",
      "Token be do what -> token id 75261\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:52.01,Validation Acc:52.0\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:54.15,Validation Acc:53.08\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:55.495,Validation Acc:54.4\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:56.635,Validation Acc:55.68\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:57.975,Validation Acc:57.36\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:58.57,Validation Acc:58.32\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:58.195,Validation Acc:57.9\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:59.265,Validation Acc:58.62\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:59.33,Validation Acc:58.32\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:59.605,Validation Acc:58.28\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:60.06,Validation Acc:59.02\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:60.69,Validation Acc:60.04\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:60.535,Validation Acc:59.4\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:60.985,Validation Acc:59.94\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:60.39,Validation Acc:59.64\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:61.905,Validation Acc:60.86\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:61.485,Validation Acc:60.36\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:61.775,Validation Acc:60.46\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:62.775,Validation Acc:62.28\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:62.915,Validation Acc:61.88\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:62.905,Validation Acc:62.4\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:62.665,Validation Acc:61.64\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:62.915,Validation Acc:62.2\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:62.86,Validation Acc:61.68\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:63.02,Validation Acc:62.08\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:62.925,Validation Acc:61.86\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:63.43,Validation Acc:62.84\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:63.655,Validation Acc:63.0\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:63.68,Validation Acc:62.7\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:63.865,Validation Acc:63.36\n",
      "This loop took = 374.5850911140442 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 20 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 99154 -> token and way too\n",
      "Token and way too -> token id 99154\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:53.89,Validation Acc:53.54\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:53.84,Validation Acc:53.6\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:54.065,Validation Acc:53.8\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:54.855,Validation Acc:54.32\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:55.0,Validation Acc:54.52\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:55.185,Validation Acc:54.8\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:55.99,Validation Acc:55.58\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:56.61,Validation Acc:55.98\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:56.445,Validation Acc:55.9\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:56.49,Validation Acc:55.88\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:56.84,Validation Acc:56.36\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:57.2,Validation Acc:56.76\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:58.185,Validation Acc:57.22\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:58.6,Validation Acc:57.6\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:58.795,Validation Acc:58.42\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:58.93,Validation Acc:58.02\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:58.895,Validation Acc:57.92\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:59.395,Validation Acc:58.26\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:59.88,Validation Acc:58.68\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:60.09,Validation Acc:58.86\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:60.0,Validation Acc:59.08\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:60.0,Validation Acc:58.88\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:60.435,Validation Acc:59.1\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:60.46,Validation Acc:60.18\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:60.9,Validation Acc:59.44\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:60.89,Validation Acc:59.72\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:60.985,Validation Acc:59.68\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:61.185,Validation Acc:60.68\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:61.46,Validation Acc:60.34\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:61.59,Validation Acc:60.8\n",
      "This loop took = 304.8678500652313 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 21 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 739052 -> token s**t about viewer\n",
      "Token s**t about viewer -> token id 739052\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:50.27,Validation Acc:50.84\n",
      "Epoch:[1/5],Step:[101/313],Training Acc:50.865,Validation Acc:51.16\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:50.92,Validation Acc:51.68\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:51.215,Validation Acc:51.76\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:51.555,Validation Acc:51.8\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:52.19,Validation Acc:52.66\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:52.605,Validation Acc:52.76\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:52.865,Validation Acc:53.36\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:53.315,Validation Acc:53.4\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:53.895,Validation Acc:54.16\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:54.28,Validation Acc:54.6\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:55.08,Validation Acc:55.32\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:55.285,Validation Acc:55.28\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:55.605,Validation Acc:55.58\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:55.955,Validation Acc:56.14\n",
      "Epoch:[3/5],Step:[201/313],Training Acc:55.935,Validation Acc:55.82\n",
      "Epoch:[3/5],Step:[251/313],Training Acc:56.325,Validation Acc:56.04\n",
      "Epoch:[3/5],Step:[301/313],Training Acc:56.515,Validation Acc:55.72\n",
      "Epoch:[4/5],Step:[51/313],Training Acc:56.855,Validation Acc:56.14\n",
      "Epoch:[4/5],Step:[101/313],Training Acc:57.305,Validation Acc:57.28\n",
      "Epoch:[4/5],Step:[151/313],Training Acc:57.355,Validation Acc:57.24\n",
      "Epoch:[4/5],Step:[201/313],Training Acc:57.58,Validation Acc:57.42\n",
      "Epoch:[4/5],Step:[251/313],Training Acc:57.905,Validation Acc:57.32\n",
      "Epoch:[4/5],Step:[301/313],Training Acc:57.57,Validation Acc:56.3\n",
      "Epoch:[5/5],Step:[51/313],Training Acc:57.99,Validation Acc:56.84\n",
      "Epoch:[5/5],Step:[101/313],Training Acc:58.055,Validation Acc:56.88\n",
      "Epoch:[5/5],Step:[151/313],Training Acc:58.545,Validation Acc:57.48\n",
      "Epoch:[5/5],Step:[201/313],Training Acc:58.75,Validation Acc:57.6\n",
      "Epoch:[5/5],Step:[251/313],Training Acc:58.98,Validation Acc:58.26\n",
      "Epoch:[5/5],Step:[301/313],Training Acc:59.23,Validation Acc:58.24\n",
      "This loop took = 1051.5103831291199 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 22 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 152150 -> token crude but\n",
      "Token crude but -> token id 152150\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[26/157],Training Acc:49.315,Validation Acc:48.24\n",
      "Epoch:[1/5],Step:[51/157],Training Acc:50.795,Validation Acc:50.4\n",
      "Epoch:[1/5],Step:[76/157],Training Acc:51.185,Validation Acc:50.8\n",
      "Epoch:[1/5],Step:[101/157],Training Acc:51.68,Validation Acc:51.04\n",
      "Epoch:[1/5],Step:[126/157],Training Acc:51.815,Validation Acc:51.48\n",
      "Epoch:[1/5],Step:[151/157],Training Acc:51.96,Validation Acc:51.78\n",
      "Epoch:[2/5],Step:[26/157],Training Acc:52.23,Validation Acc:51.86\n",
      "Epoch:[2/5],Step:[51/157],Training Acc:52.45,Validation Acc:52.2\n",
      "Epoch:[2/5],Step:[76/157],Training Acc:52.89,Validation Acc:52.48\n",
      "Epoch:[2/5],Step:[101/157],Training Acc:53.43,Validation Acc:52.92\n",
      "Epoch:[2/5],Step:[126/157],Training Acc:53.575,Validation Acc:53.3\n",
      "Epoch:[2/5],Step:[151/157],Training Acc:54.025,Validation Acc:53.36\n",
      "Epoch:[3/5],Step:[26/157],Training Acc:54.46,Validation Acc:53.94\n",
      "Epoch:[3/5],Step:[51/157],Training Acc:54.65,Validation Acc:54.4\n",
      "Epoch:[3/5],Step:[76/157],Training Acc:55.12,Validation Acc:55.08\n",
      "Epoch:[3/5],Step:[101/157],Training Acc:55.45,Validation Acc:55.3\n",
      "Epoch:[3/5],Step:[126/157],Training Acc:55.55,Validation Acc:55.56\n",
      "Epoch:[3/5],Step:[151/157],Training Acc:55.885,Validation Acc:55.52\n",
      "Epoch:[4/5],Step:[26/157],Training Acc:56.325,Validation Acc:55.78\n",
      "Epoch:[4/5],Step:[51/157],Training Acc:56.535,Validation Acc:56.16\n",
      "Epoch:[4/5],Step:[76/157],Training Acc:56.76,Validation Acc:56.4\n",
      "Epoch:[4/5],Step:[101/157],Training Acc:56.98,Validation Acc:56.48\n",
      "Epoch:[4/5],Step:[126/157],Training Acc:57.255,Validation Acc:56.3\n",
      "Epoch:[4/5],Step:[151/157],Training Acc:57.31,Validation Acc:56.34\n",
      "Epoch:[5/5],Step:[26/157],Training Acc:57.8,Validation Acc:56.68\n",
      "Epoch:[5/5],Step:[51/157],Training Acc:57.845,Validation Acc:56.58\n",
      "Epoch:[5/5],Step:[76/157],Training Acc:57.985,Validation Acc:56.96\n",
      "Epoch:[5/5],Step:[101/157],Training Acc:58.26,Validation Acc:57.08\n",
      "Epoch:[5/5],Step:[126/157],Training Acc:57.955,Validation Acc:56.58\n",
      "Epoch:[5/5],Step:[151/157],Training Acc:57.995,Validation Acc:56.88\n",
      "This loop took = 600.1463623046875 seconds!\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 23 / 96\n",
      "Learning Rate = 0.01\n",
      "Ngram = 3\n",
      "Vocab Size = 1000000\n",
      "Embedding Dimension = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 14233317\n",
      "Token id 593306 -> token show almost no\n",
      "Token show almost no -> token id 593306\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/5],Step:[51/313],Training Acc:50.035,Validation Acc:50.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/5],Step:[101/313],Training Acc:50.195,Validation Acc:50.16\n",
      "Epoch:[1/5],Step:[151/313],Training Acc:51.66,Validation Acc:51.7\n",
      "Epoch:[1/5],Step:[201/313],Training Acc:53.13,Validation Acc:54.04\n",
      "Epoch:[1/5],Step:[251/313],Training Acc:54.89,Validation Acc:55.12\n",
      "Epoch:[1/5],Step:[301/313],Training Acc:56.26,Validation Acc:56.68\n",
      "Epoch:[2/5],Step:[51/313],Training Acc:56.88,Validation Acc:57.56\n",
      "Epoch:[2/5],Step:[101/313],Training Acc:57.48,Validation Acc:58.36\n",
      "Epoch:[2/5],Step:[151/313],Training Acc:59.09,Validation Acc:58.7\n",
      "Epoch:[2/5],Step:[201/313],Training Acc:59.385,Validation Acc:59.08\n",
      "Epoch:[2/5],Step:[251/313],Training Acc:59.78,Validation Acc:59.48\n",
      "Epoch:[2/5],Step:[301/313],Training Acc:60.34,Validation Acc:59.84\n",
      "Epoch:[3/5],Step:[51/313],Training Acc:60.735,Validation Acc:59.84\n",
      "Epoch:[3/5],Step:[101/313],Training Acc:61.26,Validation Acc:60.74\n",
      "Epoch:[3/5],Step:[151/313],Training Acc:61.645,Validation Acc:60.98\n"
     ]
    }
   ],
   "source": [
    "param_val_losses_sgd_lemma = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 5,\n",
    "                                         optimizer_name = \"SGD\",\n",
    "                                          lemmatize = True)\n",
    "pkl.dump(param_val_losses_sgd_lemma,\n",
    "         open(\"param_val_losses_sgd_lemma.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
