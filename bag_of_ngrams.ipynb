{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses bag-of-ngrams approach to sentiment classification using the IMDB review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loc = \"data/imdb_reviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    \"\"\"Reads all .txt files in a folder to a list\"\"\"\n",
    "    \n",
    "    file_list = os.listdir(folder_path)\n",
    "    # for debugging, printing out the folder path and some files in it\n",
    "    print(folder_path)\n",
    "    print(file_list[:10])\n",
    "    \n",
    "    all_reviews = []\n",
    "    for file_path in file_list:\n",
    "        f = open(folder_path + file_path,\"r\")\n",
    "        all_reviews.append(f.readline())\n",
    "        \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_reviews/train/pos/\n",
      "['4715_9.txt', '12390_8.txt', '8329_7.txt', '9063_8.txt', '3092_10.txt', '9865_8.txt', '6639_10.txt', '10460_10.txt', '10331_10.txt', '11606_10.txt']\n",
      "12500\n",
      "data/imdb_reviews/train/neg/\n",
      "['1821_4.txt', '10402_1.txt', '1062_4.txt', '9056_1.txt', '5392_3.txt', '2682_3.txt', '3351_4.txt', '399_2.txt', '10447_1.txt', '10096_1.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/pos/\n",
      "['4715_9.txt', '1930_9.txt', '3205_9.txt', '10186_10.txt', '147_10.txt', '7511_7.txt', '616_10.txt', '10460_10.txt', '3240_9.txt', '1975_9.txt']\n",
      "12500\n",
      "data/imdb_reviews/test/neg/\n",
      "['1821_4.txt', '9487_1.txt', '4604_4.txt', '2828_2.txt', '10890_1.txt', '3351_4.txt', '8070_2.txt', '1027_4.txt', '8248_3.txt', '4290_4.txt']\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_txt_files(folder_path=data_loc+\"train/pos/\")\n",
    "print(len(train_pos))\n",
    "train_neg = read_txt_files(folder_path=data_loc+\"train/neg/\")\n",
    "print(len(train_neg))\n",
    "test_pos = read_txt_files(folder_path=data_loc+\"test/pos/\")\n",
    "print(len(test_pos))\n",
    "test_neg = read_txt_files(folder_path=data_loc+\"test/neg/\")\n",
    "print(len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)\n",
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Positive examples = 12500\n",
      "Train Negative examples = 12500\n",
      "Test Positive examples = 12500\n",
      "Test Negative examples = 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Positive examples = \" + str(len(train_pos)))\n",
    "print(\"Train Negative examples = \" + str(len(train_neg)))\n",
    "print(\"Test Positive examples = \" + str(len(test_pos)))\n",
    "print(\"Test Negative examples = \" + str(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_labels = np.ones((len(train_pos),), dtype=int)\n",
    "train_pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_labels = np.zeros((len(train_neg),), dtype=int)\n",
    "train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_labels = np.concatenate((train_pos_labels,train_neg_labels))\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the labels of the test set for Test Error Measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_labels = np.ones((len(test_pos),), dtype=int)\n",
    "test_neg_labels = np.zeros((len(test_neg),), dtype=int)\n",
    "test_data_labels = np.concatenate((test_pos_labels,test_neg_labels))\n",
    "print(len(test_data_labels))\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_clean = [cleanhtml(x) for x in train_pos]\n",
    "train_neg_clean = [cleanhtml(x) for x in train_neg]\n",
    "\n",
    "test_pos_clean = [cleanhtml(x) for x in test_pos]\n",
    "test_neg_clean = [cleanhtml(x) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sudden Impact is the best of the five Dirty Harry movies. They don't come any leaner and meaner than this as Harry romps through a series of violent clashes, with the bad guys getting their just desserts. Which is just the way I like it. Great story too and ably directed by Clint himself. Excellent entertainment.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing dots & question marks & paranthesis with space\n",
    "\n",
    "It seems that punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"asdasdasds.asdasda\".replace(\".\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_dqmp(review):\n",
    "    \n",
    "#     review = review.replace(\".\",\" \")\n",
    "#     review = review.replace(\"?\",\" \")\n",
    "#     review = review.replace(\")\",\" \")\n",
    "#     review = review.replace(\"(\",\" \")\n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove_dqmp(train_pos_clean[random_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_pos_clean = [remove_dqmp(x) for x in train_pos_clean]\n",
    "# train_neg_clean = [remove_dqmp(x) for x in train_neg_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# This is word tokenizer\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "#     #return [token.text.lower() for token in tokens]\n",
    "    \n",
    "# Modified for n-grams\n",
    "def tokenize(sent, n_gram = 0):\n",
    "    \n",
    "    tokens = tokenizer(sent)\n",
    "    \n",
    "    # unigrams\n",
    "    unigrams = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    output = []\n",
    "    output.extend(unigrams)\n",
    "    \n",
    "    n = 2\n",
    "    while n <= n_gram:\n",
    "        ngram_tokens = [\" \".join(unigrams[x:x+n]) \\\n",
    "                            for x in range(len(unigrams)-n+1)]\n",
    "        output.extend(ngram_tokens)\n",
    "        n = n + 1\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9262\n"
     ]
    }
   ],
   "source": [
    "random_text = np.random.randint(1, high=len(train_pos)-1)\n",
    "print(random_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maybe i identify with this film cause i live in nyc and suffer from bad insomnia but whatever it is, i must praise the filmmaker on a most amazing job. to do what she did with no budget...wow, thats all i can say. really, really good. like no money was spent on this film and it still blew me away. i definitley suggest checking it out if you can. great directing, fantastic score and of course a script that will knock you on your arse. see it.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_clean[random_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'identify', 'with', 'this', 'film', 'cause', 'i', 'live', 'in', 'nyc', 'and', 'suffer', 'from', 'bad', 'insomnia', 'but', 'whatever', 'it', 'is', 'i', 'must', 'praise', 'the', 'filmmaker', 'on', 'a', 'most', 'amazing', 'job', 'to', 'do', 'what', 'she', 'did', 'with', 'no', 'budget', '...', 'wow', 'that', 's', 'all', 'i', 'can', 'say', 'really', 'really', 'good', 'like', 'no', 'money', 'was', 'spent', 'on', 'this', 'film', 'and', 'it', 'still', 'blew', 'me', 'away', 'i', 'definitley', 'suggest', 'checking', 'it', 'out', 'if', 'you', 'can', 'great', 'directing', 'fantastic', 'score', 'and', 'of', 'course', 'a', 'script', 'that', 'will', 'knock', 'you', 'on', 'your', 'arse', 'see', 'it', 'maybe i', 'i identify', 'identify with', 'with this', 'this film', 'film cause', 'cause i', 'i live', 'live in', 'in nyc', 'nyc and', 'and suffer', 'suffer from', 'from bad', 'bad insomnia', 'insomnia but', 'but whatever', 'whatever it', 'it is', 'is i', 'i must', 'must praise', 'praise the', 'the filmmaker', 'filmmaker on', 'on a', 'a most', 'most amazing', 'amazing job', 'job to', 'to do', 'do what', 'what she', 'she did', 'did with', 'with no', 'no budget', 'budget ...', '... wow', 'wow that', 'that s', 's all', 'all i', 'i can', 'can say', 'say really', 'really really', 'really good', 'good like', 'like no', 'no money', 'money was', 'was spent', 'spent on', 'on this', 'this film', 'film and', 'and it', 'it still', 'still blew', 'blew me', 'me away', 'away i', 'i definitley', 'definitley suggest', 'suggest checking', 'checking it', 'it out', 'out if', 'if you', 'you can', 'can great', 'great directing', 'directing fantastic', 'fantastic score', 'score and', 'and of', 'of course', 'course a', 'a script', 'script that', 'that will', 'will knock', 'knock you', 'you on', 'on your', 'your arse', 'arse see', 'see it', 'maybe i identify', 'i identify with', 'identify with this', 'with this film', 'this film cause', 'film cause i', 'cause i live', 'i live in', 'live in nyc', 'in nyc and', 'nyc and suffer', 'and suffer from', 'suffer from bad', 'from bad insomnia', 'bad insomnia but', 'insomnia but whatever', 'but whatever it', 'whatever it is', 'it is i', 'is i must', 'i must praise', 'must praise the', 'praise the filmmaker', 'the filmmaker on', 'filmmaker on a', 'on a most', 'a most amazing', 'most amazing job', 'amazing job to', 'job to do', 'to do what', 'do what she', 'what she did', 'she did with', 'did with no', 'with no budget', 'no budget ...', 'budget ... wow', '... wow that', 'wow that s', 'that s all', 's all i', 'all i can', 'i can say', 'can say really', 'say really really', 'really really good', 'really good like', 'good like no', 'like no money', 'no money was', 'money was spent', 'was spent on', 'spent on this', 'on this film', 'this film and', 'film and it', 'and it still', 'it still blew', 'still blew me', 'blew me away', 'me away i', 'away i definitley', 'i definitley suggest', 'definitley suggest checking', 'suggest checking it', 'checking it out', 'it out if', 'out if you', 'if you can', 'you can great', 'can great directing', 'great directing fantastic', 'directing fantastic score', 'fantastic score and', 'score and of', 'and of course', 'of course a', 'course a script', 'a script that', 'script that will', 'that will knock', 'will knock you', 'knock you on', 'you on your', 'on your arse', 'your arse see', 'arse see it', 'maybe i identify with', 'i identify with this', 'identify with this film', 'with this film cause', 'this film cause i', 'film cause i live', 'cause i live in', 'i live in nyc', 'live in nyc and', 'in nyc and suffer', 'nyc and suffer from', 'and suffer from bad', 'suffer from bad insomnia', 'from bad insomnia but', 'bad insomnia but whatever', 'insomnia but whatever it', 'but whatever it is', 'whatever it is i', 'it is i must', 'is i must praise', 'i must praise the', 'must praise the filmmaker', 'praise the filmmaker on', 'the filmmaker on a', 'filmmaker on a most', 'on a most amazing', 'a most amazing job', 'most amazing job to', 'amazing job to do', 'job to do what', 'to do what she', 'do what she did', 'what she did with', 'she did with no', 'did with no budget', 'with no budget ...', 'no budget ... wow', 'budget ... wow that', '... wow that s', 'wow that s all', 'that s all i', 's all i can', 'all i can say', 'i can say really', 'can say really really', 'say really really good', 'really really good like', 'really good like no', 'good like no money', 'like no money was', 'no money was spent', 'money was spent on', 'was spent on this', 'spent on this film', 'on this film and', 'this film and it', 'film and it still', 'and it still blew', 'it still blew me', 'still blew me away', 'blew me away i', 'me away i definitley', 'away i definitley suggest', 'i definitley suggest checking', 'definitley suggest checking it', 'suggest checking it out', 'checking it out if', 'it out if you', 'out if you can', 'if you can great', 'you can great directing', 'can great directing fantastic', 'great directing fantastic score', 'directing fantastic score and', 'fantastic score and of', 'score and of course', 'and of course a', 'of course a script', 'course a script that', 'a script that will', 'script that will knock', 'that will knock you', 'will knock you on', 'knock you on your', 'you on your arse', 'on your arse see', 'your arse see it']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tokens = tokenize(train_pos_clean[random_text], n_gram = 4)\n",
    "#tokens = tokenize(train_pos_clean[random_text])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "train_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_clean = train_pos_clean + train_neg_clean\n",
    "len(train_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging neg and pos examples - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the order of concatenation\n",
    "test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_clean = test_pos_clean + test_neg_clean\n",
    "len(test_all_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training -> Training + Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be smaller than 25000\n",
    "training_size = 20000\n",
    "\n",
    "assert training_size < 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[15821 15685  4147 ... 18888 20316 23805]\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15821, 15685,  4147, ..., 17207, 22378, 14852])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_index[:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset, n_gram):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], \n",
    "#                                                batch_size=512, \n",
    "#                                                n_threads=4)):\n",
    "\n",
    "    itr = 0\n",
    "    for sample in dataset:\n",
    "        \n",
    "        if itr % 50 == 0:\n",
    "            print(str(itr) + \" / \" + str(len(dataset)))\n",
    "        # unigram version\n",
    "        #tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        # n-gram version\n",
    "        tokens = tokenize(sample,n_gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "        itr = itr + 1\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n"
     ]
    }
   ],
   "source": [
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram = 2)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                     n_gram = 2)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n",
      "0 / 25000\n",
      "50 / 25000\n",
      "100 / 25000\n",
      "150 / 25000\n",
      "200 / 25000\n",
      "250 / 25000\n",
      "300 / 25000\n",
      "350 / 25000\n",
      "400 / 25000\n",
      "450 / 25000\n",
      "500 / 25000\n",
      "550 / 25000\n",
      "600 / 25000\n",
      "650 / 25000\n",
      "700 / 25000\n",
      "750 / 25000\n",
      "800 / 25000\n",
      "850 / 25000\n",
      "900 / 25000\n",
      "950 / 25000\n",
      "1000 / 25000\n",
      "1050 / 25000\n",
      "1100 / 25000\n",
      "1150 / 25000\n",
      "1200 / 25000\n",
      "1250 / 25000\n",
      "1300 / 25000\n",
      "1350 / 25000\n",
      "1400 / 25000\n",
      "1450 / 25000\n",
      "1500 / 25000\n",
      "1550 / 25000\n",
      "1600 / 25000\n",
      "1650 / 25000\n",
      "1700 / 25000\n",
      "1750 / 25000\n",
      "1800 / 25000\n",
      "1850 / 25000\n",
      "1900 / 25000\n",
      "1950 / 25000\n",
      "2000 / 25000\n",
      "2050 / 25000\n",
      "2100 / 25000\n",
      "2150 / 25000\n",
      "2200 / 25000\n",
      "2250 / 25000\n",
      "2300 / 25000\n",
      "2350 / 25000\n",
      "2400 / 25000\n",
      "2450 / 25000\n",
      "2500 / 25000\n",
      "2550 / 25000\n",
      "2600 / 25000\n",
      "2650 / 25000\n",
      "2700 / 25000\n",
      "2750 / 25000\n",
      "2800 / 25000\n",
      "2850 / 25000\n",
      "2900 / 25000\n",
      "2950 / 25000\n",
      "3000 / 25000\n",
      "3050 / 25000\n",
      "3100 / 25000\n",
      "3150 / 25000\n",
      "3200 / 25000\n",
      "3250 / 25000\n",
      "3300 / 25000\n",
      "3350 / 25000\n",
      "3400 / 25000\n",
      "3450 / 25000\n",
      "3500 / 25000\n",
      "3550 / 25000\n",
      "3600 / 25000\n",
      "3650 / 25000\n",
      "3700 / 25000\n",
      "3750 / 25000\n",
      "3800 / 25000\n",
      "3850 / 25000\n",
      "3900 / 25000\n",
      "3950 / 25000\n",
      "4000 / 25000\n",
      "4050 / 25000\n",
      "4100 / 25000\n",
      "4150 / 25000\n",
      "4200 / 25000\n",
      "4250 / 25000\n",
      "4300 / 25000\n",
      "4350 / 25000\n",
      "4400 / 25000\n",
      "4450 / 25000\n",
      "4500 / 25000\n",
      "4550 / 25000\n",
      "4600 / 25000\n",
      "4650 / 25000\n",
      "4700 / 25000\n",
      "4750 / 25000\n",
      "4800 / 25000\n",
      "4850 / 25000\n",
      "4900 / 25000\n",
      "4950 / 25000\n",
      "5000 / 25000\n",
      "5050 / 25000\n",
      "5100 / 25000\n",
      "5150 / 25000\n",
      "5200 / 25000\n",
      "5250 / 25000\n",
      "5300 / 25000\n",
      "5350 / 25000\n",
      "5400 / 25000\n",
      "5450 / 25000\n",
      "5500 / 25000\n",
      "5550 / 25000\n",
      "5600 / 25000\n",
      "5650 / 25000\n",
      "5700 / 25000\n",
      "5750 / 25000\n",
      "5800 / 25000\n",
      "5850 / 25000\n",
      "5900 / 25000\n",
      "5950 / 25000\n",
      "6000 / 25000\n",
      "6050 / 25000\n",
      "6100 / 25000\n",
      "6150 / 25000\n",
      "6200 / 25000\n",
      "6250 / 25000\n",
      "6300 / 25000\n",
      "6350 / 25000\n",
      "6400 / 25000\n",
      "6450 / 25000\n",
      "6500 / 25000\n",
      "6550 / 25000\n",
      "6600 / 25000\n",
      "6650 / 25000\n",
      "6700 / 25000\n",
      "6750 / 25000\n",
      "6800 / 25000\n",
      "6850 / 25000\n",
      "6900 / 25000\n",
      "6950 / 25000\n",
      "7000 / 25000\n",
      "7050 / 25000\n",
      "7100 / 25000\n",
      "7150 / 25000\n",
      "7200 / 25000\n",
      "7250 / 25000\n",
      "7300 / 25000\n",
      "7350 / 25000\n",
      "7400 / 25000\n",
      "7450 / 25000\n",
      "7500 / 25000\n",
      "7550 / 25000\n",
      "7600 / 25000\n",
      "7650 / 25000\n",
      "7700 / 25000\n",
      "7750 / 25000\n",
      "7800 / 25000\n",
      "7850 / 25000\n",
      "7900 / 25000\n",
      "7950 / 25000\n",
      "8000 / 25000\n",
      "8050 / 25000\n",
      "8100 / 25000\n",
      "8150 / 25000\n",
      "8200 / 25000\n",
      "8250 / 25000\n",
      "8300 / 25000\n",
      "8350 / 25000\n",
      "8400 / 25000\n",
      "8450 / 25000\n",
      "8500 / 25000\n",
      "8550 / 25000\n",
      "8600 / 25000\n",
      "8650 / 25000\n",
      "8700 / 25000\n",
      "8750 / 25000\n",
      "8800 / 25000\n",
      "8850 / 25000\n",
      "8900 / 25000\n",
      "8950 / 25000\n",
      "9000 / 25000\n",
      "9050 / 25000\n",
      "9100 / 25000\n",
      "9150 / 25000\n",
      "9200 / 25000\n",
      "9250 / 25000\n",
      "9300 / 25000\n",
      "9350 / 25000\n",
      "9400 / 25000\n",
      "9450 / 25000\n",
      "9500 / 25000\n",
      "9550 / 25000\n",
      "9600 / 25000\n",
      "9650 / 25000\n",
      "9700 / 25000\n",
      "9750 / 25000\n",
      "9800 / 25000\n",
      "9850 / 25000\n",
      "9900 / 25000\n",
      "9950 / 25000\n",
      "10000 / 25000\n",
      "10050 / 25000\n",
      "10100 / 25000\n",
      "10150 / 25000\n",
      "10200 / 25000\n",
      "10250 / 25000\n",
      "10300 / 25000\n",
      "10350 / 25000\n",
      "10400 / 25000\n",
      "10450 / 25000\n",
      "10500 / 25000\n",
      "10550 / 25000\n",
      "10600 / 25000\n",
      "10650 / 25000\n",
      "10700 / 25000\n",
      "10750 / 25000\n",
      "10800 / 25000\n",
      "10850 / 25000\n",
      "10900 / 25000\n",
      "10950 / 25000\n",
      "11000 / 25000\n",
      "11050 / 25000\n",
      "11100 / 25000\n",
      "11150 / 25000\n",
      "11200 / 25000\n",
      "11250 / 25000\n",
      "11300 / 25000\n",
      "11350 / 25000\n",
      "11400 / 25000\n",
      "11450 / 25000\n",
      "11500 / 25000\n",
      "11550 / 25000\n",
      "11600 / 25000\n",
      "11650 / 25000\n",
      "11700 / 25000\n",
      "11750 / 25000\n",
      "11800 / 25000\n",
      "11850 / 25000\n",
      "11900 / 25000\n",
      "11950 / 25000\n",
      "12000 / 25000\n",
      "12050 / 25000\n",
      "12100 / 25000\n",
      "12150 / 25000\n",
      "12200 / 25000\n",
      "12250 / 25000\n",
      "12300 / 25000\n",
      "12350 / 25000\n",
      "12400 / 25000\n",
      "12450 / 25000\n",
      "12500 / 25000\n",
      "12550 / 25000\n",
      "12600 / 25000\n",
      "12650 / 25000\n",
      "12700 / 25000\n",
      "12750 / 25000\n",
      "12800 / 25000\n",
      "12850 / 25000\n",
      "12900 / 25000\n",
      "12950 / 25000\n",
      "13000 / 25000\n",
      "13050 / 25000\n",
      "13100 / 25000\n",
      "13150 / 25000\n",
      "13200 / 25000\n",
      "13250 / 25000\n",
      "13300 / 25000\n",
      "13350 / 25000\n",
      "13400 / 25000\n",
      "13450 / 25000\n",
      "13500 / 25000\n",
      "13550 / 25000\n",
      "13600 / 25000\n",
      "13650 / 25000\n",
      "13700 / 25000\n",
      "13750 / 25000\n",
      "13800 / 25000\n",
      "13850 / 25000\n",
      "13900 / 25000\n",
      "13950 / 25000\n",
      "14000 / 25000\n",
      "14050 / 25000\n",
      "14100 / 25000\n",
      "14150 / 25000\n",
      "14200 / 25000\n",
      "14250 / 25000\n",
      "14300 / 25000\n",
      "14350 / 25000\n",
      "14400 / 25000\n",
      "14450 / 25000\n",
      "14500 / 25000\n",
      "14550 / 25000\n",
      "14600 / 25000\n",
      "14650 / 25000\n",
      "14700 / 25000\n",
      "14750 / 25000\n",
      "14800 / 25000\n",
      "14850 / 25000\n",
      "14900 / 25000\n",
      "14950 / 25000\n",
      "15000 / 25000\n",
      "15050 / 25000\n",
      "15100 / 25000\n",
      "15150 / 25000\n",
      "15200 / 25000\n",
      "15250 / 25000\n",
      "15300 / 25000\n",
      "15350 / 25000\n",
      "15400 / 25000\n",
      "15450 / 25000\n",
      "15500 / 25000\n",
      "15550 / 25000\n",
      "15600 / 25000\n",
      "15650 / 25000\n",
      "15700 / 25000\n",
      "15750 / 25000\n",
      "15800 / 25000\n",
      "15850 / 25000\n",
      "15900 / 25000\n",
      "15950 / 25000\n",
      "16000 / 25000\n",
      "16050 / 25000\n",
      "16100 / 25000\n",
      "16150 / 25000\n",
      "16200 / 25000\n",
      "16250 / 25000\n",
      "16300 / 25000\n",
      "16350 / 25000\n",
      "16400 / 25000\n",
      "16450 / 25000\n",
      "16500 / 25000\n",
      "16550 / 25000\n",
      "16600 / 25000\n",
      "16650 / 25000\n",
      "16700 / 25000\n",
      "16750 / 25000\n",
      "16800 / 25000\n",
      "16850 / 25000\n",
      "16900 / 25000\n",
      "16950 / 25000\n",
      "17000 / 25000\n",
      "17050 / 25000\n",
      "17100 / 25000\n",
      "17150 / 25000\n",
      "17200 / 25000\n",
      "17250 / 25000\n",
      "17300 / 25000\n",
      "17350 / 25000\n",
      "17400 / 25000\n",
      "17450 / 25000\n",
      "17500 / 25000\n",
      "17550 / 25000\n",
      "17600 / 25000\n",
      "17650 / 25000\n",
      "17700 / 25000\n",
      "17750 / 25000\n",
      "17800 / 25000\n",
      "17850 / 25000\n",
      "17900 / 25000\n",
      "17950 / 25000\n",
      "18000 / 25000\n",
      "18050 / 25000\n",
      "18100 / 25000\n",
      "18150 / 25000\n",
      "18200 / 25000\n",
      "18250 / 25000\n",
      "18300 / 25000\n",
      "18350 / 25000\n",
      "18400 / 25000\n",
      "18450 / 25000\n",
      "18500 / 25000\n",
      "18550 / 25000\n",
      "18600 / 25000\n",
      "18650 / 25000\n",
      "18700 / 25000\n",
      "18750 / 25000\n",
      "18800 / 25000\n",
      "18850 / 25000\n",
      "18900 / 25000\n",
      "18950 / 25000\n",
      "19000 / 25000\n",
      "19050 / 25000\n",
      "19100 / 25000\n",
      "19150 / 25000\n",
      "19200 / 25000\n",
      "19250 / 25000\n",
      "19300 / 25000\n",
      "19350 / 25000\n",
      "19400 / 25000\n",
      "19450 / 25000\n",
      "19500 / 25000\n",
      "19550 / 25000\n",
      "19600 / 25000\n",
      "19650 / 25000\n",
      "19700 / 25000\n",
      "19750 / 25000\n",
      "19800 / 25000\n",
      "19850 / 25000\n",
      "19900 / 25000\n",
      "19950 / 25000\n",
      "20000 / 25000\n",
      "20050 / 25000\n",
      "20100 / 25000\n",
      "20150 / 25000\n",
      "20200 / 25000\n",
      "20250 / 25000\n",
      "20300 / 25000\n",
      "20350 / 25000\n",
      "20400 / 25000\n",
      "20450 / 25000\n",
      "20500 / 25000\n",
      "20550 / 25000\n",
      "20600 / 25000\n",
      "20650 / 25000\n",
      "20700 / 25000\n",
      "20750 / 25000\n",
      "20800 / 25000\n",
      "20850 / 25000\n",
      "20900 / 25000\n",
      "20950 / 25000\n",
      "21000 / 25000\n",
      "21050 / 25000\n",
      "21100 / 25000\n",
      "21150 / 25000\n",
      "21200 / 25000\n",
      "21250 / 25000\n",
      "21300 / 25000\n",
      "21350 / 25000\n",
      "21400 / 25000\n",
      "21450 / 25000\n",
      "21500 / 25000\n",
      "21550 / 25000\n",
      "21600 / 25000\n",
      "21650 / 25000\n",
      "21700 / 25000\n",
      "21750 / 25000\n",
      "21800 / 25000\n",
      "21850 / 25000\n",
      "21900 / 25000\n",
      "21950 / 25000\n",
      "22000 / 25000\n",
      "22050 / 25000\n",
      "22100 / 25000\n",
      "22150 / 25000\n",
      "22200 / 25000\n",
      "22250 / 25000\n",
      "22300 / 25000\n",
      "22350 / 25000\n",
      "22400 / 25000\n",
      "22450 / 25000\n",
      "22500 / 25000\n",
      "22550 / 25000\n",
      "22600 / 25000\n",
      "22650 / 25000\n",
      "22700 / 25000\n",
      "22750 / 25000\n",
      "22800 / 25000\n",
      "22850 / 25000\n",
      "22900 / 25000\n",
      "22950 / 25000\n",
      "23000 / 25000\n",
      "23050 / 25000\n",
      "23100 / 25000\n",
      "23150 / 25000\n",
      "23200 / 25000\n",
      "23250 / 25000\n",
      "23300 / 25000\n",
      "23350 / 25000\n",
      "23400 / 25000\n",
      "23450 / 25000\n",
      "23500 / 25000\n",
      "23550 / 25000\n",
      "23600 / 25000\n",
      "23650 / 25000\n",
      "23700 / 25000\n",
      "23750 / 25000\n",
      "23800 / 25000\n",
      "23850 / 25000\n",
      "23900 / 25000\n",
      "23950 / 25000\n",
      "24000 / 25000\n",
      "24050 / 25000\n",
      "24100 / 25000\n",
      "24150 / 25000\n",
      "24200 / 25000\n",
      "24250 / 25000\n",
      "24300 / 25000\n",
      "24350 / 25000\n",
      "24400 / 25000\n",
      "24450 / 25000\n",
      "24500 / 25000\n",
      "24550 / 25000\n",
      "24600 / 25000\n",
      "24650 / 25000\n",
      "24700 / 25000\n",
      "24750 / 25000\n",
      "24800 / 25000\n",
      "24850 / 25000\n",
      "24900 / 25000\n",
      "24950 / 25000\n"
     ]
    }
   ],
   "source": [
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_all_clean,\n",
    "                                      n_gram = 2)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'surely', 'one', 'of', 'the', 'worst', 'films', 'ever', 'made', 'and', 'released', 'by', 'a', 'major', 'hollywood', 'studio', 'the', 'plot', 'is', 'simply', 'stupid', 'the', 'dialog', 'is', 'written', 'in', 'clichs', 'you', 'can', 'complete', 'a', 'great', 'many', 'sentences', 'in', 'the', 'script', 'because', 'of', 'this', 'the', 'acting', 'is', 'ridiculously', 'bad', 'especially', 'that', 'of', 'rod', 'cameron', 'the', 'choreography', 'is', 'silly', 'and', 'wholly', 'unerotic', 'one', 'can', 'only', 'pity', 'the', 'reviewer', 'who', 'saw', '23-year', 'old', 'yvonne', \"'s\", 'dance', 'as', 'sexual', 'it', \"'s\", 'merely', 'very', 'bad', 'choreography', 'the', 'ballet', 'scene', 'in', 'the', 'film', \"'s\", 'beginning', 'is', 'especially', 'ludicrous', 'if', 'you', 'are', 'into', 'bad', 'movies', 'and', 'enjoy', 'laughing', 'at', 'some', 'of', 'hollywood', \"'s\", 'turkeys', 'this', 'is', 'for', 'you', 'i', 'bought', 'the', 'colorized', 'version', 'on', 'vhs', 'making', 'the', 'movie', 'even', 'worse', 'yvonne', \"'s\", 'heavy', 'makeup', 'when', 'colored', 'has', 'her', 'looking', 'like', 'a', 'clown', 'all', 'the', 'time', 'and', 'she', \"'s\", 'the', 'best', 'part', 'of', 'this', 'film', 'what', 'a', 'way', 'to', 'launch', 'a', 'career', 'this is', 'is surely', 'surely one', 'one of', 'of the', 'the worst', 'worst films', 'films ever', 'ever made', 'made and', 'and released', 'released by', 'by a', 'a major', 'major hollywood', 'hollywood studio', 'studio the', 'the plot', 'plot is', 'is simply', 'simply stupid', 'stupid the', 'the dialog', 'dialog is', 'is written', 'written in', 'in clichs', 'clichs you', 'you can', 'can complete', 'complete a', 'a great', 'great many', 'many sentences', 'sentences in', 'in the', 'the script', 'script because', 'because of', 'of this', 'this the', 'the acting', 'acting is', 'is ridiculously', 'ridiculously bad', 'bad especially', 'especially that', 'that of', 'of rod', 'rod cameron', 'cameron the', 'the choreography', 'choreography is', 'is silly', 'silly and', 'and wholly', 'wholly unerotic', 'unerotic one', 'one can', 'can only', 'only pity', 'pity the', 'the reviewer', 'reviewer who', 'who saw', 'saw 23-year', '23-year old', 'old yvonne', \"yvonne 's\", \"'s dance\", 'dance as', 'as sexual', 'sexual it', \"it 's\", \"'s merely\", 'merely very', 'very bad', 'bad choreography', 'choreography the', 'the ballet', 'ballet scene', 'scene in', 'in the', 'the film', \"film 's\", \"'s beginning\", 'beginning is', 'is especially', 'especially ludicrous', 'ludicrous if', 'if you', 'you are', 'are into', 'into bad', 'bad movies', 'movies and', 'and enjoy', 'enjoy laughing', 'laughing at', 'at some', 'some of', 'of hollywood', \"hollywood 's\", \"'s turkeys\", 'turkeys this', 'this is', 'is for', 'for you', 'you i', 'i bought', 'bought the', 'the colorized', 'colorized version', 'version on', 'on vhs', 'vhs making', 'making the', 'the movie', 'movie even', 'even worse', 'worse yvonne', \"yvonne 's\", \"'s heavy\", 'heavy makeup', 'makeup when', 'when colored', 'colored has', 'has her', 'her looking', 'looking like', 'like a', 'a clown', 'clown all', 'all the', 'the time', 'time and', 'and she', \"she 's\", \"'s the\", 'the best', 'best part', 'part of', 'of this', 'this film', 'film what', 'what a', 'a way', 'way to', 'to launch', 'launch a', 'a career'], ['mabel', 'at', 'the', 'wheel', 'is', 'one', 'of', 'those', 'movies', 'with', 'a', 'behind', 'the', 'scenes', 'story', 'that', \"'s\", 'more', 'interesting', 'than', 'the', 'movie', 'itself', 'this', 'was', 'chaplin', \"'s\", 'tenth', 'comedy', 'for', 'keystone', 'during', 'his', 'year', 'of', 'apprenticeship', 'and', 'his', 'first', 'two', 'reeler', 'here', 'he', 'played', 'one', 'of', 'his', 'last', 'out', 'and', 'out', 'villain', 'roles', 'although', 'the', 'feature', 'length', 'tillie', \"'s\", 'punctured', 'romance', 'was', 'yet', 'to', 'come', 'and', 'it', 'also', 'marked', 'one', 'of', 'the', 'last', 'times', 'he', 'would', 'work', 'for', 'a', 'director', 'other', 'than', 'himself', 'in', 'fact', 'chaplin', \"'s\", 'conflicts', 'with', 'director', 'and', 'co', 'star', 'mabel', 'normand', 'almost', 'got', 'him', 'fired', 'from', 'the', 'studio', 'chaplin', 'had', \"n't\", 'gotten', 'along', 'with', 'his', 'earlier', 'directors', 'henry', 'lehrman', 'and', 'george', 'nichols', 'but', 'according', 'to', 'his', 'autobiography', 'having', 'to', 'take', 'direction', 'from', 'a', 'mere', 'girl', 'was', 'the', 'last', 'straw', 'charlie', 'and', 'mabel', 'argued', 'bitterly', 'during', 'the', 'making', 'of', 'this', 'film', 'chaplin', 'was', 'still', 'a', 'newcomer', 'at', 'keystone', 'and', 'his', 'colleagues', 'did', \"n't\", 'know', 'what', 'to', 'make', 'of', 'him', 'but', 'everyone', 'loved', 'mabel', 'producer', 'mack', 'sennett', 'was', 'on', 'the', 'verge', 'of', 'firing', 'chaplin', 'when', 'he', 'learned', 'that', 'the', 'newcomer', \"'s\", 'films', 'were', 'catching', 'on', 'and', 'exhibitors', 'wanted', 'more', 'of', 'them', 'a.s.a.p.', 'so', 'chaplin', 'was', 'promised', 'the', 'chance', 'to', 'direct', 'himself', 'in', 'return', 'for', 'finishing', 'this', 'movie', 'the', 'way', 'mabel', 'wanted', 'it', 'unfortunately', 'none', 'of', 'that', 'drama', 'is', 'visible', 'on', 'screen', 'in', 'mabel', 'at', 'the', 'wheel', 'which', 'looks', 'like', 'typical', 'keystone', 'chaos', 'the', 'story', 'concerns', 'an', 'auto', 'race', 'in', 'which', 'mabel', \"'s\", 'beau', 'harry', 'mccoy', 'is', 'scheduled', 'to', 'compete', 'but', 'wicked', 'charlie', 'and', 'his', 'henchmen', 'abduct', 'the', 'lad', 'and', 'mabel', 'must', 'take', 'the', 'wheel', 'in', 'his', 'place', 'for', 'all', 'the', 'racing', 'around', 'brick', 'hurling', 'and', 'finger', 'biting', 'the', 'film', 'is', 'frankly', 'short', 'on', 'laughs', 'but', 'there', 'are', 'a', 'few', 'points', 'of', 'interest', 'there', \"'s\", 'some', 'good', 'cinematography', 'and', 'editing', 'in', 'the', 'race', 'sequence', 'though', 'there', 'are', \"n't\", 'really', 'any', 'gags', 'just', 'lots', 'of', 'frantic', 'activity', 'chaplin', 'himself', 'looks', 'odd', 'sporting', 'a', 'goat', 'like', 'beard', 'on', 'his', 'chin', 'and', 'wearing', 'the', 'top', 'hat', 'and', 'frock', 'coat', 'he', 'wore', 'in', 'his', 'very', 'first', 'film', 'appearance', 'making', 'a', 'living', 'but', 'the', 'outfit', 'suits', 'the', 'old', 'fashioned', 'villainy', 'he', 'displays', 'throughout', 'at', 'least', 'it', \"'s\", 'novel', 'to', 'watch', 'him', 'play', 'such', 'an', 'uncharacteristic', 'role', 'visible', 'in', 'the', 'stands', 'at', 'the', 'race', 'track', 'are', 'such', 'keystone', 'stalwarts', 'as', 'chester', 'conklin', 'edgar', 'kennedy', 'in', 'a', 'strangely', 'dandified', 'get', 'up', 'and', 'a', 'more', 'characteristic', 'mack', 'sennett', 'spitting', 'tobacco', 'and', 'doing', 'his', 'usual', 'mindless', 'rube', 'routine', 'as', 'a', 'performer', 'sennett', 'was', 'about', 'as', 'subtle', 'as', 'the', 'movies', 'he', 'produced', 'but', 'you', 'have', 'to', 'give', 'the', 'guy', 'credit', 'he', 'knew', 'what', 'people', 'liked', 'these', 'films', 'were', 'hugely', 'popular', 'in', 'their', 'day', 'mack', \"'s\", 'performance', 'does', \"n't\", 'add', 'much', 'to', 'mabel', 'at', 'the', 'wheel', 'but', 'he', 'probably', 'had', 'to', 'be', 'on', 'hand', 'for', 'the', 'filming', 'of', 'this', 'one', 'to', 'make', 'sure', 'his', 'stars', 'did', \"n't\", 'kill', 'each', 'other', 'mabel at', 'at the', 'the wheel', 'wheel is', 'is one', 'one of', 'of those', 'those movies', 'movies with', 'with a', 'a behind', 'behind the', 'the scenes', 'scenes story', 'story that', \"that 's\", \"'s more\", 'more interesting', 'interesting than', 'than the', 'the movie', 'movie itself', 'itself this', 'this was', 'was chaplin', \"chaplin 's\", \"'s tenth\", 'tenth comedy', 'comedy for', 'for keystone', 'keystone during', 'during his', 'his year', 'year of', 'of apprenticeship', 'apprenticeship and', 'and his', 'his first', 'first two', 'two reeler', 'reeler here', 'here he', 'he played', 'played one', 'one of', 'of his', 'his last', 'last out', 'out and', 'and out', 'out villain', 'villain roles', 'roles although', 'although the', 'the feature', 'feature length', 'length tillie', \"tillie 's\", \"'s punctured\", 'punctured romance', 'romance was', 'was yet', 'yet to', 'to come', 'come and', 'and it', 'it also', 'also marked', 'marked one', 'one of', 'of the', 'the last', 'last times', 'times he', 'he would', 'would work', 'work for', 'for a', 'a director', 'director other', 'other than', 'than himself', 'himself in', 'in fact', 'fact chaplin', \"chaplin 's\", \"'s conflicts\", 'conflicts with', 'with director', 'director and', 'and co', 'co star', 'star mabel', 'mabel normand', 'normand almost', 'almost got', 'got him', 'him fired', 'fired from', 'from the', 'the studio', 'studio chaplin', 'chaplin had', \"had n't\", \"n't gotten\", 'gotten along', 'along with', 'with his', 'his earlier', 'earlier directors', 'directors henry', 'henry lehrman', 'lehrman and', 'and george', 'george nichols', 'nichols but', 'but according', 'according to', 'to his', 'his autobiography', 'autobiography having', 'having to', 'to take', 'take direction', 'direction from', 'from a', 'a mere', 'mere girl', 'girl was', 'was the', 'the last', 'last straw', 'straw charlie', 'charlie and', 'and mabel', 'mabel argued', 'argued bitterly', 'bitterly during', 'during the', 'the making', 'making of', 'of this', 'this film', 'film chaplin', 'chaplin was', 'was still', 'still a', 'a newcomer', 'newcomer at', 'at keystone', 'keystone and', 'and his', 'his colleagues', 'colleagues did', \"did n't\", \"n't know\", 'know what', 'what to', 'to make', 'make of', 'of him', 'him but', 'but everyone', 'everyone loved', 'loved mabel', 'mabel producer', 'producer mack', 'mack sennett', 'sennett was', 'was on', 'on the', 'the verge', 'verge of', 'of firing', 'firing chaplin', 'chaplin when', 'when he', 'he learned', 'learned that', 'that the', 'the newcomer', \"newcomer 's\", \"'s films\", 'films were', 'were catching', 'catching on', 'on and', 'and exhibitors', 'exhibitors wanted', 'wanted more', 'more of', 'of them', 'them a.s.a.p.', 'a.s.a.p. so', 'so chaplin', 'chaplin was', 'was promised', 'promised the', 'the chance', 'chance to', 'to direct', 'direct himself', 'himself in', 'in return', 'return for', 'for finishing', 'finishing this', 'this movie', 'movie the', 'the way', 'way mabel', 'mabel wanted', 'wanted it', 'it unfortunately', 'unfortunately none', 'none of', 'of that', 'that drama', 'drama is', 'is visible', 'visible on', 'on screen', 'screen in', 'in mabel', 'mabel at', 'at the', 'the wheel', 'wheel which', 'which looks', 'looks like', 'like typical', 'typical keystone', 'keystone chaos', 'chaos the', 'the story', 'story concerns', 'concerns an', 'an auto', 'auto race', 'race in', 'in which', 'which mabel', \"mabel 's\", \"'s beau\", 'beau harry', 'harry mccoy', 'mccoy is', 'is scheduled', 'scheduled to', 'to compete', 'compete but', 'but wicked', 'wicked charlie', 'charlie and', 'and his', 'his henchmen', 'henchmen abduct', 'abduct the', 'the lad', 'lad and', 'and mabel', 'mabel must', 'must take', 'take the', 'the wheel', 'wheel in', 'in his', 'his place', 'place for', 'for all', 'all the', 'the racing', 'racing around', 'around brick', 'brick hurling', 'hurling and', 'and finger', 'finger biting', 'biting the', 'the film', 'film is', 'is frankly', 'frankly short', 'short on', 'on laughs', 'laughs but', 'but there', 'there are', 'are a', 'a few', 'few points', 'points of', 'of interest', 'interest there', \"there 's\", \"'s some\", 'some good', 'good cinematography', 'cinematography and', 'and editing', 'editing in', 'in the', 'the race', 'race sequence', 'sequence though', 'though there', 'there are', \"are n't\", \"n't really\", 'really any', 'any gags', 'gags just', 'just lots', 'lots of', 'of frantic', 'frantic activity', 'activity chaplin', 'chaplin himself', 'himself looks', 'looks odd', 'odd sporting', 'sporting a', 'a goat', 'goat like', 'like beard', 'beard on', 'on his', 'his chin', 'chin and', 'and wearing', 'wearing the', 'the top', 'top hat', 'hat and', 'and frock', 'frock coat', 'coat he', 'he wore', 'wore in', 'in his', 'his very', 'very first', 'first film', 'film appearance', 'appearance making', 'making a', 'a living', 'living but', 'but the', 'the outfit', 'outfit suits', 'suits the', 'the old', 'old fashioned', 'fashioned villainy', 'villainy he', 'he displays', 'displays throughout', 'throughout at', 'at least', 'least it', \"it 's\", \"'s novel\", 'novel to', 'to watch', 'watch him', 'him play', 'play such', 'such an', 'an uncharacteristic', 'uncharacteristic role', 'role visible', 'visible in', 'in the', 'the stands', 'stands at', 'at the', 'the race', 'race track', 'track are', 'are such', 'such keystone', 'keystone stalwarts', 'stalwarts as', 'as chester', 'chester conklin', 'conklin edgar', 'edgar kennedy', 'kennedy in', 'in a', 'a strangely', 'strangely dandified', 'dandified get', 'get up', 'up and', 'and a', 'a more', 'more characteristic', 'characteristic mack', 'mack sennett', 'sennett spitting', 'spitting tobacco', 'tobacco and', 'and doing', 'doing his', 'his usual', 'usual mindless', 'mindless rube', 'rube routine', 'routine as', 'as a', 'a performer', 'performer sennett', 'sennett was', 'was about', 'about as', 'as subtle', 'subtle as', 'as the', 'the movies', 'movies he', 'he produced', 'produced but', 'but you', 'you have', 'have to', 'to give', 'give the', 'the guy', 'guy credit', 'credit he', 'he knew', 'knew what', 'what people', 'people liked', 'liked these', 'these films', 'films were', 'were hugely', 'hugely popular', 'popular in', 'in their', 'their day', 'day mack', \"mack 's\", \"'s performance\", 'performance does', \"does n't\", \"n't add\", 'add much', 'much to', 'to mabel', 'mabel at', 'at the', 'the wheel', 'wheel but', 'but he', 'he probably', 'probably had', 'had to', 'to be', 'be on', 'on hand', 'hand for', 'for the', 'the filming', 'filming of', 'of this', 'this one', 'one to', 'to make', 'make sure', 'sure his', 'his stars', 'stars did', \"did n't\", \"n't kill\", 'kill each', 'each other']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'surely', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(all_train_tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove blank space tokens\n",
    "\n",
    "In the above tokenization, some blankspace strings were observed, thus this section adresses that by deleting them from the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blankspaces = [\" \",\"  \",\"   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def remove_blankspaces(review):\n",
    "    \n",
    "#     review = [x for x in review if x not in blankspaces] \n",
    "    \n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(remove_blankspaces(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data_tokens_clean = [remove_blankspaces(token) for token in train_data_tokens]\n",
    "# len(train_data_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train_tokens_clean = remove_blankspaces(all_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9538806"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289344"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,vocab_size=max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 8255 ; token my money\n",
      "Token my money; token id 8255\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's \n",
    "    readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), \n",
    "            torch.LongTensor(length_list), \n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDBDataset(train_data_indices, training_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, validation_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_data_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgrams(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgrams classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfNgrams(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "## try both sgd and adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [51/625], Training Acc: 51.965,Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [101/625], Training Acc: 69.47,Validation Acc: 67.8\n",
      "Epoch: [1/10], Step: [151/625], Training Acc: 81.755,Validation Acc: 80.14\n",
      "Epoch: [1/10], Step: [201/625], Training Acc: 84.655,Validation Acc: 82.76\n",
      "Epoch: [1/10], Step: [251/625], Training Acc: 86.205,Validation Acc: 84.12\n",
      "Epoch: [1/10], Step: [301/625], Training Acc: 87.52,Validation Acc: 84.44\n",
      "Epoch: [1/10], Step: [351/625], Training Acc: 88.47,Validation Acc: 85.52\n",
      "Epoch: [1/10], Step: [401/625], Training Acc: 88.695,Validation Acc: 85.42\n",
      "Epoch: [1/10], Step: [451/625], Training Acc: 88.65,Validation Acc: 85.02\n",
      "Epoch: [1/10], Step: [501/625], Training Acc: 90.575,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [551/625], Training Acc: 90.955,Validation Acc: 86.12\n",
      "Epoch: [1/10], Step: [601/625], Training Acc: 91.545,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [51/625], Training Acc: 91.66,Validation Acc: 86.42\n",
      "Epoch: [2/10], Step: [101/625], Training Acc: 91.36,Validation Acc: 86.16\n",
      "Epoch: [2/10], Step: [151/625], Training Acc: 91.815,Validation Acc: 86.7\n",
      "Epoch: [2/10], Step: [201/625], Training Acc: 92.165,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [251/625], Training Acc: 92.52,Validation Acc: 86.56\n",
      "Epoch: [2/10], Step: [301/625], Training Acc: 91.02,Validation Acc: 84.94\n",
      "Epoch: [2/10], Step: [351/625], Training Acc: 90.405,Validation Acc: 84.7\n",
      "Epoch: [2/10], Step: [401/625], Training Acc: 92.965,Validation Acc: 86.8\n",
      "Epoch: [2/10], Step: [451/625], Training Acc: 93.18,Validation Acc: 86.64\n",
      "Epoch: [2/10], Step: [501/625], Training Acc: 92.6,Validation Acc: 85.84\n",
      "Epoch: [2/10], Step: [551/625], Training Acc: 93.67,Validation Acc: 86.9\n",
      "Epoch: [2/10], Step: [601/625], Training Acc: 94.16,Validation Acc: 86.9\n",
      "Epoch: [3/10], Step: [51/625], Training Acc: 94.08,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [101/625], Training Acc: 94.445,Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [151/625], Training Acc: 94.335,Validation Acc: 86.64\n",
      "Epoch: [3/10], Step: [201/625], Training Acc: 93.075,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [251/625], Training Acc: 94.62,Validation Acc: 86.42\n",
      "Epoch: [3/10], Step: [301/625], Training Acc: 94.36,Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [351/625], Training Acc: 94.85,Validation Acc: 86.4\n",
      "Epoch: [3/10], Step: [401/625], Training Acc: 94.705,Validation Acc: 85.98\n",
      "Epoch: [3/10], Step: [451/625], Training Acc: 95.19,Validation Acc: 85.66\n",
      "Epoch: [3/10], Step: [501/625], Training Acc: 95.215,Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [551/625], Training Acc: 94.475,Validation Acc: 85.5\n",
      "Epoch: [3/10], Step: [601/625], Training Acc: 95.13,Validation Acc: 85.92\n",
      "Epoch: [4/10], Step: [51/625], Training Acc: 94.39,Validation Acc: 85.26\n",
      "Epoch: [4/10], Step: [101/625], Training Acc: 95.16,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [151/625], Training Acc: 95.31,Validation Acc: 85.86\n",
      "Epoch: [4/10], Step: [201/625], Training Acc: 94.805,Validation Acc: 85.22\n",
      "Epoch: [4/10], Step: [251/625], Training Acc: 95.23,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [301/625], Training Acc: 95.89,Validation Acc: 85.3\n",
      "Epoch: [4/10], Step: [351/625], Training Acc: 95.895,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [401/625], Training Acc: 95.84,Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [451/625], Training Acc: 96.21,Validation Acc: 85.18\n",
      "Epoch: [4/10], Step: [501/625], Training Acc: 96.0,Validation Acc: 85.78\n",
      "Epoch: [4/10], Step: [551/625], Training Acc: 95.8,Validation Acc: 85.42\n",
      "Epoch: [4/10], Step: [601/625], Training Acc: 96.345,Validation Acc: 85.62\n",
      "Epoch: [5/10], Step: [51/625], Training Acc: 96.41,Validation Acc: 85.5\n",
      "Epoch: [5/10], Step: [101/625], Training Acc: 96.415,Validation Acc: 85.32\n",
      "Epoch: [5/10], Step: [151/625], Training Acc: 96.14,Validation Acc: 84.98\n",
      "Epoch: [5/10], Step: [201/625], Training Acc: 95.265,Validation Acc: 84.26\n",
      "Epoch: [5/10], Step: [251/625], Training Acc: 95.785,Validation Acc: 84.56\n",
      "Epoch: [5/10], Step: [301/625], Training Acc: 96.415,Validation Acc: 85.36\n",
      "Epoch: [5/10], Step: [351/625], Training Acc: 96.61,Validation Acc: 85.48\n",
      "Epoch: [5/10], Step: [401/625], Training Acc: 95.835,Validation Acc: 84.32\n",
      "Epoch: [5/10], Step: [451/625], Training Acc: 96.675,Validation Acc: 85.12\n",
      "Epoch: [5/10], Step: [501/625], Training Acc: 96.98,Validation Acc: 85.3\n",
      "Epoch: [5/10], Step: [551/625], Training Acc: 96.965,Validation Acc: 85.08\n",
      "Epoch: [5/10], Step: [601/625], Training Acc: 96.755,Validation Acc: 84.64\n",
      "Epoch: [6/10], Step: [51/625], Training Acc: 97.115,Validation Acc: 85.04\n",
      "Epoch: [6/10], Step: [101/625], Training Acc: 97.185,Validation Acc: 84.78\n",
      "Epoch: [6/10], Step: [151/625], Training Acc: 96.855,Validation Acc: 84.54\n",
      "Epoch: [6/10], Step: [201/625], Training Acc: 96.615,Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [251/625], Training Acc: 95.92,Validation Acc: 83.54\n",
      "Epoch: [6/10], Step: [301/625], Training Acc: 96.895,Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [351/625], Training Acc: 96.995,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [401/625], Training Acc: 97.005,Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [451/625], Training Acc: 96.785,Validation Acc: 84.26\n",
      "Epoch: [6/10], Step: [501/625], Training Acc: 97.03,Validation Acc: 84.74\n",
      "Epoch: [6/10], Step: [551/625], Training Acc: 95.895,Validation Acc: 83.78\n",
      "Epoch: [6/10], Step: [601/625], Training Acc: 96.885,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [51/625], Training Acc: 96.99,Validation Acc: 84.42\n",
      "Epoch: [7/10], Step: [101/625], Training Acc: 97.6,Validation Acc: 84.66\n",
      "Epoch: [7/10], Step: [151/625], Training Acc: 97.25,Validation Acc: 84.4\n",
      "Epoch: [7/10], Step: [201/625], Training Acc: 97.48,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [251/625], Training Acc: 97.555,Validation Acc: 84.32\n",
      "Epoch: [7/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 84.54\n",
      "Epoch: [7/10], Step: [351/625], Training Acc: 96.79,Validation Acc: 83.8\n",
      "Epoch: [7/10], Step: [401/625], Training Acc: 97.3,Validation Acc: 83.76\n",
      "Epoch: [7/10], Step: [451/625], Training Acc: 97.43,Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [501/625], Training Acc: 97.525,Validation Acc: 84.22\n",
      "Epoch: [7/10], Step: [551/625], Training Acc: 97.67,Validation Acc: 84.44\n",
      "Epoch: [7/10], Step: [601/625], Training Acc: 97.295,Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [51/625], Training Acc: 97.83,Validation Acc: 84.56\n",
      "Epoch: [8/10], Step: [101/625], Training Acc: 97.805,Validation Acc: 84.52\n",
      "Epoch: [8/10], Step: [151/625], Training Acc: 97.855,Validation Acc: 84.18\n",
      "Epoch: [8/10], Step: [201/625], Training Acc: 97.81,Validation Acc: 84.32\n",
      "Epoch: [8/10], Step: [251/625], Training Acc: 97.655,Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [301/625], Training Acc: 97.075,Validation Acc: 84.06\n",
      "Epoch: [8/10], Step: [351/625], Training Acc: 97.505,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [401/625], Training Acc: 97.59,Validation Acc: 83.98\n",
      "Epoch: [8/10], Step: [451/625], Training Acc: 97.46,Validation Acc: 83.82\n",
      "Epoch: [8/10], Step: [501/625], Training Acc: 97.655,Validation Acc: 83.9\n",
      "Epoch: [8/10], Step: [551/625], Training Acc: 96.37,Validation Acc: 82.9\n",
      "Epoch: [8/10], Step: [601/625], Training Acc: 96.815,Validation Acc: 83.08\n",
      "Epoch: [9/10], Step: [51/625], Training Acc: 97.91,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [101/625], Training Acc: 97.89,Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [151/625], Training Acc: 97.805,Validation Acc: 83.56\n",
      "Epoch: [9/10], Step: [201/625], Training Acc: 97.41,Validation Acc: 83.74\n",
      "Epoch: [9/10], Step: [251/625], Training Acc: 97.95,Validation Acc: 83.98\n",
      "Epoch: [9/10], Step: [301/625], Training Acc: 97.44,Validation Acc: 83.68\n",
      "Epoch: [9/10], Step: [351/625], Training Acc: 97.85,Validation Acc: 83.92\n",
      "Epoch: [9/10], Step: [401/625], Training Acc: 98.08,Validation Acc: 83.64\n",
      "Epoch: [9/10], Step: [451/625], Training Acc: 98.22,Validation Acc: 83.48\n",
      "Epoch: [9/10], Step: [501/625], Training Acc: 97.505,Validation Acc: 83.3\n",
      "Epoch: [9/10], Step: [551/625], Training Acc: 98.14,Validation Acc: 83.62\n",
      "Epoch: [9/10], Step: [601/625], Training Acc: 97.92,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [51/625], Training Acc: 98.1,Validation Acc: 83.78\n",
      "Epoch: [10/10], Step: [101/625], Training Acc: 96.255,Validation Acc: 82.4\n",
      "Epoch: [10/10], Step: [151/625], Training Acc: 97.645,Validation Acc: 83.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [201/625], Training Acc: 98.11,Validation Acc: 83.42\n",
      "Epoch: [10/10], Step: [251/625], Training Acc: 96.115,Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [301/625], Training Acc: 97.575,Validation Acc: 83.18\n",
      "Epoch: [10/10], Step: [351/625], Training Acc: 98.245,Validation Acc: 83.76\n",
      "Epoch: [10/10], Step: [401/625], Training Acc: 98.13,Validation Acc: 83.66\n",
      "Epoch: [10/10], Step: [451/625], Training Acc: 98.185,Validation Acc: 83.58\n",
      "Epoch: [10/10], Step: [501/625], Training Acc: 98.445,Validation Acc: 83.36\n",
      "Epoch: [10/10], Step: [551/625], Training Acc: 97.95,Validation Acc: 82.54\n",
      "Epoch: [10/10], Step: [601/625], Training Acc: 97.66,Validation Acc: 83.54\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # check training score every 100 iterations\n",
    "        ## validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {},Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, \n",
    "                len(train_loader), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we are going to try to optimize are the following:\n",
    "\n",
    "* n-gram max length\n",
    "* optimizer choice\n",
    "* embedding size\n",
    "* vocab size\n",
    "* learning rate of the optimizer\n",
    "\n",
    "And maybe increase the batch size to speed up the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = [torch.optim.Adam(model.parameters(), \n",
    "                               lr=learning_rate),             \n",
    "              torch.optim.SGD(model.parameters(), \n",
    "                              lr=learning_rate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[14269 16316  4649 ...  8312 14965 23410]\n",
      "20000\n",
      "20000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(len(train_all_clean))\n",
    "print(len(shuffled_index))\n",
    "print(shuffled_index)\n",
    "\n",
    "shuffled_index[:training_size]\n",
    "\n",
    "training_all_clean = [train_all_clean[i] for i in shuffled_index[:training_size]]\n",
    "training_labels = [train_data_labels[i] for i in shuffled_index[:training_size]]\n",
    "print(len(training_all_clean))\n",
    "print(len(training_labels))\n",
    "\n",
    "validation_all_clean = [train_all_clean[i] for i in shuffled_index[training_size:]]\n",
    "validation_labels = [train_data_labels[i] for i in shuffled_index[training_size:]]\n",
    "print(len(validation_all_clean))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 10000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all ngram tokens for easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams = params[1]\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 20000\n",
      "50 / 20000\n",
      "100 / 20000\n",
      "150 / 20000\n",
      "200 / 20000\n",
      "250 / 20000\n",
      "300 / 20000\n",
      "350 / 20000\n",
      "400 / 20000\n",
      "450 / 20000\n",
      "500 / 20000\n",
      "550 / 20000\n",
      "600 / 20000\n",
      "650 / 20000\n",
      "700 / 20000\n",
      "750 / 20000\n",
      "800 / 20000\n",
      "850 / 20000\n",
      "900 / 20000\n",
      "950 / 20000\n",
      "1000 / 20000\n",
      "1050 / 20000\n",
      "1100 / 20000\n",
      "1150 / 20000\n",
      "1200 / 20000\n",
      "1250 / 20000\n",
      "1300 / 20000\n",
      "1350 / 20000\n",
      "1400 / 20000\n",
      "1450 / 20000\n",
      "1500 / 20000\n",
      "1550 / 20000\n",
      "1600 / 20000\n",
      "1650 / 20000\n",
      "1700 / 20000\n",
      "1750 / 20000\n",
      "1800 / 20000\n",
      "1850 / 20000\n",
      "1900 / 20000\n",
      "1950 / 20000\n",
      "2000 / 20000\n",
      "2050 / 20000\n",
      "2100 / 20000\n",
      "2150 / 20000\n",
      "2200 / 20000\n",
      "2250 / 20000\n",
      "2300 / 20000\n",
      "2350 / 20000\n",
      "2400 / 20000\n",
      "2450 / 20000\n",
      "2500 / 20000\n",
      "2550 / 20000\n",
      "2600 / 20000\n",
      "2650 / 20000\n",
      "2700 / 20000\n",
      "2750 / 20000\n",
      "2800 / 20000\n",
      "2850 / 20000\n",
      "2900 / 20000\n",
      "2950 / 20000\n",
      "3000 / 20000\n",
      "3050 / 20000\n",
      "3100 / 20000\n",
      "3150 / 20000\n",
      "3200 / 20000\n",
      "3250 / 20000\n",
      "3300 / 20000\n",
      "3350 / 20000\n",
      "3400 / 20000\n",
      "3450 / 20000\n",
      "3500 / 20000\n",
      "3550 / 20000\n",
      "3600 / 20000\n",
      "3650 / 20000\n",
      "3700 / 20000\n",
      "3750 / 20000\n",
      "3800 / 20000\n",
      "3850 / 20000\n",
      "3900 / 20000\n",
      "3950 / 20000\n",
      "4000 / 20000\n",
      "4050 / 20000\n",
      "4100 / 20000\n",
      "4150 / 20000\n",
      "4200 / 20000\n",
      "4250 / 20000\n",
      "4300 / 20000\n",
      "4350 / 20000\n",
      "4400 / 20000\n",
      "4450 / 20000\n",
      "4500 / 20000\n",
      "4550 / 20000\n",
      "4600 / 20000\n",
      "4650 / 20000\n",
      "4700 / 20000\n",
      "4750 / 20000\n",
      "4800 / 20000\n",
      "4850 / 20000\n",
      "4900 / 20000\n",
      "4950 / 20000\n",
      "5000 / 20000\n",
      "5050 / 20000\n",
      "5100 / 20000\n",
      "5150 / 20000\n",
      "5200 / 20000\n",
      "5250 / 20000\n",
      "5300 / 20000\n",
      "5350 / 20000\n",
      "5400 / 20000\n",
      "5450 / 20000\n",
      "5500 / 20000\n",
      "5550 / 20000\n",
      "5600 / 20000\n",
      "5650 / 20000\n",
      "5700 / 20000\n",
      "5750 / 20000\n",
      "5800 / 20000\n",
      "5850 / 20000\n",
      "5900 / 20000\n",
      "5950 / 20000\n",
      "6000 / 20000\n",
      "6050 / 20000\n",
      "6100 / 20000\n",
      "6150 / 20000\n",
      "6200 / 20000\n",
      "6250 / 20000\n",
      "6300 / 20000\n",
      "6350 / 20000\n",
      "6400 / 20000\n",
      "6450 / 20000\n",
      "6500 / 20000\n",
      "6550 / 20000\n",
      "6600 / 20000\n",
      "6650 / 20000\n",
      "6700 / 20000\n",
      "6750 / 20000\n",
      "6800 / 20000\n",
      "6850 / 20000\n",
      "6900 / 20000\n",
      "6950 / 20000\n",
      "7000 / 20000\n",
      "7050 / 20000\n",
      "7100 / 20000\n",
      "7150 / 20000\n",
      "7200 / 20000\n",
      "7250 / 20000\n",
      "7300 / 20000\n",
      "7350 / 20000\n",
      "7400 / 20000\n",
      "7450 / 20000\n",
      "7500 / 20000\n",
      "7550 / 20000\n",
      "7600 / 20000\n",
      "7650 / 20000\n",
      "7700 / 20000\n",
      "7750 / 20000\n",
      "7800 / 20000\n",
      "7850 / 20000\n",
      "7900 / 20000\n",
      "7950 / 20000\n",
      "8000 / 20000\n",
      "8050 / 20000\n",
      "8100 / 20000\n",
      "8150 / 20000\n",
      "8200 / 20000\n",
      "8250 / 20000\n",
      "8300 / 20000\n",
      "8350 / 20000\n",
      "8400 / 20000\n",
      "8450 / 20000\n",
      "8500 / 20000\n",
      "8550 / 20000\n",
      "8600 / 20000\n",
      "8650 / 20000\n",
      "8700 / 20000\n",
      "8750 / 20000\n",
      "8800 / 20000\n",
      "8850 / 20000\n",
      "8900 / 20000\n",
      "8950 / 20000\n",
      "9000 / 20000\n",
      "9050 / 20000\n",
      "9100 / 20000\n",
      "9150 / 20000\n",
      "9200 / 20000\n",
      "9250 / 20000\n",
      "9300 / 20000\n",
      "9350 / 20000\n",
      "9400 / 20000\n",
      "9450 / 20000\n",
      "9500 / 20000\n",
      "9550 / 20000\n",
      "9600 / 20000\n",
      "9650 / 20000\n",
      "9700 / 20000\n",
      "9750 / 20000\n",
      "9800 / 20000\n",
      "9850 / 20000\n",
      "9900 / 20000\n",
      "9950 / 20000\n",
      "10000 / 20000\n",
      "10050 / 20000\n",
      "10100 / 20000\n",
      "10150 / 20000\n",
      "10200 / 20000\n",
      "10250 / 20000\n",
      "10300 / 20000\n",
      "10350 / 20000\n",
      "10400 / 20000\n",
      "10450 / 20000\n",
      "10500 / 20000\n",
      "10550 / 20000\n",
      "10600 / 20000\n",
      "10650 / 20000\n",
      "10700 / 20000\n",
      "10750 / 20000\n",
      "10800 / 20000\n",
      "10850 / 20000\n",
      "10900 / 20000\n",
      "10950 / 20000\n",
      "11000 / 20000\n",
      "11050 / 20000\n",
      "11100 / 20000\n",
      "11150 / 20000\n",
      "11200 / 20000\n",
      "11250 / 20000\n",
      "11300 / 20000\n",
      "11350 / 20000\n",
      "11400 / 20000\n",
      "11450 / 20000\n",
      "11500 / 20000\n",
      "11550 / 20000\n",
      "11600 / 20000\n",
      "11650 / 20000\n",
      "11700 / 20000\n",
      "11750 / 20000\n",
      "11800 / 20000\n",
      "11850 / 20000\n",
      "11900 / 20000\n",
      "11950 / 20000\n",
      "12000 / 20000\n",
      "12050 / 20000\n",
      "12100 / 20000\n",
      "12150 / 20000\n",
      "12200 / 20000\n",
      "12250 / 20000\n",
      "12300 / 20000\n",
      "12350 / 20000\n",
      "12400 / 20000\n",
      "12450 / 20000\n",
      "12500 / 20000\n",
      "12550 / 20000\n",
      "12600 / 20000\n",
      "12650 / 20000\n",
      "12700 / 20000\n",
      "12750 / 20000\n",
      "12800 / 20000\n",
      "12850 / 20000\n",
      "12900 / 20000\n",
      "12950 / 20000\n",
      "13000 / 20000\n",
      "13050 / 20000\n",
      "13100 / 20000\n",
      "13150 / 20000\n",
      "13200 / 20000\n",
      "13250 / 20000\n",
      "13300 / 20000\n",
      "13350 / 20000\n",
      "13400 / 20000\n",
      "13450 / 20000\n",
      "13500 / 20000\n",
      "13550 / 20000\n",
      "13600 / 20000\n",
      "13650 / 20000\n",
      "13700 / 20000\n",
      "13750 / 20000\n",
      "13800 / 20000\n",
      "13850 / 20000\n",
      "13900 / 20000\n",
      "13950 / 20000\n",
      "14000 / 20000\n",
      "14050 / 20000\n",
      "14100 / 20000\n",
      "14150 / 20000\n",
      "14200 / 20000\n",
      "14250 / 20000\n",
      "14300 / 20000\n",
      "14350 / 20000\n",
      "14400 / 20000\n",
      "14450 / 20000\n",
      "14500 / 20000\n",
      "14550 / 20000\n",
      "14600 / 20000\n",
      "14650 / 20000\n",
      "14700 / 20000\n",
      "14750 / 20000\n",
      "14800 / 20000\n",
      "14850 / 20000\n",
      "14900 / 20000\n",
      "14950 / 20000\n",
      "15000 / 20000\n",
      "15050 / 20000\n",
      "15100 / 20000\n",
      "15150 / 20000\n",
      "15200 / 20000\n",
      "15250 / 20000\n",
      "15300 / 20000\n",
      "15350 / 20000\n",
      "15400 / 20000\n",
      "15450 / 20000\n",
      "15500 / 20000\n",
      "15550 / 20000\n",
      "15600 / 20000\n",
      "15650 / 20000\n",
      "15700 / 20000\n",
      "15750 / 20000\n",
      "15800 / 20000\n",
      "15850 / 20000\n",
      "15900 / 20000\n",
      "15950 / 20000\n",
      "16000 / 20000\n",
      "16050 / 20000\n",
      "16100 / 20000\n",
      "16150 / 20000\n",
      "16200 / 20000\n",
      "16250 / 20000\n",
      "16300 / 20000\n",
      "16350 / 20000\n",
      "16400 / 20000\n",
      "16450 / 20000\n",
      "16500 / 20000\n",
      "16550 / 20000\n",
      "16600 / 20000\n",
      "16650 / 20000\n",
      "16700 / 20000\n",
      "16750 / 20000\n",
      "16800 / 20000\n",
      "16850 / 20000\n",
      "16900 / 20000\n",
      "16950 / 20000\n",
      "17000 / 20000\n",
      "17050 / 20000\n",
      "17100 / 20000\n",
      "17150 / 20000\n",
      "17200 / 20000\n",
      "17250 / 20000\n",
      "17300 / 20000\n",
      "17350 / 20000\n",
      "17400 / 20000\n",
      "17450 / 20000\n",
      "17500 / 20000\n",
      "17550 / 20000\n",
      "17600 / 20000\n",
      "17650 / 20000\n",
      "17700 / 20000\n",
      "17750 / 20000\n",
      "17800 / 20000\n",
      "17850 / 20000\n",
      "17900 / 20000\n",
      "17950 / 20000\n",
      "18000 / 20000\n",
      "18050 / 20000\n",
      "18100 / 20000\n",
      "18150 / 20000\n",
      "18200 / 20000\n",
      "18250 / 20000\n",
      "18300 / 20000\n",
      "18350 / 20000\n",
      "18400 / 20000\n",
      "18450 / 20000\n",
      "18500 / 20000\n",
      "18550 / 20000\n",
      "18600 / 20000\n",
      "18650 / 20000\n",
      "18700 / 20000\n",
      "18750 / 20000\n",
      "18800 / 20000\n",
      "18850 / 20000\n",
      "18900 / 20000\n",
      "18950 / 20000\n",
      "19000 / 20000\n",
      "19050 / 20000\n",
      "19100 / 20000\n",
      "19150 / 20000\n",
      "19200 / 20000\n",
      "19250 / 20000\n",
      "19300 / 20000\n",
      "19350 / 20000\n",
      "19400 / 20000\n",
      "19450 / 20000\n",
      "19500 / 20000\n",
      "19550 / 20000\n",
      "19600 / 20000\n",
      "19650 / 20000\n",
      "19700 / 20000\n",
      "19750 / 20000\n",
      "19800 / 20000\n",
      "19850 / 20000\n",
      "19900 / 20000\n",
      "19950 / 20000\n",
      "0 / 5000\n",
      "50 / 5000\n",
      "100 / 5000\n",
      "150 / 5000\n",
      "200 / 5000\n",
      "250 / 5000\n",
      "300 / 5000\n",
      "350 / 5000\n",
      "400 / 5000\n",
      "450 / 5000\n",
      "500 / 5000\n",
      "550 / 5000\n",
      "600 / 5000\n",
      "650 / 5000\n",
      "700 / 5000\n",
      "750 / 5000\n",
      "800 / 5000\n",
      "850 / 5000\n",
      "900 / 5000\n",
      "950 / 5000\n",
      "1000 / 5000\n",
      "1050 / 5000\n",
      "1100 / 5000\n",
      "1150 / 5000\n",
      "1200 / 5000\n",
      "1250 / 5000\n",
      "1300 / 5000\n",
      "1350 / 5000\n",
      "1400 / 5000\n",
      "1450 / 5000\n",
      "1500 / 5000\n",
      "1550 / 5000\n",
      "1600 / 5000\n",
      "1650 / 5000\n",
      "1700 / 5000\n",
      "1750 / 5000\n",
      "1800 / 5000\n",
      "1850 / 5000\n",
      "1900 / 5000\n",
      "1950 / 5000\n",
      "2000 / 5000\n",
      "2050 / 5000\n",
      "2100 / 5000\n",
      "2150 / 5000\n",
      "2200 / 5000\n",
      "2250 / 5000\n",
      "2300 / 5000\n",
      "2350 / 5000\n",
      "2400 / 5000\n",
      "2450 / 5000\n",
      "2500 / 5000\n",
      "2550 / 5000\n",
      "2600 / 5000\n",
      "2650 / 5000\n",
      "2700 / 5000\n",
      "2750 / 5000\n",
      "2800 / 5000\n",
      "2850 / 5000\n",
      "2900 / 5000\n",
      "2950 / 5000\n",
      "3000 / 5000\n",
      "3050 / 5000\n",
      "3100 / 5000\n",
      "3150 / 5000\n",
      "3200 / 5000\n",
      "3250 / 5000\n",
      "3300 / 5000\n",
      "3350 / 5000\n",
      "3400 / 5000\n",
      "3450 / 5000\n",
      "3500 / 5000\n",
      "3550 / 5000\n",
      "3600 / 5000\n",
      "3650 / 5000\n",
      "3700 / 5000\n",
      "3750 / 5000\n",
      "3800 / 5000\n",
      "3850 / 5000\n",
      "3900 / 5000\n",
      "3950 / 5000\n",
      "4000 / 5000\n",
      "4050 / 5000\n",
      "4100 / 5000\n",
      "4150 / 5000\n",
      "4200 / 5000\n",
      "4250 / 5000\n",
      "4300 / 5000\n",
      "4350 / 5000\n",
      "4400 / 5000\n",
      "4450 / 5000\n",
      "4500 / 5000\n",
      "4550 / 5000\n",
      "4600 / 5000\n",
      "4650 / 5000\n",
      "4700 / 5000\n",
      "4750 / 5000\n",
      "4800 / 5000\n",
      "4850 / 5000\n",
      "4900 / 5000\n",
      "4950 / 5000\n"
     ]
    }
   ],
   "source": [
    "grams = 4\n",
    "\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(training_all_clean,\n",
    "                                                       n_gram=grams)\n",
    "\n",
    "# Tokenize Validation\n",
    "val_data_tokens, _ = tokenize_dataset(validation_all_clean,\n",
    "                                      n_gram=grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "# grams = 1\n",
    "print(grams)\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_\"+str(grams)+\".p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_\"+str(grams)+\".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_tokens[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_tokens[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(val_data_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(hyperparameter_space=params,\n",
    "                          epochs=5,\n",
    "                          optimizer_name = \"Adam\"):\n",
    "\n",
    "    # returns all the permutations of the parameter search space\n",
    "    param_space = [*itertools.product(*params)]\n",
    "    \n",
    "    # validation loss dictionary\n",
    "    val_losses = {}\n",
    "    \n",
    "    # counter for progress\n",
    "    count = 0\n",
    "    \n",
    "    for param_comb in param_space:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"Parameter Combination = \" + str(count+1) + \" / \" + str(len(param_space)))\n",
    "        count = count + 1      \n",
    "        \n",
    "        NUM_EPOCHS = epochs\n",
    "        lr_rate = param_comb[0]             # learning rate\n",
    "        grams = param_comb[1]               # n-grams\n",
    "        max_vocab_size = int(param_comb[2]) # vocabulary size\n",
    "        embed_dimension = param_comb[3]     # embedding vector size\n",
    "        MAX_SENTENCE_LENGTH = param_comb[4] # max sentence length of data loader\n",
    "        BATCH_SIZE = param_comb[5]\n",
    "        \n",
    "        print(\"Learning Rate = \" + str(lr_rate))\n",
    "        print(\"Ngram = \" + str(grams))\n",
    "        print(\"Vocab Size = \" + str(max_vocab_size))\n",
    "        print(\"Embedding Dimension = \" + str(embed_dimension))\n",
    "        print(\"Max Sentence Length = \" + str(MAX_SENTENCE_LENGTH))\n",
    "        print(\"Batch Size = \" + str(BATCH_SIZE))\n",
    "\n",
    "        # Tokenization\n",
    "        # All tokens are created before the hyperparameter search loop\n",
    "        # Load the tokens here\n",
    "        train_data_tokens = pkl.load(open(\"train_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        all_train_tokens = pkl.load(open(\"all_train_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "\n",
    "        val_data_tokens = pkl.load(open(\"val_data_tokens_\"+str(grams)+\".p\", \"rb\"))\n",
    "        \n",
    "        print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "        print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "        \n",
    "        # Building Vocabulary\n",
    "        # implicitly gets the max_vocab_size parameter\n",
    "        token2id, id2token = build_vocab(all_train_tokens,\n",
    "                                         max_vocab_size=max_vocab_size)\n",
    "        \n",
    "        # Lets check the dictionary by loading random token from it\n",
    "        random_token_id = random.randint(0, len(id2token)-1)\n",
    "        random_token = id2token[random_token_id]\n",
    "        print (\"Token id {} -> token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "        print (\"Token {} -> token id {}\".format(random_token, token2id[random_token]))\n",
    "        \n",
    "        train_data_indices = token2index_dataset(train_data_tokens)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens)\n",
    "        # double checking\n",
    "        print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "        print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Load training and validation data\n",
    "        train_dataset = IMDBDataset(train_data_indices, \n",
    "                                    training_labels)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        val_dataset = IMDBDataset(val_data_indices, \n",
    "                                  validation_labels)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=imdb_func,\n",
    "                                                   shuffle=True)  \n",
    "\n",
    "        # Initialize the N-gram Model\n",
    "        model = BagOfNgrams(len(id2token), embed_dimension)\n",
    "        \n",
    "        # Both Adam and SGD will be tried\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "        else:\n",
    "            print(\"this optimizer is not implemented yet\")\n",
    "        \n",
    "        # Cross Entropy Loss will be used\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        \n",
    "        # Validation Losses will be stored in a list\n",
    "        # Caution: Two different optimizers\n",
    "        val_losses[param_comb] = []\n",
    "        \n",
    "    #for optimizer in optimizers:\n",
    "        print(\"Optimization Start\")\n",
    "        print(optimizer)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Validate every 100 iterations\n",
    "                # Adjust it to accustom changing batch sizes\n",
    "                if i > 0 and i % (50 * (64 / BATCH_SIZE)) == 0:\n",
    "\n",
    "                    # Accuracy Calculations\n",
    "                    train_acc = test_model(train_loader, model)\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    val_losses[param_comb].append(val_acc)\n",
    "\n",
    "                    # Logging\n",
    "                    print('Epoch:[{}/{}],Step:[{}/{}],Training Acc:{},Validation Acc:{}'.format( \n",
    "                               epoch+1, NUM_EPOCHS, \n",
    "                                i+1, len(train_loader), \n",
    "                                train_acc, val_acc))\n",
    "                      \n",
    "    return val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.01, 1, 100000.0, 100, 100, 64),\n",
       " (0.01, 1, 100000.0, 100, 100, 128),\n",
       " (0.01, 1, 100000.0, 100, 200, 64),\n",
       " (0.01, 1, 100000.0, 100, 200, 128),\n",
       " (0.01, 1, 100000.0, 150, 100, 64),\n",
       " (0.01, 1, 100000.0, 150, 100, 128),\n",
       " (0.01, 1, 100000.0, 150, 200, 64),\n",
       " (0.01, 1, 100000.0, 150, 200, 128),\n",
       " (0.01, 1, 100000.0, 200, 100, 64),\n",
       " (0.01, 1, 100000.0, 200, 100, 128),\n",
       " (0.01, 1, 100000.0, 200, 200, 64),\n",
       " (0.01, 1, 100000.0, 200, 200, 128),\n",
       " (0.01, 1, 1000000.0, 100, 100, 64),\n",
       " (0.01, 1, 1000000.0, 100, 100, 128),\n",
       " (0.01, 1, 1000000.0, 100, 200, 64),\n",
       " (0.01, 1, 1000000.0, 100, 200, 128),\n",
       " (0.01, 1, 1000000.0, 150, 100, 64),\n",
       " (0.01, 1, 1000000.0, 150, 100, 128),\n",
       " (0.01, 1, 1000000.0, 150, 200, 64),\n",
       " (0.01, 1, 1000000.0, 150, 200, 128),\n",
       " (0.01, 1, 1000000.0, 200, 100, 64),\n",
       " (0.01, 1, 1000000.0, 200, 100, 128),\n",
       " (0.01, 1, 1000000.0, 200, 200, 64),\n",
       " (0.01, 1, 1000000.0, 200, 200, 128),\n",
       " (0.01, 2, 100000.0, 100, 100, 64),\n",
       " (0.01, 2, 100000.0, 100, 100, 128),\n",
       " (0.01, 2, 100000.0, 100, 200, 64),\n",
       " (0.01, 2, 100000.0, 100, 200, 128),\n",
       " (0.01, 2, 100000.0, 150, 100, 64),\n",
       " (0.01, 2, 100000.0, 150, 100, 128),\n",
       " (0.01, 2, 100000.0, 150, 200, 64),\n",
       " (0.01, 2, 100000.0, 150, 200, 128),\n",
       " (0.01, 2, 100000.0, 200, 100, 64),\n",
       " (0.01, 2, 100000.0, 200, 100, 128),\n",
       " (0.01, 2, 100000.0, 200, 200, 64),\n",
       " (0.01, 2, 100000.0, 200, 200, 128),\n",
       " (0.01, 2, 1000000.0, 100, 100, 64),\n",
       " (0.01, 2, 1000000.0, 100, 100, 128),\n",
       " (0.01, 2, 1000000.0, 100, 200, 64),\n",
       " (0.01, 2, 1000000.0, 100, 200, 128),\n",
       " (0.01, 2, 1000000.0, 150, 100, 64),\n",
       " (0.01, 2, 1000000.0, 150, 100, 128),\n",
       " (0.01, 2, 1000000.0, 150, 200, 64),\n",
       " (0.01, 2, 1000000.0, 150, 200, 128),\n",
       " (0.01, 2, 1000000.0, 200, 100, 64),\n",
       " (0.01, 2, 1000000.0, 200, 100, 128),\n",
       " (0.01, 2, 1000000.0, 200, 200, 64),\n",
       " (0.01, 2, 1000000.0, 200, 200, 128),\n",
       " (0.01, 3, 100000.0, 100, 100, 64),\n",
       " (0.01, 3, 100000.0, 100, 100, 128),\n",
       " (0.01, 3, 100000.0, 100, 200, 64),\n",
       " (0.01, 3, 100000.0, 100, 200, 128),\n",
       " (0.01, 3, 100000.0, 150, 100, 64),\n",
       " (0.01, 3, 100000.0, 150, 100, 128),\n",
       " (0.01, 3, 100000.0, 150, 200, 64),\n",
       " (0.01, 3, 100000.0, 150, 200, 128),\n",
       " (0.01, 3, 100000.0, 200, 100, 64),\n",
       " (0.01, 3, 100000.0, 200, 100, 128),\n",
       " (0.01, 3, 100000.0, 200, 200, 64),\n",
       " (0.01, 3, 100000.0, 200, 200, 128),\n",
       " (0.01, 3, 1000000.0, 100, 100, 64),\n",
       " (0.01, 3, 1000000.0, 100, 100, 128),\n",
       " (0.01, 3, 1000000.0, 100, 200, 64),\n",
       " (0.01, 3, 1000000.0, 100, 200, 128),\n",
       " (0.01, 3, 1000000.0, 150, 100, 64),\n",
       " (0.01, 3, 1000000.0, 150, 100, 128),\n",
       " (0.01, 3, 1000000.0, 150, 200, 64),\n",
       " (0.01, 3, 1000000.0, 150, 200, 128),\n",
       " (0.01, 3, 1000000.0, 200, 100, 64),\n",
       " (0.01, 3, 1000000.0, 200, 100, 128),\n",
       " (0.01, 3, 1000000.0, 200, 200, 64),\n",
       " (0.01, 3, 1000000.0, 200, 200, 128),\n",
       " (0.01, 4, 100000.0, 100, 100, 64),\n",
       " (0.01, 4, 100000.0, 100, 100, 128),\n",
       " (0.01, 4, 100000.0, 100, 200, 64),\n",
       " (0.01, 4, 100000.0, 100, 200, 128),\n",
       " (0.01, 4, 100000.0, 150, 100, 64),\n",
       " (0.01, 4, 100000.0, 150, 100, 128),\n",
       " (0.01, 4, 100000.0, 150, 200, 64),\n",
       " (0.01, 4, 100000.0, 150, 200, 128),\n",
       " (0.01, 4, 100000.0, 200, 100, 64),\n",
       " (0.01, 4, 100000.0, 200, 100, 128),\n",
       " (0.01, 4, 100000.0, 200, 200, 64),\n",
       " (0.01, 4, 100000.0, 200, 200, 128),\n",
       " (0.01, 4, 1000000.0, 100, 100, 64),\n",
       " (0.01, 4, 1000000.0, 100, 100, 128),\n",
       " (0.01, 4, 1000000.0, 100, 200, 64),\n",
       " (0.01, 4, 1000000.0, 100, 200, 128),\n",
       " (0.01, 4, 1000000.0, 150, 100, 64),\n",
       " (0.01, 4, 1000000.0, 150, 100, 128),\n",
       " (0.01, 4, 1000000.0, 150, 200, 64),\n",
       " (0.01, 4, 1000000.0, 150, 200, 128),\n",
       " (0.01, 4, 1000000.0, 200, 100, 64),\n",
       " (0.01, 4, 1000000.0, 200, 100, 128),\n",
       " (0.01, 4, 1000000.0, 200, 200, 64),\n",
       " (0.01, 4, 1000000.0, 200, 200, 128),\n",
       " (0.1, 1, 100000.0, 100, 100, 64),\n",
       " (0.1, 1, 100000.0, 100, 100, 128),\n",
       " (0.1, 1, 100000.0, 100, 200, 64),\n",
       " (0.1, 1, 100000.0, 100, 200, 128),\n",
       " (0.1, 1, 100000.0, 150, 100, 64),\n",
       " (0.1, 1, 100000.0, 150, 100, 128),\n",
       " (0.1, 1, 100000.0, 150, 200, 64),\n",
       " (0.1, 1, 100000.0, 150, 200, 128),\n",
       " (0.1, 1, 100000.0, 200, 100, 64),\n",
       " (0.1, 1, 100000.0, 200, 100, 128),\n",
       " (0.1, 1, 100000.0, 200, 200, 64),\n",
       " (0.1, 1, 100000.0, 200, 200, 128),\n",
       " (0.1, 1, 1000000.0, 100, 100, 64),\n",
       " (0.1, 1, 1000000.0, 100, 100, 128),\n",
       " (0.1, 1, 1000000.0, 100, 200, 64),\n",
       " (0.1, 1, 1000000.0, 100, 200, 128),\n",
       " (0.1, 1, 1000000.0, 150, 100, 64),\n",
       " (0.1, 1, 1000000.0, 150, 100, 128),\n",
       " (0.1, 1, 1000000.0, 150, 200, 64),\n",
       " (0.1, 1, 1000000.0, 150, 200, 128),\n",
       " (0.1, 1, 1000000.0, 200, 100, 64),\n",
       " (0.1, 1, 1000000.0, 200, 100, 128),\n",
       " (0.1, 1, 1000000.0, 200, 200, 64),\n",
       " (0.1, 1, 1000000.0, 200, 200, 128),\n",
       " (0.1, 2, 100000.0, 100, 100, 64),\n",
       " (0.1, 2, 100000.0, 100, 100, 128),\n",
       " (0.1, 2, 100000.0, 100, 200, 64),\n",
       " (0.1, 2, 100000.0, 100, 200, 128),\n",
       " (0.1, 2, 100000.0, 150, 100, 64),\n",
       " (0.1, 2, 100000.0, 150, 100, 128),\n",
       " (0.1, 2, 100000.0, 150, 200, 64),\n",
       " (0.1, 2, 100000.0, 150, 200, 128),\n",
       " (0.1, 2, 100000.0, 200, 100, 64),\n",
       " (0.1, 2, 100000.0, 200, 100, 128),\n",
       " (0.1, 2, 100000.0, 200, 200, 64),\n",
       " (0.1, 2, 100000.0, 200, 200, 128),\n",
       " (0.1, 2, 1000000.0, 100, 100, 64),\n",
       " (0.1, 2, 1000000.0, 100, 100, 128),\n",
       " (0.1, 2, 1000000.0, 100, 200, 64),\n",
       " (0.1, 2, 1000000.0, 100, 200, 128),\n",
       " (0.1, 2, 1000000.0, 150, 100, 64),\n",
       " (0.1, 2, 1000000.0, 150, 100, 128),\n",
       " (0.1, 2, 1000000.0, 150, 200, 64),\n",
       " (0.1, 2, 1000000.0, 150, 200, 128),\n",
       " (0.1, 2, 1000000.0, 200, 100, 64),\n",
       " (0.1, 2, 1000000.0, 200, 100, 128),\n",
       " (0.1, 2, 1000000.0, 200, 200, 64),\n",
       " (0.1, 2, 1000000.0, 200, 200, 128),\n",
       " (0.1, 3, 100000.0, 100, 100, 64),\n",
       " (0.1, 3, 100000.0, 100, 100, 128),\n",
       " (0.1, 3, 100000.0, 100, 200, 64),\n",
       " (0.1, 3, 100000.0, 100, 200, 128),\n",
       " (0.1, 3, 100000.0, 150, 100, 64),\n",
       " (0.1, 3, 100000.0, 150, 100, 128),\n",
       " (0.1, 3, 100000.0, 150, 200, 64),\n",
       " (0.1, 3, 100000.0, 150, 200, 128),\n",
       " (0.1, 3, 100000.0, 200, 100, 64),\n",
       " (0.1, 3, 100000.0, 200, 100, 128),\n",
       " (0.1, 3, 100000.0, 200, 200, 64),\n",
       " (0.1, 3, 100000.0, 200, 200, 128),\n",
       " (0.1, 3, 1000000.0, 100, 100, 64),\n",
       " (0.1, 3, 1000000.0, 100, 100, 128),\n",
       " (0.1, 3, 1000000.0, 100, 200, 64),\n",
       " (0.1, 3, 1000000.0, 100, 200, 128),\n",
       " (0.1, 3, 1000000.0, 150, 100, 64),\n",
       " (0.1, 3, 1000000.0, 150, 100, 128),\n",
       " (0.1, 3, 1000000.0, 150, 200, 64),\n",
       " (0.1, 3, 1000000.0, 150, 200, 128),\n",
       " (0.1, 3, 1000000.0, 200, 100, 64),\n",
       " (0.1, 3, 1000000.0, 200, 100, 128),\n",
       " (0.1, 3, 1000000.0, 200, 200, 64),\n",
       " (0.1, 3, 1000000.0, 200, 200, 128),\n",
       " (0.1, 4, 100000.0, 100, 100, 64),\n",
       " (0.1, 4, 100000.0, 100, 100, 128),\n",
       " (0.1, 4, 100000.0, 100, 200, 64),\n",
       " (0.1, 4, 100000.0, 100, 200, 128),\n",
       " (0.1, 4, 100000.0, 150, 100, 64),\n",
       " (0.1, 4, 100000.0, 150, 100, 128),\n",
       " (0.1, 4, 100000.0, 150, 200, 64),\n",
       " (0.1, 4, 100000.0, 150, 200, 128),\n",
       " (0.1, 4, 100000.0, 200, 100, 64),\n",
       " (0.1, 4, 100000.0, 200, 100, 128),\n",
       " (0.1, 4, 100000.0, 200, 200, 64),\n",
       " (0.1, 4, 100000.0, 200, 200, 128),\n",
       " (0.1, 4, 1000000.0, 100, 100, 64),\n",
       " (0.1, 4, 1000000.0, 100, 100, 128),\n",
       " (0.1, 4, 1000000.0, 100, 200, 64),\n",
       " (0.1, 4, 1000000.0, 100, 200, 128),\n",
       " (0.1, 4, 1000000.0, 150, 100, 64),\n",
       " (0.1, 4, 1000000.0, 150, 100, 128),\n",
       " (0.1, 4, 1000000.0, 150, 200, 64),\n",
       " (0.1, 4, 1000000.0, 150, 200, 128),\n",
       " (0.1, 4, 1000000.0, 200, 100, 64),\n",
       " (0.1, 4, 1000000.0, 200, 100, 128),\n",
       " (0.1, 4, 1000000.0, 200, 200, 64),\n",
       " (0.1, 4, 1000000.0, 200, 200, 128),\n",
       " (0.5, 1, 100000.0, 100, 100, 64),\n",
       " (0.5, 1, 100000.0, 100, 100, 128),\n",
       " (0.5, 1, 100000.0, 100, 200, 64),\n",
       " (0.5, 1, 100000.0, 100, 200, 128),\n",
       " (0.5, 1, 100000.0, 150, 100, 64),\n",
       " (0.5, 1, 100000.0, 150, 100, 128),\n",
       " (0.5, 1, 100000.0, 150, 200, 64),\n",
       " (0.5, 1, 100000.0, 150, 200, 128),\n",
       " (0.5, 1, 100000.0, 200, 100, 64),\n",
       " (0.5, 1, 100000.0, 200, 100, 128),\n",
       " (0.5, 1, 100000.0, 200, 200, 64),\n",
       " (0.5, 1, 100000.0, 200, 200, 128),\n",
       " (0.5, 1, 1000000.0, 100, 100, 64),\n",
       " (0.5, 1, 1000000.0, 100, 100, 128),\n",
       " (0.5, 1, 1000000.0, 100, 200, 64),\n",
       " (0.5, 1, 1000000.0, 100, 200, 128),\n",
       " (0.5, 1, 1000000.0, 150, 100, 64),\n",
       " (0.5, 1, 1000000.0, 150, 100, 128),\n",
       " (0.5, 1, 1000000.0, 150, 200, 64),\n",
       " (0.5, 1, 1000000.0, 150, 200, 128),\n",
       " (0.5, 1, 1000000.0, 200, 100, 64),\n",
       " (0.5, 1, 1000000.0, 200, 100, 128),\n",
       " (0.5, 1, 1000000.0, 200, 200, 64),\n",
       " (0.5, 1, 1000000.0, 200, 200, 128),\n",
       " (0.5, 2, 100000.0, 100, 100, 64),\n",
       " (0.5, 2, 100000.0, 100, 100, 128),\n",
       " (0.5, 2, 100000.0, 100, 200, 64),\n",
       " (0.5, 2, 100000.0, 100, 200, 128),\n",
       " (0.5, 2, 100000.0, 150, 100, 64),\n",
       " (0.5, 2, 100000.0, 150, 100, 128),\n",
       " (0.5, 2, 100000.0, 150, 200, 64),\n",
       " (0.5, 2, 100000.0, 150, 200, 128),\n",
       " (0.5, 2, 100000.0, 200, 100, 64),\n",
       " (0.5, 2, 100000.0, 200, 100, 128),\n",
       " (0.5, 2, 100000.0, 200, 200, 64),\n",
       " (0.5, 2, 100000.0, 200, 200, 128),\n",
       " (0.5, 2, 1000000.0, 100, 100, 64),\n",
       " (0.5, 2, 1000000.0, 100, 100, 128),\n",
       " (0.5, 2, 1000000.0, 100, 200, 64),\n",
       " (0.5, 2, 1000000.0, 100, 200, 128),\n",
       " (0.5, 2, 1000000.0, 150, 100, 64),\n",
       " (0.5, 2, 1000000.0, 150, 100, 128),\n",
       " (0.5, 2, 1000000.0, 150, 200, 64),\n",
       " (0.5, 2, 1000000.0, 150, 200, 128),\n",
       " (0.5, 2, 1000000.0, 200, 100, 64),\n",
       " (0.5, 2, 1000000.0, 200, 100, 128),\n",
       " (0.5, 2, 1000000.0, 200, 200, 64),\n",
       " (0.5, 2, 1000000.0, 200, 200, 128),\n",
       " (0.5, 3, 100000.0, 100, 100, 64),\n",
       " (0.5, 3, 100000.0, 100, 100, 128),\n",
       " (0.5, 3, 100000.0, 100, 200, 64),\n",
       " (0.5, 3, 100000.0, 100, 200, 128),\n",
       " (0.5, 3, 100000.0, 150, 100, 64),\n",
       " (0.5, 3, 100000.0, 150, 100, 128),\n",
       " (0.5, 3, 100000.0, 150, 200, 64),\n",
       " (0.5, 3, 100000.0, 150, 200, 128),\n",
       " (0.5, 3, 100000.0, 200, 100, 64),\n",
       " (0.5, 3, 100000.0, 200, 100, 128),\n",
       " (0.5, 3, 100000.0, 200, 200, 64),\n",
       " (0.5, 3, 100000.0, 200, 200, 128),\n",
       " (0.5, 3, 1000000.0, 100, 100, 64),\n",
       " (0.5, 3, 1000000.0, 100, 100, 128),\n",
       " (0.5, 3, 1000000.0, 100, 200, 64),\n",
       " (0.5, 3, 1000000.0, 100, 200, 128),\n",
       " (0.5, 3, 1000000.0, 150, 100, 64),\n",
       " (0.5, 3, 1000000.0, 150, 100, 128),\n",
       " (0.5, 3, 1000000.0, 150, 200, 64),\n",
       " (0.5, 3, 1000000.0, 150, 200, 128),\n",
       " (0.5, 3, 1000000.0, 200, 100, 64),\n",
       " (0.5, 3, 1000000.0, 200, 100, 128),\n",
       " (0.5, 3, 1000000.0, 200, 200, 64),\n",
       " (0.5, 3, 1000000.0, 200, 200, 128),\n",
       " (0.5, 4, 100000.0, 100, 100, 64),\n",
       " (0.5, 4, 100000.0, 100, 100, 128),\n",
       " (0.5, 4, 100000.0, 100, 200, 64),\n",
       " (0.5, 4, 100000.0, 100, 200, 128),\n",
       " (0.5, 4, 100000.0, 150, 100, 64),\n",
       " (0.5, 4, 100000.0, 150, 100, 128),\n",
       " (0.5, 4, 100000.0, 150, 200, 64),\n",
       " (0.5, 4, 100000.0, 150, 200, 128),\n",
       " (0.5, 4, 100000.0, 200, 100, 64),\n",
       " (0.5, 4, 100000.0, 200, 100, 128),\n",
       " (0.5, 4, 100000.0, 200, 200, 64),\n",
       " (0.5, 4, 100000.0, 200, 200, 128),\n",
       " (0.5, 4, 1000000.0, 100, 100, 64),\n",
       " (0.5, 4, 1000000.0, 100, 100, 128),\n",
       " (0.5, 4, 1000000.0, 100, 200, 64),\n",
       " (0.5, 4, 1000000.0, 100, 200, 128),\n",
       " (0.5, 4, 1000000.0, 150, 100, 64),\n",
       " (0.5, 4, 1000000.0, 150, 100, 128),\n",
       " (0.5, 4, 1000000.0, 150, 200, 64),\n",
       " (0.5, 4, 1000000.0, 150, 200, 128),\n",
       " (0.5, 4, 1000000.0, 200, 100, 64),\n",
       " (0.5, 4, 1000000.0, 200, 100, 128),\n",
       " (0.5, 4, 1000000.0, 200, 200, 64),\n",
       " (0.5, 4, 1000000.0, 200, 200, 128),\n",
       " (1, 1, 100000.0, 100, 100, 64),\n",
       " (1, 1, 100000.0, 100, 100, 128),\n",
       " (1, 1, 100000.0, 100, 200, 64),\n",
       " (1, 1, 100000.0, 100, 200, 128),\n",
       " (1, 1, 100000.0, 150, 100, 64),\n",
       " (1, 1, 100000.0, 150, 100, 128),\n",
       " (1, 1, 100000.0, 150, 200, 64),\n",
       " (1, 1, 100000.0, 150, 200, 128),\n",
       " (1, 1, 100000.0, 200, 100, 64),\n",
       " (1, 1, 100000.0, 200, 100, 128),\n",
       " (1, 1, 100000.0, 200, 200, 64),\n",
       " (1, 1, 100000.0, 200, 200, 128),\n",
       " (1, 1, 1000000.0, 100, 100, 64),\n",
       " (1, 1, 1000000.0, 100, 100, 128),\n",
       " (1, 1, 1000000.0, 100, 200, 64),\n",
       " (1, 1, 1000000.0, 100, 200, 128),\n",
       " (1, 1, 1000000.0, 150, 100, 64),\n",
       " (1, 1, 1000000.0, 150, 100, 128),\n",
       " (1, 1, 1000000.0, 150, 200, 64),\n",
       " (1, 1, 1000000.0, 150, 200, 128),\n",
       " (1, 1, 1000000.0, 200, 100, 64),\n",
       " (1, 1, 1000000.0, 200, 100, 128),\n",
       " (1, 1, 1000000.0, 200, 200, 64),\n",
       " (1, 1, 1000000.0, 200, 200, 128),\n",
       " (1, 2, 100000.0, 100, 100, 64),\n",
       " (1, 2, 100000.0, 100, 100, 128),\n",
       " (1, 2, 100000.0, 100, 200, 64),\n",
       " (1, 2, 100000.0, 100, 200, 128),\n",
       " (1, 2, 100000.0, 150, 100, 64),\n",
       " (1, 2, 100000.0, 150, 100, 128),\n",
       " (1, 2, 100000.0, 150, 200, 64),\n",
       " (1, 2, 100000.0, 150, 200, 128),\n",
       " (1, 2, 100000.0, 200, 100, 64),\n",
       " (1, 2, 100000.0, 200, 100, 128),\n",
       " (1, 2, 100000.0, 200, 200, 64),\n",
       " (1, 2, 100000.0, 200, 200, 128),\n",
       " (1, 2, 1000000.0, 100, 100, 64),\n",
       " (1, 2, 1000000.0, 100, 100, 128),\n",
       " (1, 2, 1000000.0, 100, 200, 64),\n",
       " (1, 2, 1000000.0, 100, 200, 128),\n",
       " (1, 2, 1000000.0, 150, 100, 64),\n",
       " (1, 2, 1000000.0, 150, 100, 128),\n",
       " (1, 2, 1000000.0, 150, 200, 64),\n",
       " (1, 2, 1000000.0, 150, 200, 128),\n",
       " (1, 2, 1000000.0, 200, 100, 64),\n",
       " (1, 2, 1000000.0, 200, 100, 128),\n",
       " (1, 2, 1000000.0, 200, 200, 64),\n",
       " (1, 2, 1000000.0, 200, 200, 128),\n",
       " (1, 3, 100000.0, 100, 100, 64),\n",
       " (1, 3, 100000.0, 100, 100, 128),\n",
       " (1, 3, 100000.0, 100, 200, 64),\n",
       " (1, 3, 100000.0, 100, 200, 128),\n",
       " (1, 3, 100000.0, 150, 100, 64),\n",
       " (1, 3, 100000.0, 150, 100, 128),\n",
       " (1, 3, 100000.0, 150, 200, 64),\n",
       " (1, 3, 100000.0, 150, 200, 128),\n",
       " (1, 3, 100000.0, 200, 100, 64),\n",
       " (1, 3, 100000.0, 200, 100, 128),\n",
       " (1, 3, 100000.0, 200, 200, 64),\n",
       " (1, 3, 100000.0, 200, 200, 128),\n",
       " (1, 3, 1000000.0, 100, 100, 64),\n",
       " (1, 3, 1000000.0, 100, 100, 128),\n",
       " (1, 3, 1000000.0, 100, 200, 64),\n",
       " (1, 3, 1000000.0, 100, 200, 128),\n",
       " (1, 3, 1000000.0, 150, 100, 64),\n",
       " (1, 3, 1000000.0, 150, 100, 128),\n",
       " (1, 3, 1000000.0, 150, 200, 64),\n",
       " (1, 3, 1000000.0, 150, 200, 128),\n",
       " (1, 3, 1000000.0, 200, 100, 64),\n",
       " (1, 3, 1000000.0, 200, 100, 128),\n",
       " (1, 3, 1000000.0, 200, 200, 64),\n",
       " (1, 3, 1000000.0, 200, 200, 128),\n",
       " (1, 4, 100000.0, 100, 100, 64),\n",
       " (1, 4, 100000.0, 100, 100, 128),\n",
       " (1, 4, 100000.0, 100, 200, 64),\n",
       " (1, 4, 100000.0, 100, 200, 128),\n",
       " (1, 4, 100000.0, 150, 100, 64),\n",
       " (1, 4, 100000.0, 150, 100, 128),\n",
       " (1, 4, 100000.0, 150, 200, 64),\n",
       " (1, 4, 100000.0, 150, 200, 128),\n",
       " (1, 4, 100000.0, 200, 100, 64),\n",
       " (1, 4, 100000.0, 200, 100, 128),\n",
       " (1, 4, 100000.0, 200, 200, 64),\n",
       " (1, 4, 100000.0, 200, 200, 128),\n",
       " (1, 4, 1000000.0, 100, 100, 64),\n",
       " (1, 4, 1000000.0, 100, 100, 128),\n",
       " (1, 4, 1000000.0, 100, 200, 64),\n",
       " (1, 4, 1000000.0, 100, 200, 128),\n",
       " (1, 4, 1000000.0, 150, 100, 64),\n",
       " (1, 4, 1000000.0, 150, 100, 128),\n",
       " (1, 4, 1000000.0, 150, 200, 64),\n",
       " (1, 4, 1000000.0, 150, 200, 128),\n",
       " (1, 4, 1000000.0, 200, 100, 64),\n",
       " (1, 4, 1000000.0, 200, 100, 128),\n",
       " (1, 4, 1000000.0, 200, 200, 64),\n",
       " (1, 4, 1000000.0, 200, 200, 128),\n",
       " (2, 1, 100000.0, 100, 100, 64),\n",
       " (2, 1, 100000.0, 100, 100, 128),\n",
       " (2, 1, 100000.0, 100, 200, 64),\n",
       " (2, 1, 100000.0, 100, 200, 128),\n",
       " (2, 1, 100000.0, 150, 100, 64),\n",
       " (2, 1, 100000.0, 150, 100, 128),\n",
       " (2, 1, 100000.0, 150, 200, 64),\n",
       " (2, 1, 100000.0, 150, 200, 128),\n",
       " (2, 1, 100000.0, 200, 100, 64),\n",
       " (2, 1, 100000.0, 200, 100, 128),\n",
       " (2, 1, 100000.0, 200, 200, 64),\n",
       " (2, 1, 100000.0, 200, 200, 128),\n",
       " (2, 1, 1000000.0, 100, 100, 64),\n",
       " (2, 1, 1000000.0, 100, 100, 128),\n",
       " (2, 1, 1000000.0, 100, 200, 64),\n",
       " (2, 1, 1000000.0, 100, 200, 128),\n",
       " (2, 1, 1000000.0, 150, 100, 64),\n",
       " (2, 1, 1000000.0, 150, 100, 128),\n",
       " (2, 1, 1000000.0, 150, 200, 64),\n",
       " (2, 1, 1000000.0, 150, 200, 128),\n",
       " (2, 1, 1000000.0, 200, 100, 64),\n",
       " (2, 1, 1000000.0, 200, 100, 128),\n",
       " (2, 1, 1000000.0, 200, 200, 64),\n",
       " (2, 1, 1000000.0, 200, 200, 128),\n",
       " (2, 2, 100000.0, 100, 100, 64),\n",
       " (2, 2, 100000.0, 100, 100, 128),\n",
       " (2, 2, 100000.0, 100, 200, 64),\n",
       " (2, 2, 100000.0, 100, 200, 128),\n",
       " (2, 2, 100000.0, 150, 100, 64),\n",
       " (2, 2, 100000.0, 150, 100, 128),\n",
       " (2, 2, 100000.0, 150, 200, 64),\n",
       " (2, 2, 100000.0, 150, 200, 128),\n",
       " (2, 2, 100000.0, 200, 100, 64),\n",
       " (2, 2, 100000.0, 200, 100, 128),\n",
       " (2, 2, 100000.0, 200, 200, 64),\n",
       " (2, 2, 100000.0, 200, 200, 128),\n",
       " (2, 2, 1000000.0, 100, 100, 64),\n",
       " (2, 2, 1000000.0, 100, 100, 128),\n",
       " (2, 2, 1000000.0, 100, 200, 64),\n",
       " (2, 2, 1000000.0, 100, 200, 128),\n",
       " (2, 2, 1000000.0, 150, 100, 64),\n",
       " (2, 2, 1000000.0, 150, 100, 128),\n",
       " (2, 2, 1000000.0, 150, 200, 64),\n",
       " (2, 2, 1000000.0, 150, 200, 128),\n",
       " (2, 2, 1000000.0, 200, 100, 64),\n",
       " (2, 2, 1000000.0, 200, 100, 128),\n",
       " (2, 2, 1000000.0, 200, 200, 64),\n",
       " (2, 2, 1000000.0, 200, 200, 128),\n",
       " (2, 3, 100000.0, 100, 100, 64),\n",
       " (2, 3, 100000.0, 100, 100, 128),\n",
       " (2, 3, 100000.0, 100, 200, 64),\n",
       " (2, 3, 100000.0, 100, 200, 128),\n",
       " (2, 3, 100000.0, 150, 100, 64),\n",
       " (2, 3, 100000.0, 150, 100, 128),\n",
       " (2, 3, 100000.0, 150, 200, 64),\n",
       " (2, 3, 100000.0, 150, 200, 128),\n",
       " (2, 3, 100000.0, 200, 100, 64),\n",
       " (2, 3, 100000.0, 200, 100, 128),\n",
       " (2, 3, 100000.0, 200, 200, 64),\n",
       " (2, 3, 100000.0, 200, 200, 128),\n",
       " (2, 3, 1000000.0, 100, 100, 64),\n",
       " (2, 3, 1000000.0, 100, 100, 128),\n",
       " (2, 3, 1000000.0, 100, 200, 64),\n",
       " (2, 3, 1000000.0, 100, 200, 128),\n",
       " (2, 3, 1000000.0, 150, 100, 64),\n",
       " (2, 3, 1000000.0, 150, 100, 128),\n",
       " (2, 3, 1000000.0, 150, 200, 64),\n",
       " (2, 3, 1000000.0, 150, 200, 128),\n",
       " (2, 3, 1000000.0, 200, 100, 64),\n",
       " (2, 3, 1000000.0, 200, 100, 128),\n",
       " (2, 3, 1000000.0, 200, 200, 64),\n",
       " (2, 3, 1000000.0, 200, 200, 128),\n",
       " (2, 4, 100000.0, 100, 100, 64),\n",
       " (2, 4, 100000.0, 100, 100, 128),\n",
       " (2, 4, 100000.0, 100, 200, 64),\n",
       " (2, 4, 100000.0, 100, 200, 128),\n",
       " (2, 4, 100000.0, 150, 100, 64),\n",
       " (2, 4, 100000.0, 150, 100, 128),\n",
       " (2, 4, 100000.0, 150, 200, 64),\n",
       " (2, 4, 100000.0, 150, 200, 128),\n",
       " (2, 4, 100000.0, 200, 100, 64),\n",
       " (2, 4, 100000.0, 200, 100, 128),\n",
       " (2, 4, 100000.0, 200, 200, 64),\n",
       " (2, 4, 100000.0, 200, 200, 128),\n",
       " (2, 4, 1000000.0, 100, 100, 64),\n",
       " (2, 4, 1000000.0, 100, 100, 128),\n",
       " (2, 4, 1000000.0, 100, 200, 64),\n",
       " (2, 4, 1000000.0, 100, 200, 128),\n",
       " (2, 4, 1000000.0, 150, 100, 64),\n",
       " (2, 4, 1000000.0, 150, 100, 128),\n",
       " (2, 4, 1000000.0, 150, 200, 64),\n",
       " (2, 4, 1000000.0, 150, 200, 128),\n",
       " (2, 4, 1000000.0, 200, 100, 64),\n",
       " (2, 4, 1000000.0, 200, 100, 128),\n",
       " (2, 4, 1000000.0, 200, 200, 64),\n",
       " (2, 4, 1000000.0, 200, 200, 128)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [[1e-2,1e-1,5e-1,1,2], ## learning rates\n",
    "          list(range(1,5)), ## ngrams\n",
    "          [1e5,1e6], ## vocab size\n",
    "          [100,150,200], ## embedding size\n",
    "          [100,200], ## max sentence length\n",
    "          [64,128] ## batch size\n",
    "         ]\n",
    "\n",
    "# params = [[1e-1,1,2,5], ## learning rates\n",
    "#           list(range(1,2)), ## ngrams\n",
    "#           [1e5], ## vocab size\n",
    "#           [100], ## embedding size\n",
    "#           [100], ## max sentence length\n",
    "#           [64] ## batch size\n",
    "#          ]\n",
    "\n",
    "print(len([*itertools.product(*params)]))\n",
    "[*itertools.product(*params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 1 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 1948 -> token fate\n",
      "Token fate -> token id 1948\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:70.62,Validation Acc:69.36\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:81.99,Validation Acc:80.88\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.16,Validation Acc:83.86\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.715,Validation Acc:84.4\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.31,Validation Acc:85.72\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.01,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:88.905,Validation Acc:85.36\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:89.795,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.58,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.205,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.685,Validation Acc:85.16\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.53,Validation Acc:86.2\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 2 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 43413 -> token robyn\n",
      "Token robyn -> token id 43413\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:66.13,Validation Acc:64.2\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:72.11,Validation Acc:71.04\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:78.99,Validation Acc:77.54\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.815,Validation Acc:82.24\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:85.525,Validation Acc:83.58\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:86.535,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.24,Validation Acc:84.6\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:88.17,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.01,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.205,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.01,Validation Acc:86.34\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:89.715,Validation Acc:85.62\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 3 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 25784 -> token nitro\n",
      "Token nitro -> token id 25784\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:64.59,Validation Acc:63.76\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:79.045,Validation Acc:77.62\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.31,Validation Acc:83.9\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:86.59,Validation Acc:84.26\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:86.955,Validation Acc:84.14\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.24,Validation Acc:86.0\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.82,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.93,Validation Acc:84.82\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.275,Validation Acc:85.94\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.905,Validation Acc:86.26\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:90.95,Validation Acc:85.86\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:90.95,Validation Acc:85.68\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 4 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 100\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 81015 -> token devlin\n",
      "Token devlin -> token id 81015\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:61.84,Validation Acc:60.42\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:70.765,Validation Acc:69.9\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:77.41,Validation Acc:76.56\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:83.4,Validation Acc:81.9\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:84.85,Validation Acc:83.06\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.25,Validation Acc:85.42\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:87.995,Validation Acc:85.46\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:87.995,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:88.985,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.625,Validation Acc:86.42\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.285,Validation Acc:86.58\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.325,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 5 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 54872 -> token ramen\n",
      "Token ramen -> token id 54872\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:73.61,Validation Acc:72.24\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:82.7,Validation Acc:81.06\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:86.03,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.745,Validation Acc:85.22\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.395,Validation Acc:85.3\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.45,Validation Acc:85.78\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:89.935,Validation Acc:85.82\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.505,Validation Acc:84.1\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:89.96,Validation Acc:85.24\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.9,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.135,Validation Acc:85.3\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.85,Validation Acc:86.08\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 6 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 52972 -> token this.*frodo\n",
      "Token this.*frodo -> token id 52972\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:68.825,Validation Acc:68.4\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:74.51,Validation Acc:73.1\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:81.8,Validation Acc:80.48\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.92,Validation Acc:83.56\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.56,Validation Acc:84.32\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.715,Validation Acc:85.12\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.745,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.1,Validation Acc:86.32\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.0,Validation Acc:85.8\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:90.275,Validation Acc:86.54\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.575,Validation Acc:86.2\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.11,Validation Acc:86.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Parameter Combination = 7 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 68072 -> token astaire).it\n",
      "Token astaire).it -> token id 68072\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:74.67,Validation Acc:73.32\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.455,Validation Acc:81.96\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:84.85,Validation Acc:82.68\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:87.645,Validation Acc:84.82\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:88.325,Validation Acc:84.92\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.78,Validation Acc:86.24\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.265,Validation Acc:86.38\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:88.255,Validation Acc:84.4\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:91.0,Validation Acc:86.08\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:91.36,Validation Acc:86.22\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:88.17,Validation Acc:83.76\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.5,Validation Acc:85.76\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 8 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 150\n",
      "Max Sentence Length = 200\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 62287 -> token doodlebop\n",
      "Token doodlebop -> token id 62287\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:69.385,Validation Acc:68.32\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:75.915,Validation Acc:74.94\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:82.655,Validation Acc:81.62\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:84.415,Validation Acc:82.76\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:86.495,Validation Acc:84.48\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:87.775,Validation Acc:85.2\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:88.79,Validation Acc:85.74\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.25,Validation Acc:85.66\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:89.915,Validation Acc:85.96\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.02,Validation Acc:85.04\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.605,Validation Acc:86.28\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:91.125,Validation Acc:86.46\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 9 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 64\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 1045 -> token cold\n",
      "Token cold -> token id 1045\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[51/313],Training Acc:69.245,Validation Acc:67.52\n",
      "Epoch:[1/2],Step:[101/313],Training Acc:83.82,Validation Acc:82.48\n",
      "Epoch:[1/2],Step:[151/313],Training Acc:85.995,Validation Acc:84.18\n",
      "Epoch:[1/2],Step:[201/313],Training Acc:88.2,Validation Acc:85.78\n",
      "Epoch:[1/2],Step:[251/313],Training Acc:89.095,Validation Acc:85.7\n",
      "Epoch:[1/2],Step:[301/313],Training Acc:89.41,Validation Acc:85.6\n",
      "Epoch:[2/2],Step:[51/313],Training Acc:90.25,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/313],Training Acc:90.11,Validation Acc:85.54\n",
      "Epoch:[2/2],Step:[151/313],Training Acc:90.075,Validation Acc:84.92\n",
      "Epoch:[2/2],Step:[201/313],Training Acc:90.785,Validation Acc:85.64\n",
      "Epoch:[2/2],Step:[251/313],Training Acc:91.455,Validation Acc:86.12\n",
      "Epoch:[2/2],Step:[301/313],Training Acc:91.545,Validation Acc:86.02\n",
      "-----------------------------------------------------------\n",
      "Parameter Combination = 10 / 480\n",
      "Learning Rate = 0.01\n",
      "Ngram = 1\n",
      "Vocab Size = 100000\n",
      "Embedding Dimension = 200\n",
      "Max Sentence Length = 100\n",
      "Batch Size = 128\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Total number of tokens in train dataset is 4763856\n",
      "Token id 28071 -> token deadliest\n",
      "Token deadliest -> token id 28071\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Optimization Start\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch:[1/2],Step:[26/157],Training Acc:69.825,Validation Acc:69.06\n",
      "Epoch:[1/2],Step:[51/157],Training Acc:78.95,Validation Acc:77.62\n",
      "Epoch:[1/2],Step:[76/157],Training Acc:84.325,Validation Acc:82.08\n",
      "Epoch:[1/2],Step:[101/157],Training Acc:85.78,Validation Acc:83.68\n",
      "Epoch:[1/2],Step:[126/157],Training Acc:87.455,Validation Acc:84.78\n",
      "Epoch:[1/2],Step:[151/157],Training Acc:88.18,Validation Acc:85.32\n",
      "Epoch:[2/2],Step:[26/157],Training Acc:89.375,Validation Acc:85.68\n",
      "Epoch:[2/2],Step:[51/157],Training Acc:89.135,Validation Acc:85.56\n",
      "Epoch:[2/2],Step:[76/157],Training Acc:90.365,Validation Acc:86.06\n",
      "Epoch:[2/2],Step:[101/157],Training Acc:89.62,Validation Acc:85.5\n",
      "Epoch:[2/2],Step:[126/157],Training Acc:90.43,Validation Acc:85.72\n",
      "Epoch:[2/2],Step:[151/157],Training Acc:90.095,Validation Acc:85.38\n"
     ]
    }
   ],
   "source": [
    "param_val_losses_adam = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 2,\n",
    "                                         optimizer_name = \"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_val_losses_sgd = hyperparameter_search(hyperparameter_space = params,\n",
    "                                         epochs = 10,\n",
    "                                         optimizer_name = \"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 1, 100000.0, 100, 100, 64) [82.26, 83.48, 84.34, 85.2, 83.78, 85.28, 84.88, 82.8, 84.84, 84.4, 83.16, 84.8]\n",
      "(0.1, 1, 100000.0, 100, 100, 128) [83.24, 78.62, 84.92, 85.38, 85.7, 85.4, 85.28, 84.44, 85.2, 84.86, 85.4, 85.96]\n",
      "(1, 1, 100000.0, 100, 100, 64) [78.52, 79.34, 64.98, 82.32, 79.98, 79.36, 75.4, 79.26, 80.8, 75.32, 80.42, 81.56]\n",
      "(1, 1, 100000.0, 100, 100, 128) [51.9, 52.48, 75.2, 82.9, 75.08, 73.48, 84.02, 84.46, 80.56, 83.08, 83.2, 80.08]\n",
      "(2, 1, 100000.0, 100, 100, 64) [78.58, 52.7, 79.3, 77.6, 69.44, 78.26, 81.92, 81.8, 73.88, 74.9, 79.58, 79.26]\n",
      "(2, 1, 100000.0, 100, 100, 128) [68.06, 67.5, 80.56, 68.62, 82.84, 83.54, 80.02, 82.04, 82.1, 81.46, 83.42, 83.66]\n",
      "(5, 1, 100000.0, 100, 100, 64) [81.5, 76.28, 70.52, 56.0, 74.78, 72.3, 71.08, 79.52, 82.86, 81.08, 74.76, 77.86]\n",
      "(5, 1, 100000.0, 100, 100, 128) [77.3, 71.52, 82.48, 81.4, 75.44, 82.62, 78.06, 79.76, 81.08, 83.5, 74.34, 81.64]\n"
     ]
    }
   ],
   "source": [
    "for key, value in param_val_losses_adam.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in param_val_losses_sgd.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
